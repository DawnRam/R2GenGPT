The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]0it [00:00, ?it/s]
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at microsoft/swin-base-patch4-window7-224 were not used when initializing SwinModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
Using pad_token, but it is not set yet.
Traceback (most recent call last):
  File "train.py", line 51, in <module>
    main()
  File "train.py", line 47, in main
    train(args)
  File "train.py", line 33, in train
    model = R2GenGPT(args)
  File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 63, in __init__
    self.lm_model = AutoModelForCausalLM.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 456, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 957, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 671, in __getitem__
    raise KeyError(key)
KeyError: 'stablelm'
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at microsoft/swin-base-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
Using pad_token, but it is not set yet.
Traceback (most recent call last):
  File "train.py", line 51, in <module>
    main()
  File "train.py", line 47, in main
    train(args)
  File "train.py", line 33, in train
    model = R2GenGPT(args)
  File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 63, in __init__
    self.lm_model = AutoModelForCausalLM.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 456, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 957, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 671, in __getitem__
    raise KeyError(key)
KeyError: 'stablelm'
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at microsoft/swin-base-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
Using pad_token, but it is not set yet.
stabilityai/stablelm-3b-4e1t
Traceback (most recent call last):
  File "train.py", line 51, in <module>
    main()
  File "train.py", line 47, in main
    train(args)
  File "train.py", line 33, in train
    model = R2GenGPT(args)
  File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 65, in __init__
    self.lm_model = AutoModelForCausalLM.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 456, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 957, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 671, in __getitem__
    raise KeyError(key)
KeyError: 'stablelm'
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[rank: 1] Global seed set to 42
[rank: 2] Global seed set to 42
[rank: 3] Global seed set to 42
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank1]:     main()
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank1]:     train(args)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank1]:     trainer.fit(model, datamodule=dm)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank1]:     return function(*args, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 949, in _run
[rank1]:     self.strategy.setup(self)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 151, in setup
[rank1]:     self.model_to_device()
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 305, in model_to_device
[rank1]:     self.model.to(self.root_device)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 54, in to
[rank1]:     return super().to(*args, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
[rank1]:     return self._apply(convert)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank1]:     module._apply(fn)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank1]:     module._apply(fn)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank1]:     module._apply(fn)
[rank1]:   [Previous line repeated 3 more times]
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
[rank1]:     param_applied = fn(param)
[rank1]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
[rank1]:     return t.to(
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 1 has a total capacity of 23.59 GiB of which 23.75 MiB is free. Process 3083494 has 18.94 GiB memory in use. Including non-PyTorch memory, this process has 4.61 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 158.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "train.py", line 51, in <module>
[rank0]:     main()
[rank0]:   File "train.py", line 47, in main
[rank0]:     train(args)
[rank0]:   File "train.py", line 40, in train
[rank0]:     trainer.fit(model, datamodule=dm)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 949, in _run
[rank0]:     self.strategy.setup(self)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 151, in setup
[rank0]:     self.model_to_device()
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 305, in model_to_device
[rank0]:     self.model.to(self.root_device)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 54, in to
[rank0]:     return super().to(*args, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
[rank0]:     return self._apply(convert)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank0]:     module._apply(fn)
[rank0]:   [Previous line repeated 3 more times]
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
[rank0]:     param_applied = fn(param)
[rank0]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
[rank0]:     return t.to(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 23.59 GiB of which 31.75 MiB is free. Process 3083493 has 18.94 GiB memory in use. Including non-PyTorch memory, this process has 4.60 GiB memory in use. Of the allocated memory 4.09 GiB is allocated by PyTorch, and 158.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank3]:     main()
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank3]:     train(args)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank3]:     trainer.fit(model, datamodule=dm)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank3]:     return function(*args, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 949, in _run
[rank3]:     self.strategy.setup(self)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 151, in setup
[rank3]:     self.model_to_device()
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 305, in model_to_device
[rank3]:     self.model.to(self.root_device)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 54, in to
[rank3]:     return super().to(*args, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
[rank3]:     return self._apply(convert)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank3]:     module._apply(fn)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank3]:     module._apply(fn)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank3]:     module._apply(fn)
[rank3]:   [Previous line repeated 3 more times]
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
[rank3]:     param_applied = fn(param)
[rank3]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
[rank3]:     return t.to(
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 3 has a total capacity of 23.59 GiB of which 23.75 MiB is free. Process 3083496 has 18.94 GiB memory in use. Including non-PyTorch memory, this process has 4.61 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 158.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W623 11:29:24.216247928 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank2]:     main()
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank2]:     train(args)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank2]:     trainer.fit(model, datamodule=dm)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank2]:     call._call_and_handle_interrupt(
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank2]:     return function(*args, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank2]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 949, in _run
[rank2]:     self.strategy.setup(self)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 151, in setup
[rank2]:     self.model_to_device()
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 305, in model_to_device
[rank2]:     self.model.to(self.root_device)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 54, in to
[rank2]:     return super().to(*args, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
[rank2]:     return self._apply(convert)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank2]:     module._apply(fn)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank2]:     module._apply(fn)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank2]:     module._apply(fn)
[rank2]:   [Previous line repeated 3 more times]
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
[rank2]:     param_applied = fn(param)
[rank2]:   File "/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
[rank2]:     return t.to(
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 2 has a total capacity of 23.59 GiB of which 23.75 MiB is free. Process 3083495 has 18.94 GiB memory in use. Including non-PyTorch memory, this process has 4.61 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 158.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at microsoft/swin-base-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
Using pad_token, but it is not set yet.
stabilityai/stablelm-3b-4e1t
Traceback (most recent call last):
  File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
    main()
  File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
    train(args)
  File "/home/eechengyang/Code/R2GenGPT/train.py", line 33, in train
    model = R2GenGPT(args)
  File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 65, in __init__
    self.lm_model = AutoModelForCausalLM.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 456, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 957, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 671, in __getitem__
    raise KeyError(key)
KeyError: 'stablelm'
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at microsoft/swin-base-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
Using pad_token, but it is not set yet.
stabilityai/stablelm-3b-4e1t
Traceback (most recent call last):
  File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
    main()
  File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
    train(args)
  File "/home/eechengyang/Code/R2GenGPT/train.py", line 33, in train
    model = R2GenGPT(args)
  File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 65, in __init__
    self.lm_model = AutoModelForCausalLM.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 456, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 957, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 671, in __getitem__
    raise KeyError(key)
KeyError: 'stablelm'
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
Traceback (most recent call last):
  File "/home/eechengyang/Code/R2GenGPT/train.py", line 6, in <module>
    from models.R2GenGPT import R2GenGPT
  File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 12, in <module>
    from lightning_tools.optim import config_optimizer
  File "/home/eechengyang/Code/R2GenGPT/lightning_tools/optim.py", line 1, in <module>
    from transformers import AdamW
ImportError: cannot import name 'AdamW' from 'transformers' (/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/transformers/__init__.py)
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
[rank: 3] Global seed set to 42
[rank: 2] Global seed set to 42
[rank: 1] Global seed set to 42
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,3,7]

  | Name           | Type                | Params
-------------------------------------------------------
0 | visual_encoder | SwinModel           | 86.7 M
1 | lm_model       | StableLmForCausalLM | 2.8 B 
2 | embed_tokens   | Embedding           | 128 M 
3 | lm_proj        | Linear              | 2.6 M 
4 | layer_norm     | LayerNorm           | 5.1 K 
-------------------------------------------------------
2.6 M     Trainable params
2.9 B     Non-trainable params
2.9 B     Total params
11,539.262Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 16,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0:  50%|     | 1/2 [00:08<00:08,  8.12s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:13<00:00,  6.62s/it][rank1]: Traceback (most recent call last):
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank1]:     main()
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank1]:     train(args)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank1]:     trainer.fit(model, datamodule=dm)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank1]:     return function(*args, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank1]:     results = self._run_stage()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank1]:     self._run_sanity_check()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank1]:     val_loop.run()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank1]:     return loop_run(self, *args, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank1]:     return self.on_run_end()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank1]:     self._on_evaluation_epoch_end()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank1]:     call._call_lightning_module_hook(trainer, hook_name)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank1]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank1]:     score, scores = scorer.compute_score(ref, hypo)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank1]:     stat = self._stat(res[i][0], gts[i])
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank1]:     self.meteor_p.stdin.flush()
[rank1]: BrokenPipeError: [Errno 32] Broken pipe
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank2]:     main()
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank2]:     train(args)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank2]:     trainer.fit(model, datamodule=dm)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank2]:     call._call_and_handle_interrupt(
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank2]:     return function(*args, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank2]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank2]:     results = self._run_stage()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank2]:     self._run_sanity_check()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank2]:     val_loop.run()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank2]:     return loop_run(self, *args, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank2]:     return self.on_run_end()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank2]:     self._on_evaluation_epoch_end()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank2]:     call._call_lightning_module_hook(trainer, hook_name)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank2]:     output = fn(*args, **kwargs)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank2]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank2]:     score, scores = scorer.compute_score(ref, hypo)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank2]:     stat = self._stat(res[i][0], gts[i])
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank2]:     self.meteor_p.stdin.flush()
[rank2]: BrokenPipeError: [Errno 32] Broken pipe
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank0]:     main()
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank0]:     train(args)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank0]:     trainer.fit(model, datamodule=dm)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank0]:     results = self._run_stage()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank0]:     self._run_sanity_check()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank0]:     val_loop.run()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank0]:     return loop_run(self, *args, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank0]:     return self.on_run_end()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank0]:     self._on_evaluation_epoch_end()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank0]:     call._call_lightning_module_hook(trainer, hook_name)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank0]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank0]:     score, scores = scorer.compute_score(ref, hypo)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank0]:     stat = self._stat(res[i][0], gts[i])
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank0]:     self.meteor_p.stdin.flush()
[rank0]: BrokenPipeError: [Errno 32] Broken pipe
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank3]:     main()
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank3]:     train(args)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank3]:     trainer.fit(model, datamodule=dm)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank3]:     return function(*args, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank3]:     results = self._run_stage()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank3]:     self._run_sanity_check()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank3]:     val_loop.run()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank3]:     return loop_run(self, *args, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank3]:     return self.on_run_end()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank3]:     self._on_evaluation_epoch_end()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank3]:     call._call_lightning_module_hook(trainer, hook_name)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank3]:     output = fn(*args, **kwargs)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank3]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank3]:     score, scores = scorer.compute_score(ref, hypo)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank3]:     stat = self._stat(res[i][0], gts[i])
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank3]:     self.meteor_p.stdin.flush()
[rank3]: BrokenPipeError: [Errno 32] Broken pipe
                                                                           [rank0]:[W623 11:44:35.531589817 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
[rank: 1] Global seed set to 42
[rank: 3] Global seed set to 42
[rank: 2] Global seed set to 42
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,3,7]

  | Name           | Type                | Params
-------------------------------------------------------
0 | visual_encoder | SwinModel           | 86.7 M
1 | lm_model       | StableLmForCausalLM | 2.8 B 
2 | embed_tokens   | Embedding           | 128 M 
3 | lm_proj        | Linear              | 2.6 M 
4 | layer_norm     | LayerNorm           | 5.1 K 
-------------------------------------------------------
2.6 M     Trainable params
2.9 B     Non-trainable params
2.9 B     Total params
11,539.262Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 12,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0:  50%|     | 1/2 [00:07<00:07,  7.70s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:12<00:00,  6.25s/it][rank0]: Traceback (most recent call last):
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank0]:     main()
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank0]:     train(args)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank0]:     trainer.fit(model, datamodule=dm)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank0]:     results = self._run_stage()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank0]:     self._run_sanity_check()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank0]:     val_loop.run()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank0]:     return loop_run(self, *args, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank0]:     return self.on_run_end()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank0]:     self._on_evaluation_epoch_end()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank0]:     call._call_lightning_module_hook(trainer, hook_name)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank0]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank0]:     score, scores = scorer.compute_score(ref, hypo)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank0]:     stat = self._stat(res[i][0], gts[i])
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank0]:     self.meteor_p.stdin.flush()
[rank0]: BrokenPipeError: [Errno 32] Broken pipe
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank1]:     main()
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank1]:     train(args)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank1]:     trainer.fit(model, datamodule=dm)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank1]:     return function(*args, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank1]:     results = self._run_stage()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank1]:     self._run_sanity_check()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank1]:     val_loop.run()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank1]:     return loop_run(self, *args, **kwargs)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank1]:     return self.on_run_end()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank1]:     self._on_evaluation_epoch_end()
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank1]:     call._call_lightning_module_hook(trainer, hook_name)
[rank1]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank1]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank1]:     score, scores = scorer.compute_score(ref, hypo)
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank1]:     stat = self._stat(res[i][0], gts[i])
[rank1]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank1]:     self.meteor_p.stdin.flush()
[rank1]: BrokenPipeError: [Errno 32] Broken pipe
                                                                           [rank2]: Traceback (most recent call last):
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank2]:     main()
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank2]:     train(args)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank2]:     trainer.fit(model, datamodule=dm)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank2]:     call._call_and_handle_interrupt(
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank2]:     return function(*args, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank2]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank2]:     results = self._run_stage()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank2]:     self._run_sanity_check()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank2]:     val_loop.run()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank2]:     return loop_run(self, *args, **kwargs)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank2]:     return self.on_run_end()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank2]:     self._on_evaluation_epoch_end()
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank2]:     call._call_lightning_module_hook(trainer, hook_name)
[rank2]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank2]:     output = fn(*args, **kwargs)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank2]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank2]:     score, scores = scorer.compute_score(ref, hypo)
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank2]:     stat = self._stat(res[i][0], gts[i])
[rank2]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank2]:     self.meteor_p.stdin.flush()
[rank2]: BrokenPipeError: [Errno 32] Broken pipe
[rank0]:[W623 11:45:52.006295759 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank3]:     main()
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank3]:     train(args)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank3]:     trainer.fit(model, datamodule=dm)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank3]:     return function(*args, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank3]:     results = self._run_stage()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank3]:     self._run_sanity_check()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank3]:     val_loop.run()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank3]:     return loop_run(self, *args, **kwargs)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank3]:     return self.on_run_end()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank3]:     self._on_evaluation_epoch_end()
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank3]:     call._call_lightning_module_hook(trainer, hook_name)
[rank3]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank3]:     output = fn(*args, **kwargs)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank3]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank3]:     score, scores = scorer.compute_score(ref, hypo)
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank3]:     stat = self._stat(res[i][0], gts[i])
[rank3]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank3]:     self.meteor_p.stdin.flush()
[rank3]: BrokenPipeError: [Errno 32] Broken pipe
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 4,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 4,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
[rank: 2] Global seed set to 42
[rank: 1] Global seed set to 42
[rank: 3] Global seed set to 42
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,3,7]

  | Name           | Type                | Params
-------------------------------------------------------
0 | visual_encoder | SwinModel           | 86.7 M
1 | lm_model       | StableLmForCausalLM | 2.8 B 
2 | embed_tokens   | Embedding           | 128 M 
3 | lm_proj        | Linear              | 2.6 M 
4 | layer_norm     | LayerNorm           | 5.1 K 
-------------------------------------------------------
2.6 M     Trainable params
2.9 B     Non-trainable params
2.9 B     Total params
11,539.262Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 4,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 4,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 4,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 4,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 4,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 4,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0:  50%|     | 1/2 [00:06<00:06,  6.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:09<00:00,  4.76s/it][rank0]: Traceback (most recent call last):
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 51, in <module>
[rank0]:     main()
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 47, in main
[rank0]:     train(args)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/train.py", line 40, in train
[rank0]:     trainer.fit(model, datamodule=dm)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
[rank0]:     results = self._run_stage()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1014, in _run_stage
[rank0]:     self._run_sanity_check()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1043, in _run_sanity_check
[rank0]:     val_loop.run()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 177, in _decorator
[rank0]:     return loop_run(self, *args, **kwargs)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 122, in run
[rank0]:     return self.on_run_end()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 244, in on_run_end
[rank0]:     self._on_evaluation_epoch_end()
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 326, in _on_evaluation_epoch_end
[rank0]:     call._call_lightning_module_hook(trainer, hook_name)
[rank0]:   File "/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 289, in on_validation_epoch_end
[rank0]:     eval_res = self.score(ref=ref,hypo=hypo)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/models/R2GenGPT.py", line 112, in score
[rank0]:     score, scores = scorer.compute_score(ref, hypo)
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 77, in compute_score
[rank0]:     stat = self._stat(res[i][0], gts[i])
[rank0]:   File "/home/eechengyang/Code/R2GenGPT/evalcap/meteor/meteor.py", line 108, in _stat
[rank0]:     self.meteor_p.stdin.flush()
[rank0]: BrokenPipeError: [Errno 32] Broken pipe
                                                                           [rank0]:[W623 11:47:17.419367683 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 4,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 4,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
[rank: 1] Global seed set to 42
[rank: 2] Global seed set to 42
[rank: 3] Global seed set to 42
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,3,7]

  | Name           | Type                | Params
-------------------------------------------------------
0 | visual_encoder | SwinModel           | 86.7 M
1 | lm_model       | StableLmForCausalLM | 2.8 B 
2 | embed_tokens   | Embedding           | 128 M 
3 | lm_proj        | Linear              | 2.6 M 
4 | layer_norm     | LayerNorm           | 5.1 K 
-------------------------------------------------------
2.6 M     Trainable params
2.9 B     Non-trainable params
2.9 B     Total params
11,539.262Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 4,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 4,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 4,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 4,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 4,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 4,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Sanity Checking DataLoader 0:  50%|     | 1/2 [00:06<00:06,  6.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:09<00:00,  4.65s/it]                                                                           {'Bleu_1': 0.07156308851210628, 'Bleu_2': 0.028652948455686698, 'Bleu_3': 0.011681818903418939, 'Bleu_4': 1.3316216950887779e-06, 'ROUGE_L': np.float64(0.056192953858787684), 'CIDEr': np.float64(0.0007559613404394353)}
Sanity Checking DataLoader 0: 100%|| 2/2 [00:09<00:00,  4.71s/it]                                                                           Saving checkpoint at step 0 to ./save/mimic_cxr/v1_shallow/checkpoints/checkpoint_epoch0_step0_bleu0.000001_cider0.000756.pth.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:09<00:00,  4.71s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
                                                                           huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/16924 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/16924 [00:00<?, ?it/s] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Epoch 0:   0%|          | 1/16924 [00:02<10:19:54,  2.20s/it]Epoch 0:   0%|          | 1/16924 [00:02<13:47:07,  2.93s/it, v_num=4, loss=3.370]Epoch 0:   0%|          | 2/16924 [00:03<7:19:13,  1.56s/it, v_num=4, loss=3.370] Epoch 0:   0%|          | 2/16924 [00:03<8:02:01,  1.71s/it, v_num=4, loss=3.470]Epoch 0:   0%|          | 3/16924 [00:03<5:38:12,  1.20s/it, v_num=4, loss=3.470]Epoch 0:   0%|          | 3/16924 [00:03<6:06:18,  1.30s/it, v_num=4, loss=3.020]Epoch 0:   0%|          | 4/16924 [00:04<4:47:28,  1.02s/it, v_num=4, loss=3.020]Epoch 0:   0%|          | 4/16924 [00:04<5:08:45,  1.09s/it, v_num=4, loss=3.360]Epoch 0:   0%|          | 5/16924 [00:04<4:17:02,  1.10it/s, v_num=4, loss=3.360]Epoch 0:   0%|          | 5/16924 [00:04<4:33:58,  1.03it/s, v_num=4, loss=2.950]Epoch 0:   0%|          | 6/16924 [00:05<3:56:40,  1.19it/s, v_num=4, loss=2.950]Epoch 0:   0%|          | 6/16924 [00:05<4:11:18,  1.12it/s, v_num=4, loss=2.350]Epoch 0:   0%|          | 7/16924 [00:05<3:42:36,  1.27it/s, v_num=4, loss=2.350]Epoch 0:   0%|          | 7/16924 [00:05<3:54:57,  1.20it/s, v_num=4, loss=2.950]Epoch 0:   0%|          | 8/16924 [00:06<3:31:50,  1.33it/s, v_num=4, loss=2.950]Epoch 0:   0%|          | 8/16924 [00:06<3:42:25,  1.27it/s, v_num=4, loss=2.370]Epoch 0:   0%|          | 9/16924 [00:06<3:23:17,  1.39it/s, v_num=4, loss=2.370]Epoch 0:   0%|          | 9/16924 [00:06<3:32:34,  1.33it/s, v_num=4, loss=2.910]Epoch 0:   0%|          | 10/16924 [00:06<3:16:17,  1.44it/s, v_num=4, loss=2.910]Epoch 0:   0%|          | 10/16924 [00:07<3:24:43,  1.38it/s, v_num=4, loss=2.060]Epoch 0:   0%|          | 11/16924 [00:07<3:10:39,  1.48it/s, v_num=4, loss=2.060]Epoch 0:   0%|          | 11/16924 [00:07<3:18:19,  1.42it/s, v_num=4, loss=2.550]Epoch 0:   0%|          | 12/16924 [00:07<3:05:59,  1.52it/s, v_num=4, loss=2.550]Epoch 0:   0%|          | 12/16924 [00:08<3:13:00,  1.46it/s, v_num=4, loss=2.750]Epoch 0:   0%|          | 13/16924 [00:08<3:01:59,  1.55it/s, v_num=4, loss=2.750]Epoch 0:   0%|          | 13/16924 [00:08<3:08:36,  1.49it/s, v_num=4, loss=2.540]Epoch 0:   0%|          | 14/16924 [00:08<2:58:40,  1.58it/s, v_num=4, loss=2.540]Epoch 0:   0%|          | 14/16924 [00:09<3:04:38,  1.53it/s, v_num=4, loss=2.370]Epoch 0:   0%|          | 15/16924 [00:09<2:55:41,  1.60it/s, v_num=4, loss=2.370]Epoch 0:   0%|          | 15/16924 [00:09<3:01:19,  1.55it/s, v_num=4, loss=2.810]Epoch 0:   0%|          | 16/16924 [00:09<2:53:05,  1.63it/s, v_num=4, loss=2.810]Epoch 0:   0%|          | 16/16924 [00:10<2:58:23,  1.58it/s, v_num=4, loss=2.750]Epoch 0:   0%|          | 17/16924 [00:10<2:51:05,  1.65it/s, v_num=4, loss=2.750]Epoch 0:   0%|          | 17/16924 [00:10<2:55:58,  1.60it/s, v_num=4, loss=2.190]Epoch 0:   0%|          | 18/16924 [00:10<2:48:57,  1.67it/s, v_num=4, loss=2.190]Epoch 0:   0%|          | 18/16924 [00:11<2:53:43,  1.62it/s, v_num=4, loss=2.460]Epoch 0:   0%|          | 19/16924 [00:11<2:47:09,  1.69it/s, v_num=4, loss=2.460]Epoch 0:   0%|          | 19/16924 [00:11<2:51:40,  1.64it/s, v_num=4, loss=2.330]Epoch 0:   0%|          | 20/16924 [00:11<2:45:32,  1.70it/s, v_num=4, loss=2.330]Epoch 0:   0%|          | 20/16924 [00:12<2:49:50,  1.66it/s, v_num=4, loss=2.470]Epoch 0:   0%|          | 21/16924 [00:12<2:44:07,  1.72it/s, v_num=4, loss=2.470]Epoch 0:   0%|          | 21/16924 [00:12<2:48:15,  1.67it/s, v_num=4, loss=2.240]Epoch 0:   0%|          | 22/16924 [00:12<2:42:55,  1.73it/s, v_num=4, loss=2.240]Epoch 0:   0%|          | 22/16924 [00:13<2:46:54,  1.69it/s, v_num=4, loss=2.170]Epoch 0:   0%|          | 23/16924 [00:13<2:41:48,  1.74it/s, v_num=4, loss=2.170]Epoch 0:   0%|          | 23/16924 [00:13<2:45:29,  1.70it/s, v_num=4, loss=2.140]Epoch 0:   0%|          | 24/16924 [00:13<2:40:36,  1.75it/s, v_num=4, loss=2.140]Epoch 0:   0%|          | 24/16924 [00:13<2:44:12,  1.72it/s, v_num=4, loss=2.640]Epoch 0:   0%|          | 25/16924 [00:14<2:39:40,  1.76it/s, v_num=4, loss=2.640]Epoch 0:   0%|          | 25/16924 [00:14<2:43:08,  1.73it/s, v_num=4, loss=2.210]Epoch 0:   0%|          | 26/16924 [00:14<2:38:47,  1.77it/s, v_num=4, loss=2.210]Epoch 0:   0%|          | 26/16924 [00:14<2:42:08,  1.74it/s, v_num=4, loss=2.310]Epoch 0:   0%|          | 27/16924 [00:15<2:37:56,  1.78it/s, v_num=4, loss=2.310]Epoch 0:   0%|          | 27/16924 [00:15<2:41:05,  1.75it/s, v_num=4, loss=2.050]Epoch 0:   0%|          | 28/16924 [00:15<2:37:04,  1.79it/s, v_num=4, loss=2.050]Epoch 0:   0%|          | 28/16924 [00:15<2:40:09,  1.76it/s, v_num=4, loss=2.260]Epoch 0:   0%|          | 29/16924 [00:16<2:36:23,  1.80it/s, v_num=4, loss=2.260]Epoch 0:   0%|          | 29/16924 [00:16<2:39:18,  1.77it/s, v_num=4, loss=1.900]Epoch 0:   0%|          | 30/16924 [00:16<2:35:37,  1.81it/s, v_num=4, loss=1.900]Epoch 0:   0%|          | 30/16924 [00:16<2:38:25,  1.78it/s, v_num=4, loss=2.480]Epoch 0:   0%|          | 31/16924 [00:17<2:34:54,  1.82it/s, v_num=4, loss=2.480]Epoch 0:   0%|          | 31/16924 [00:17<2:37:42,  1.79it/s, v_num=4, loss=2.280]Epoch 0:   0%|          | 32/16924 [00:17<2:34:21,  1.82it/s, v_num=4, loss=2.280]Epoch 0:   0%|          | 32/16924 [00:17<2:37:00,  1.79it/s, v_num=4, loss=2.070]Epoch 0:   0%|          | 33/16924 [00:18<2:33:46,  1.83it/s, v_num=4, loss=2.070]Epoch 0:   0%|          | 33/16924 [00:18<2:36:20,  1.80it/s, v_num=4, loss=1.690]Epoch 0:   0%|          | 34/16924 [00:18<2:33:13,  1.84it/s, v_num=4, loss=1.690]Epoch 0:   0%|          | 34/16924 [00:18<2:35:43,  1.81it/s, v_num=4, loss=2.010]Epoch 0:   0%|          | 35/16924 [00:18<2:32:40,  1.84it/s, v_num=4, loss=2.010]Epoch 0:   0%|          | 35/16924 [00:19<2:35:04,  1.82it/s, v_num=4, loss=1.720]Epoch 0:   0%|          | 36/16924 [00:19<2:32:07,  1.85it/s, v_num=4, loss=1.720]Epoch 0:   0%|          | 36/16924 [00:19<2:34:31,  1.82it/s, v_num=4, loss=1.620]Epoch 0:   0%|          | 37/16924 [00:19<2:31:39,  1.86it/s, v_num=4, loss=1.620]Epoch 0:   0%|          | 37/16924 [00:20<2:33:59,  1.83it/s, v_num=4, loss=2.120]Epoch 0:   0%|          | 38/16924 [00:20<2:31:13,  1.86it/s, v_num=4, loss=2.120]Epoch 0:   0%|          | 38/16924 [00:20<2:33:27,  1.83it/s, v_num=4, loss=2.130]Epoch 0:   0%|          | 39/16924 [00:20<2:30:48,  1.87it/s, v_num=4, loss=2.130]Epoch 0:   0%|          | 39/16924 [00:21<2:32:59,  1.84it/s, v_num=4, loss=1.730]Epoch 0:   0%|          | 40/16924 [00:21<2:30:21,  1.87it/s, v_num=4, loss=1.730]Epoch 0:   0%|          | 40/16924 [00:21<2:32:33,  1.84it/s, v_num=4, loss=2.060]Epoch 0:   0%|          | 41/16924 [00:21<2:30:00,  1.88it/s, v_num=4, loss=2.060]Epoch 0:   0%|          | 41/16924 [00:22<2:32:09,  1.85it/s, v_num=4, loss=2.330]Epoch 0:   0%|          | 42/16924 [00:22<2:29:40,  1.88it/s, v_num=4, loss=2.330]Epoch 0:   0%|          | 42/16924 [00:22<2:31:47,  1.85it/s, v_num=4, loss=1.190]Epoch 0:   0%|          | 43/16924 [00:22<2:29:22,  1.88it/s, v_num=4, loss=1.190]Epoch 0:   0%|          | 43/16924 [00:23<2:31:25,  1.86it/s, v_num=4, loss=2.690]Epoch 0:   0%|          | 44/16924 [00:23<2:29:04,  1.89it/s, v_num=4, loss=2.690]Epoch 0:   0%|          | 44/16924 [00:23<2:30:48,  1.87it/s, v_num=4, loss=2.130]Epoch 0:   0%|          | 45/16924 [00:23<2:28:33,  1.89it/s, v_num=4, loss=2.130]Epoch 0:   0%|          | 45/16924 [00:24<2:30:27,  1.87it/s, v_num=4, loss=2.670]Epoch 0:   0%|          | 46/16924 [00:24<2:28:16,  1.90it/s, v_num=4, loss=2.670]Epoch 0:   0%|          | 46/16924 [00:24<2:30:11,  1.87it/s, v_num=4, loss=2.130]Epoch 0:   0%|          | 47/16924 [00:24<2:28:01,  1.90it/s, v_num=4, loss=2.130]Epoch 0:   0%|          | 47/16924 [00:25<2:29:50,  1.88it/s, v_num=4, loss=2.070]Epoch 0:   0%|          | 48/16924 [00:25<2:27:43,  1.90it/s, v_num=4, loss=2.070]Epoch 0:   0%|          | 48/16924 [00:25<2:29:31,  1.88it/s, v_num=4, loss=1.820]Epoch 0:   0%|          | 49/16924 [00:25<2:27:27,  1.91it/s, v_num=4, loss=1.820]Epoch 0:   0%|          | 49/16924 [00:25<2:29:13,  1.88it/s, v_num=4, loss=1.970]Epoch 0:   0%|          | 50/16924 [00:26<2:27:12,  1.91it/s, v_num=4, loss=1.970]Epoch 0:   0%|          | 50/16924 [00:26<2:28:54,  1.89it/s, v_num=4, loss=1.760]Epoch 0:   0%|          | 51/16924 [00:26<2:26:59,  1.91it/s, v_num=4, loss=1.760]Epoch 0:   0%|          | 51/16924 [00:26<2:28:40,  1.89it/s, v_num=4, loss=2.190]Epoch 0:   0%|          | 52/16924 [00:27<2:26:46,  1.92it/s, v_num=4, loss=2.190]Epoch 0:   0%|          | 52/16924 [00:27<2:28:24,  1.89it/s, v_num=4, loss=1.700]Epoch 0:   0%|          | 53/16924 [00:27<2:26:33,  1.92it/s, v_num=4, loss=1.700]Epoch 0:   0%|          | 53/16924 [00:27<2:28:08,  1.90it/s, v_num=4, loss=1.910]Epoch 0:   0%|          | 54/16924 [00:28<2:26:17,  1.92it/s, v_num=4, loss=1.910]Epoch 0:   0%|          | 54/16924 [00:28<2:27:55,  1.90it/s, v_num=4, loss=2.430]Epoch 0:   0%|          | 55/16924 [00:28<2:26:07,  1.92it/s, v_num=4, loss=2.430]Epoch 0:   0%|          | 55/16924 [00:28<2:27:42,  1.90it/s, v_num=4, loss=1.740]Epoch 0:   0%|          | 56/16924 [00:29<2:25:55,  1.93it/s, v_num=4, loss=1.740]Epoch 0:   0%|          | 56/16924 [00:29<2:27:29,  1.91it/s, v_num=4, loss=1.780]Epoch 0:   0%|          | 57/16924 [00:29<2:25:45,  1.93it/s, v_num=4, loss=1.780]Epoch 0:   0%|          | 57/16924 [00:29<2:27:14,  1.91it/s, v_num=4, loss=1.630]Epoch 0:   0%|          | 58/16924 [00:30<2:25:32,  1.93it/s, v_num=4, loss=1.630]Epoch 0:   0%|          | 58/16924 [00:30<2:26:59,  1.91it/s, v_num=4, loss=1.290]Epoch 0:   0%|          | 59/16924 [00:30<2:25:20,  1.93it/s, v_num=4, loss=1.290]Epoch 0:   0%|          | 59/16924 [00:30<2:26:47,  1.91it/s, v_num=4, loss=1.810]Epoch 0:   0%|          | 60/16924 [00:30<2:25:08,  1.94it/s, v_num=4, loss=1.810]Epoch 0:   0%|          | 60/16924 [00:31<2:26:32,  1.92it/s, v_num=4, loss=1.370]Epoch 0:   0%|          | 61/16924 [00:31<2:24:55,  1.94it/s, v_num=4, loss=1.370]Epoch 0:   0%|          | 61/16924 [00:31<2:26:20,  1.92it/s, v_num=4, loss=1.750]Epoch 0:   0%|          | 62/16924 [00:31<2:24:45,  1.94it/s, v_num=4, loss=1.750]Epoch 0:   0%|          | 62/16924 [00:32<2:26:08,  1.92it/s, v_num=4, loss=2.200]Epoch 0:   0%|          | 63/16924 [00:32<2:24:35,  1.94it/s, v_num=4, loss=2.200]Epoch 0:   0%|          | 63/16924 [00:32<2:25:55,  1.93it/s, v_num=4, loss=2.060]Epoch 0:   0%|          | 64/16924 [00:32<2:24:24,  1.95it/s, v_num=4, loss=2.060]Epoch 0:   0%|          | 64/16924 [00:33<2:25:44,  1.93it/s, v_num=4, loss=2.030]Epoch 0:   0%|          | 65/16924 [00:33<2:24:14,  1.95it/s, v_num=4, loss=2.030]Epoch 0:   0%|          | 65/16924 [00:33<2:25:33,  1.93it/s, v_num=4, loss=2.200]Epoch 0:   0%|          | 66/16924 [00:33<2:24:05,  1.95it/s, v_num=4, loss=2.200]Epoch 0:   0%|          | 66/16924 [00:34<2:25:21,  1.93it/s, v_num=4, loss=2.140]Epoch 0:   0%|          | 67/16924 [00:34<2:23:53,  1.95it/s, v_num=4, loss=2.140]Epoch 0:   0%|          | 67/16924 [00:34<2:25:11,  1.93it/s, v_num=4, loss=2.250]Epoch 0:   0%|          | 68/16924 [00:34<2:23:45,  1.95it/s, v_num=4, loss=2.250]Epoch 0:   0%|          | 68/16924 [00:35<2:25:00,  1.94it/s, v_num=4, loss=1.430]Epoch 0:   0%|          | 69/16924 [00:35<2:23:35,  1.96it/s, v_num=4, loss=1.430]Epoch 0:   0%|          | 69/16924 [00:35<2:24:48,  1.94it/s, v_num=4, loss=2.230]Epoch 0:   0%|          | 70/16924 [00:35<2:23:25,  1.96it/s, v_num=4, loss=2.230]Epoch 0:   0%|          | 70/16924 [00:36<2:24:39,  1.94it/s, v_num=4, loss=2.280]Epoch 0:   0%|          | 71/16924 [00:36<2:23:17,  1.96it/s, v_num=4, loss=2.280]Epoch 0:   0%|          | 71/16924 [00:36<2:24:29,  1.94it/s, v_num=4, loss=2.020]Epoch 0:   0%|          | 72/16924 [00:36<2:23:08,  1.96it/s, v_num=4, loss=2.020]Epoch 0:   0%|          | 72/16924 [00:37<2:24:26,  1.94it/s, v_num=4, loss=2.200]Epoch 0:   0%|          | 73/16924 [00:37<2:23:06,  1.96it/s, v_num=4, loss=2.200]Epoch 0:   0%|          | 73/16924 [00:37<2:24:17,  1.95it/s, v_num=4, loss=2.490]Epoch 0:   0%|          | 74/16924 [00:37<2:22:58,  1.96it/s, v_num=4, loss=2.490]Epoch 0:   0%|          | 74/16924 [00:37<2:24:11,  1.95it/s, v_num=4, loss=2.230]Epoch 0:   0%|          | 75/16924 [00:38<2:22:53,  1.97it/s, v_num=4, loss=2.230]Epoch 0:   0%|          | 75/16924 [00:38<2:24:00,  1.95it/s, v_num=4, loss=1.270]Epoch 0:   0%|          | 76/16924 [00:38<2:22:44,  1.97it/s, v_num=4, loss=1.270]Epoch 0:   0%|          | 76/16924 [00:38<2:23:53,  1.95it/s, v_num=4, loss=1.460]Epoch 0:   0%|          | 77/16924 [00:39<2:22:37,  1.97it/s, v_num=4, loss=1.460]Epoch 0:   0%|          | 77/16924 [00:39<2:23:46,  1.95it/s, v_num=4, loss=2.150]Epoch 0:   0%|          | 78/16924 [00:39<2:22:32,  1.97it/s, v_num=4, loss=2.150]Epoch 0:   0%|          | 78/16924 [00:39<2:23:39,  1.95it/s, v_num=4, loss=2.110]Epoch 0:   0%|          | 79/16924 [00:40<2:22:25,  1.97it/s, v_num=4, loss=2.110]Epoch 0:   0%|          | 79/16924 [00:40<2:23:31,  1.96it/s, v_num=4, loss=2.270]Epoch 0:   0%|          | 80/16924 [00:40<2:22:19,  1.97it/s, v_num=4, loss=2.270]Epoch 0:   0%|          | 80/16924 [00:40<2:23:24,  1.96it/s, v_num=4, loss=1.880]Epoch 0:   0%|          | 81/16924 [00:41<2:22:13,  1.97it/s, v_num=4, loss=1.880]Epoch 0:   0%|          | 81/16924 [00:41<2:23:17,  1.96it/s, v_num=4, loss=2.400]Epoch 0:   0%|          | 82/16924 [00:41<2:22:07,  1.98it/s, v_num=4, loss=2.400]Epoch 0:   0%|          | 82/16924 [00:41<2:23:12,  1.96it/s, v_num=4, loss=2.020]Epoch 0:   0%|          | 83/16924 [00:42<2:22:02,  1.98it/s, v_num=4, loss=2.020]Epoch 0:   0%|          | 83/16924 [00:42<2:23:07,  1.96it/s, v_num=4, loss=1.670]Epoch 0:   0%|          | 84/16924 [00:42<2:21:58,  1.98it/s, v_num=4, loss=1.670]Epoch 0:   0%|          | 84/16924 [00:42<2:23:00,  1.96it/s, v_num=4, loss=2.110]Epoch 0:   1%|          | 85/16924 [00:42<2:21:53,  1.98it/s, v_num=4, loss=2.110]Epoch 0:   1%|          | 85/16924 [00:43<2:22:53,  1.96it/s, v_num=4, loss=1.830]Epoch 0:   1%|          | 86/16924 [00:43<2:21:47,  1.98it/s, v_num=4, loss=1.830]Epoch 0:   1%|          | 86/16924 [00:43<2:22:47,  1.97it/s, v_num=4, loss=2.570]Epoch 0:   1%|          | 87/16924 [00:43<2:21:41,  1.98it/s, v_num=4, loss=2.570]Epoch 0:   1%|          | 87/16924 [00:44<2:22:42,  1.97it/s, v_num=4, loss=1.830]Epoch 0:   1%|          | 88/16924 [00:44<2:21:37,  1.98it/s, v_num=4, loss=1.830]Epoch 0:   1%|          | 88/16924 [00:44<2:22:35,  1.97it/s, v_num=4, loss=2.000]Epoch 0:   1%|          | 89/16924 [00:44<2:21:31,  1.98it/s, v_num=4, loss=2.000]Epoch 0:   1%|          | 89/16924 [00:45<2:22:30,  1.97it/s, v_num=4, loss=1.390]Epoch 0:   1%|          | 90/16924 [00:45<2:21:26,  1.98it/s, v_num=4, loss=1.390]Epoch 0:   1%|          | 90/16924 [00:45<2:22:23,  1.97it/s, v_num=4, loss=2.280]Epoch 0:   1%|          | 91/16924 [00:45<2:21:20,  1.98it/s, v_num=4, loss=2.280]Epoch 0:   1%|          | 91/16924 [00:46<2:22:17,  1.97it/s, v_num=4, loss=2.070]Epoch 0:   1%|          | 92/16924 [00:46<2:21:15,  1.99it/s, v_num=4, loss=2.070]Epoch 0:   1%|          | 92/16924 [00:46<2:22:10,  1.97it/s, v_num=4, loss=2.030]Epoch 0:   1%|          | 93/16924 [00:46<2:21:09,  1.99it/s, v_num=4, loss=2.030]Epoch 0:   1%|          | 93/16924 [00:47<2:22:04,  1.97it/s, v_num=4, loss=1.880]Epoch 0:   1%|          | 94/16924 [00:47<2:21:04,  1.99it/s, v_num=4, loss=1.880]Epoch 0:   1%|          | 94/16924 [00:47<2:21:58,  1.98it/s, v_num=4, loss=2.360]Epoch 0:   1%|          | 95/16924 [00:47<2:20:59,  1.99it/s, v_num=4, loss=2.360]Epoch 0:   1%|          | 95/16924 [00:48<2:21:53,  1.98it/s, v_num=4, loss=1.780]Epoch 0:   1%|          | 96/16924 [00:48<2:20:54,  1.99it/s, v_num=4, loss=1.780]Epoch 0:   1%|          | 96/16924 [00:48<2:21:48,  1.98it/s, v_num=4, loss=1.800]Epoch 0:   1%|          | 97/16924 [00:48<2:20:50,  1.99it/s, v_num=4, loss=1.800]Epoch 0:   1%|          | 97/16924 [00:49<2:21:42,  1.98it/s, v_num=4, loss=1.840]Epoch 0:   1%|          | 98/16924 [00:49<2:20:45,  1.99it/s, v_num=4, loss=1.840]Epoch 0:   1%|          | 98/16924 [00:49<2:21:37,  1.98it/s, v_num=4, loss=1.280]Epoch 0:   1%|          | 99/16924 [00:49<2:20:40,  1.99it/s, v_num=4, loss=1.280]Epoch 0:   1%|          | 99/16924 [00:49<2:21:33,  1.98it/s, v_num=4, loss=1.900]Epoch 0:   1%|          | 100/16924 [00:50<2:20:38,  1.99it/s, v_num=4, loss=1.900]Epoch 0:   1%|          | 100/16924 [00:50<2:21:29,  1.98it/s, v_num=4, loss=1.470]Epoch 0:   1%|          | 101/16924 [00:50<2:20:36,  1.99it/s, v_num=4, loss=1.470]Epoch 0:   1%|          | 101/16924 [00:50<2:21:26,  1.98it/s, v_num=4, loss=1.760]Epoch 0:   1%|          | 102/16924 [00:51<2:20:31,  2.00it/s, v_num=4, loss=1.760]Epoch 0:   1%|          | 102/16924 [00:51<2:21:21,  1.98it/s, v_num=4, loss=1.930]Epoch 0:   1%|          | 103/16924 [00:51<2:20:27,  2.00it/s, v_num=4, loss=1.930]Epoch 0:   1%|          | 103/16924 [00:51<2:21:17,  1.98it/s, v_num=4, loss=2.080]Epoch 0:   1%|          | 104/16924 [00:52<2:20:24,  2.00it/s, v_num=4, loss=2.080]Epoch 0:   1%|          | 104/16924 [00:52<2:21:13,  1.99it/s, v_num=4, loss=1.620]Epoch 0:   1%|          | 105/16924 [00:52<2:20:20,  2.00it/s, v_num=4, loss=1.620]Epoch 0:   1%|          | 105/16924 [00:52<2:21:04,  1.99it/s, v_num=4, loss=1.620]Epoch 0:   1%|          | 106/16924 [00:53<2:20:12,  2.00it/s, v_num=4, loss=1.620]Epoch 0:   1%|          | 106/16924 [00:53<2:20:58,  1.99it/s, v_num=4, loss=2.740]Epoch 0:   1%|          | 107/16924 [00:53<2:20:06,  2.00it/s, v_num=4, loss=2.740]Epoch 0:   1%|          | 107/16924 [00:53<2:20:53,  1.99it/s, v_num=4, loss=1.460]Epoch 0:   1%|          | 108/16924 [00:53<2:20:01,  2.00it/s, v_num=4, loss=1.460]Epoch 0:   1%|          | 108/16924 [00:54<2:20:49,  1.99it/s, v_num=4, loss=2.120]Epoch 0:   1%|          | 109/16924 [00:54<2:19:58,  2.00it/s, v_num=4, loss=2.120]Epoch 0:   1%|          | 109/16924 [00:54<2:20:44,  1.99it/s, v_num=4, loss=1.910]Epoch 0:   1%|          | 110/16924 [00:54<2:19:53,  2.00it/s, v_num=4, loss=1.910]Epoch 0:   1%|          | 110/16924 [00:55<2:20:39,  1.99it/s, v_num=4, loss=2.480]Epoch 0:   1%|          | 111/16924 [00:55<2:19:48,  2.00it/s, v_num=4, loss=2.480]Epoch 0:   1%|          | 111/16924 [00:55<2:20:34,  1.99it/s, v_num=4, loss=1.920]Epoch 0:   1%|          | 112/16924 [00:55<2:19:44,  2.01it/s, v_num=4, loss=1.920]Epoch 0:   1%|          | 112/16924 [00:56<2:20:30,  1.99it/s, v_num=4, loss=1.420]Epoch 0:   1%|          | 113/16924 [00:56<2:19:41,  2.01it/s, v_num=4, loss=1.420]Epoch 0:   1%|          | 113/16924 [00:56<2:20:26,  1.99it/s, v_num=4, loss=2.000]Epoch 0:   1%|          | 114/16924 [00:56<2:19:37,  2.01it/s, v_num=4, loss=2.000]Epoch 0:   1%|          | 114/16924 [00:57<2:20:22,  2.00it/s, v_num=4, loss=1.670]Epoch 0:   1%|          | 115/16924 [00:57<2:19:33,  2.01it/s, v_num=4, loss=1.670]Epoch 0:   1%|          | 115/16924 [00:57<2:20:18,  2.00it/s, v_num=4, loss=2.680]Epoch 0:   1%|          | 116/16924 [00:57<2:19:29,  2.01it/s, v_num=4, loss=2.680]Epoch 0:   1%|          | 116/16924 [00:58<2:20:13,  2.00it/s, v_num=4, loss=2.090]Epoch 0:   1%|          | 117/16924 [00:58<2:19:27,  2.01it/s, v_num=4, loss=2.090]Epoch 0:   1%|          | 117/16924 [00:58<2:20:09,  2.00it/s, v_num=4, loss=2.060]Epoch 0:   1%|          | 118/16924 [00:58<2:19:22,  2.01it/s, v_num=4, loss=2.060]Epoch 0:   1%|          | 118/16924 [00:59<2:20:05,  2.00it/s, v_num=4, loss=1.780]Epoch 0:   1%|          | 119/16924 [00:59<2:19:19,  2.01it/s, v_num=4, loss=1.780]Epoch 0:   1%|          | 119/16924 [00:59<2:20:02,  2.00it/s, v_num=4, loss=1.780]Epoch 0:   1%|          | 120/16924 [00:59<2:19:16,  2.01it/s, v_num=4, loss=1.780]Epoch 0:   1%|          | 120/16924 [00:59<2:19:58,  2.00it/s, v_num=4, loss=1.620]Epoch 0:   1%|          | 121/16924 [01:00<2:19:12,  2.01it/s, v_num=4, loss=1.620]Epoch 0:   1%|          | 121/16924 [01:00<2:19:55,  2.00it/s, v_num=4, loss=1.940]Epoch 0:   1%|          | 122/16924 [01:00<2:19:10,  2.01it/s, v_num=4, loss=1.940]Epoch 0:   1%|          | 122/16924 [01:00<2:19:52,  2.00it/s, v_num=4, loss=1.870]Epoch 0:   1%|          | 123/16924 [01:01<2:19:08,  2.01it/s, v_num=4, loss=1.870]Epoch 0:   1%|          | 123/16924 [01:01<2:19:49,  2.00it/s, v_num=4, loss=2.290]Epoch 0:   1%|          | 124/16924 [01:01<2:19:04,  2.01it/s, v_num=4, loss=2.290]Epoch 0:   1%|          | 124/16924 [01:01<2:19:45,  2.00it/s, v_num=4, loss=1.890]Epoch 0:   1%|          | 125/16924 [01:02<2:19:01,  2.01it/s, v_num=4, loss=1.890]Epoch 0:   1%|          | 125/16924 [01:02<2:19:40,  2.00it/s, v_num=4, loss=1.370]Epoch 0:   1%|          | 126/16924 [01:02<2:18:56,  2.01it/s, v_num=4, loss=1.370]Epoch 0:   1%|          | 126/16924 [01:02<2:19:36,  2.01it/s, v_num=4, loss=1.470]Epoch 0:   1%|          | 127/16924 [01:03<2:18:53,  2.02it/s, v_num=4, loss=1.470]Epoch 0:   1%|          | 127/16924 [01:03<2:19:34,  2.01it/s, v_num=4, loss=2.160]Epoch 0:   1%|          | 128/16924 [01:03<2:18:51,  2.02it/s, v_num=4, loss=2.160]Epoch 0:   1%|          | 128/16924 [01:03<2:19:31,  2.01it/s, v_num=4, loss=2.160]Epoch 0:   1%|          | 129/16924 [01:03<2:18:47,  2.02it/s, v_num=4, loss=2.160]Epoch 0:   1%|          | 129/16924 [01:04<2:19:28,  2.01it/s, v_num=4, loss=2.170]Epoch 0:   1%|          | 130/16924 [01:04<2:18:45,  2.02it/s, v_num=4, loss=2.170]Epoch 0:   1%|          | 130/16924 [01:04<2:19:24,  2.01it/s, v_num=4, loss=1.770]Epoch 0:   1%|          | 131/16924 [01:04<2:18:42,  2.02it/s, v_num=4, loss=1.770]Epoch 0:   1%|          | 131/16924 [01:05<2:19:22,  2.01it/s, v_num=4, loss=1.990]Epoch 0:   1%|          | 132/16924 [01:05<2:18:40,  2.02it/s, v_num=4, loss=1.990]Epoch 0:   1%|          | 132/16924 [01:05<2:19:18,  2.01it/s, v_num=4, loss=2.180]Epoch 0:   1%|          | 133/16924 [01:05<2:18:36,  2.02it/s, v_num=4, loss=2.180]Epoch 0:   1%|          | 133/16924 [01:06<2:19:15,  2.01it/s, v_num=4, loss=1.920]Epoch 0:   1%|          | 134/16924 [01:06<2:18:33,  2.02it/s, v_num=4, loss=1.920]Epoch 0:   1%|          | 134/16924 [01:06<2:19:13,  2.01it/s, v_num=4, loss=2.120]Epoch 0:   1%|          | 135/16924 [01:06<2:18:32,  2.02it/s, v_num=4, loss=2.120]Epoch 0:   1%|          | 135/16924 [01:07<2:19:10,  2.01it/s, v_num=4, loss=1.840]Epoch 0:   1%|          | 136/16924 [01:07<2:18:29,  2.02it/s, v_num=4, loss=1.840]Epoch 0:   1%|          | 136/16924 [01:07<2:19:04,  2.01it/s, v_num=4, loss=1.810]Epoch 0:   1%|          | 137/16924 [01:07<2:18:24,  2.02it/s, v_num=4, loss=1.810]Epoch 0:   1%|          | 137/16924 [01:08<2:19:01,  2.01it/s, v_num=4, loss=2.260]Epoch 0:   1%|          | 138/16924 [01:08<2:18:22,  2.02it/s, v_num=4, loss=2.260]Epoch 0:   1%|          | 138/16924 [01:08<2:18:59,  2.01it/s, v_num=4, loss=2.090]Epoch 0:   1%|          | 139/16924 [01:08<2:18:19,  2.02it/s, v_num=4, loss=2.090]Epoch 0:   1%|          | 139/16924 [01:09<2:18:56,  2.01it/s, v_num=4, loss=1.980]Epoch 0:   1%|          | 140/16924 [01:09<2:18:16,  2.02it/s, v_num=4, loss=1.980]Epoch 0:   1%|          | 140/16924 [01:09<2:18:53,  2.01it/s, v_num=4, loss=1.820]Epoch 0:   1%|          | 141/16924 [01:09<2:18:14,  2.02it/s, v_num=4, loss=1.820]Epoch 0:   1%|          | 141/16924 [01:09<2:18:50,  2.01it/s, v_num=4, loss=1.880]Epoch 0:   1%|          | 142/16924 [01:10<2:18:12,  2.02it/s, v_num=4, loss=1.880]Epoch 0:   1%|          | 142/16924 [01:10<2:18:49,  2.01it/s, v_num=4, loss=2.350]Epoch 0:   1%|          | 143/16924 [01:10<2:18:11,  2.02it/s, v_num=4, loss=2.350]Epoch 0:   1%|          | 143/16924 [01:10<2:18:47,  2.02it/s, v_num=4, loss=2.020]Epoch 0:   1%|          | 144/16924 [01:11<2:18:09,  2.02it/s, v_num=4, loss=2.020]Epoch 0:   1%|          | 144/16924 [01:11<2:18:44,  2.02it/s, v_num=4, loss=1.740]Epoch 0:   1%|          | 145/16924 [01:11<2:18:07,  2.02it/s, v_num=4, loss=1.740]Epoch 0:   1%|          | 145/16924 [01:11<2:18:41,  2.02it/s, v_num=4, loss=2.080]Epoch 0:   1%|          | 146/16924 [01:12<2:18:04,  2.03it/s, v_num=4, loss=2.080]Epoch 0:   1%|          | 146/16924 [01:12<2:18:39,  2.02it/s, v_num=4, loss=1.690]Epoch 0:   1%|          | 147/16924 [01:12<2:18:02,  2.03it/s, v_num=4, loss=1.690]Epoch 0:   1%|          | 147/16924 [01:12<2:18:36,  2.02it/s, v_num=4, loss=1.420]Epoch 0:   1%|          | 148/16924 [01:13<2:17:59,  2.03it/s, v_num=4, loss=1.420]Epoch 0:   1%|          | 148/16924 [01:13<2:18:34,  2.02it/s, v_num=4, loss=1.830]Epoch 0:   1%|          | 149/16924 [01:13<2:17:58,  2.03it/s, v_num=4, loss=1.830]Epoch 0:   1%|          | 149/16924 [01:13<2:18:32,  2.02it/s, v_num=4, loss=1.480]Epoch 0:   1%|          | 150/16924 [01:14<2:17:56,  2.03it/s, v_num=4, loss=1.480]Epoch 0:   1%|          | 150/16924 [01:14<2:18:31,  2.02it/s, v_num=4, loss=2.030]Epoch 0:   1%|          | 151/16924 [01:14<2:17:55,  2.03it/s, v_num=4, loss=2.030]Epoch 0:   1%|          | 151/16924 [01:14<2:18:29,  2.02it/s, v_num=4, loss=2.230]Epoch 0:   1%|          | 152/16924 [01:14<2:17:53,  2.03it/s, v_num=4, loss=2.230]Epoch 0:   1%|          | 152/16924 [01:15<2:18:27,  2.02it/s, v_num=4, loss=2.460]Epoch 0:   1%|          | 153/16924 [01:15<2:17:51,  2.03it/s, v_num=4, loss=2.460]Epoch 0:   1%|          | 153/16924 [01:15<2:18:26,  2.02it/s, v_num=4, loss=2.260]Epoch 0:   1%|          | 154/16924 [01:15<2:17:50,  2.03it/s, v_num=4, loss=2.260]Epoch 0:   1%|          | 154/16924 [01:16<2:18:26,  2.02it/s, v_num=4, loss=2.410]Epoch 0:   1%|          | 155/16924 [01:16<2:17:51,  2.03it/s, v_num=4, loss=2.410]Epoch 0:   1%|          | 155/16924 [01:16<2:18:24,  2.02it/s, v_num=4, loss=1.640]Epoch 0:   1%|          | 156/16924 [01:16<2:17:49,  2.03it/s, v_num=4, loss=1.640]Epoch 0:   1%|          | 156/16924 [01:17<2:18:22,  2.02it/s, v_num=4, loss=1.680]Epoch 0:   1%|          | 157/16924 [01:17<2:17:47,  2.03it/s, v_num=4, loss=1.680]Epoch 0:   1%|          | 157/16924 [01:17<2:18:19,  2.02it/s, v_num=4, loss=2.050]Epoch 0:   1%|          | 158/16924 [01:17<2:17:45,  2.03it/s, v_num=4, loss=2.050]Epoch 0:   1%|          | 158/16924 [01:18<2:18:17,  2.02it/s, v_num=4, loss=1.970]Epoch 0:   1%|          | 159/16924 [01:18<2:17:43,  2.03it/s, v_num=4, loss=1.970]Epoch 0:   1%|          | 159/16924 [01:18<2:18:15,  2.02it/s, v_num=4, loss=1.980]Epoch 0:   1%|          | 160/16924 [01:18<2:17:40,  2.03it/s, v_num=4, loss=1.980]Epoch 0:   1%|          | 160/16924 [01:19<2:18:13,  2.02it/s, v_num=4, loss=1.710]Epoch 0:   1%|          | 161/16924 [01:19<2:17:39,  2.03it/s, v_num=4, loss=1.710]Epoch 0:   1%|          | 161/16924 [01:19<2:18:11,  2.02it/s, v_num=4, loss=1.780]Epoch 0:   1%|          | 162/16924 [01:19<2:17:37,  2.03it/s, v_num=4, loss=1.780]Epoch 0:   1%|          | 162/16924 [01:20<2:18:09,  2.02it/s, v_num=4, loss=1.700]Epoch 0:   1%|          | 163/16924 [01:20<2:17:36,  2.03it/s, v_num=4, loss=1.700]Epoch 0:   1%|          | 163/16924 [01:20<2:18:08,  2.02it/s, v_num=4, loss=1.670]Epoch 0:   1%|          | 164/16924 [01:20<2:17:34,  2.03it/s, v_num=4, loss=1.670]Epoch 0:   1%|          | 164/16924 [01:21<2:18:05,  2.02it/s, v_num=4, loss=2.050]Epoch 0:   1%|          | 165/16924 [01:21<2:17:32,  2.03it/s, v_num=4, loss=2.050]Epoch 0:   1%|          | 165/16924 [01:21<2:18:04,  2.02it/s, v_num=4, loss=2.100]Epoch 0:   1%|          | 166/16924 [01:21<2:17:31,  2.03it/s, v_num=4, loss=2.100]Epoch 0:   1%|          | 166/16924 [01:22<2:18:01,  2.02it/s, v_num=4, loss=1.500]Epoch 0:   1%|          | 167/16924 [01:22<2:17:29,  2.03it/s, v_num=4, loss=1.500]Epoch 0:   1%|          | 167/16924 [01:22<2:17:58,  2.02it/s, v_num=4, loss=1.800]Epoch 0:   1%|          | 168/16924 [01:22<2:17:26,  2.03it/s, v_num=4, loss=1.800]Epoch 0:   1%|          | 168/16924 [01:22<2:17:57,  2.02it/s, v_num=4, loss=2.280]Epoch 0:   1%|          | 169/16924 [01:23<2:17:25,  2.03it/s, v_num=4, loss=2.280]Epoch 0:   1%|          | 169/16924 [01:23<2:17:55,  2.02it/s, v_num=4, loss=2.340]Epoch 0:   1%|          | 170/16924 [01:23<2:17:23,  2.03it/s, v_num=4, loss=2.340]Epoch 0:   1%|          | 170/16924 [01:23<2:17:54,  2.02it/s, v_num=4, loss=1.890]Epoch 0:   1%|          | 171/16924 [01:24<2:17:22,  2.03it/s, v_num=4, loss=1.890]Epoch 0:   1%|          | 171/16924 [01:24<2:17:52,  2.03it/s, v_num=4, loss=1.760]Epoch 0:   1%|          | 172/16924 [01:24<2:17:20,  2.03it/s, v_num=4, loss=1.760]Epoch 0:   1%|          | 172/16924 [01:24<2:17:50,  2.03it/s, v_num=4, loss=2.380]Epoch 0:   1%|          | 173/16924 [01:25<2:17:19,  2.03it/s, v_num=4, loss=2.380]Epoch 0:   1%|          | 173/16924 [01:25<2:17:48,  2.03it/s, v_num=4, loss=1.710]Epoch 0:   1%|          | 174/16924 [01:25<2:17:17,  2.03it/s, v_num=4, loss=1.710]Epoch 0:   1%|          | 174/16924 [01:25<2:17:46,  2.03it/s, v_num=4, loss=2.120]Epoch 0:   1%|          | 175/16924 [01:26<2:17:15,  2.03it/s, v_num=4, loss=2.120]Epoch 0:   1%|          | 175/16924 [01:26<2:17:46,  2.03it/s, v_num=4, loss=1.820]Epoch 0:   1%|          | 176/16924 [01:26<2:17:15,  2.03it/s, v_num=4, loss=1.820]Epoch 0:   1%|          | 176/16924 [01:26<2:17:45,  2.03it/s, v_num=4, loss=2.090]Epoch 0:   1%|          | 177/16924 [01:27<2:17:14,  2.03it/s, v_num=4, loss=2.090]Epoch 0:   1%|          | 177/16924 [01:27<2:17:42,  2.03it/s, v_num=4, loss=1.850]Epoch 0:   1%|          | 178/16924 [01:27<2:17:12,  2.03it/s, v_num=4, loss=1.850]Epoch 0:   1%|          | 178/16924 [01:27<2:17:41,  2.03it/s, v_num=4, loss=1.980]Epoch 0:   1%|          | 179/16924 [01:27<2:17:10,  2.03it/s, v_num=4, loss=1.980]Epoch 0:   1%|          | 179/16924 [01:28<2:17:40,  2.03it/s, v_num=4, loss=1.990]Epoch 0:   1%|          | 180/16924 [01:28<2:17:09,  2.03it/s, v_num=4, loss=1.990]Epoch 0:   1%|          | 180/16924 [01:28<2:17:38,  2.03it/s, v_num=4, loss=1.230]Epoch 0:   1%|          | 181/16924 [01:28<2:17:09,  2.03it/s, v_num=4, loss=1.230]Epoch 0:   1%|          | 181/16924 [01:29<2:17:37,  2.03it/s, v_num=4, loss=2.480]Epoch 0:   1%|          | 182/16924 [01:29<2:17:07,  2.03it/s, v_num=4, loss=2.480]Epoch 0:   1%|          | 182/16924 [01:29<2:17:35,  2.03it/s, v_num=4, loss=1.270]Epoch 0:   1%|          | 183/16924 [01:29<2:17:05,  2.04it/s, v_num=4, loss=1.270]Epoch 0:   1%|          | 183/16924 [01:30<2:17:33,  2.03it/s, v_num=4, loss=2.180]Epoch 0:   1%|          | 184/16924 [01:30<2:17:03,  2.04it/s, v_num=4, loss=2.180]Epoch 0:   1%|          | 184/16924 [01:30<2:17:31,  2.03it/s, v_num=4, loss=1.690]Epoch 0:   1%|          | 185/16924 [01:30<2:17:02,  2.04it/s, v_num=4, loss=1.690]Epoch 0:   1%|          | 185/16924 [01:31<2:17:30,  2.03it/s, v_num=4, loss=1.480]Epoch 0:   1%|          | 186/16924 [01:31<2:17:00,  2.04it/s, v_num=4, loss=1.480]Epoch 0:   1%|          | 186/16924 [01:31<2:17:28,  2.03it/s, v_num=4, loss=2.010]Epoch 0:   1%|          | 187/16924 [01:31<2:16:59,  2.04it/s, v_num=4, loss=2.010]Epoch 0:   1%|          | 187/16924 [01:32<2:17:27,  2.03it/s, v_num=4, loss=2.060]Epoch 0:   1%|          | 188/16924 [01:32<2:16:58,  2.04it/s, v_num=4, loss=2.060]Epoch 0:   1%|          | 188/16924 [01:32<2:17:26,  2.03it/s, v_num=4, loss=2.000]Epoch 0:   1%|          | 189/16924 [01:32<2:16:57,  2.04it/s, v_num=4, loss=2.000]Epoch 0:   1%|          | 189/16924 [01:33<2:17:24,  2.03it/s, v_num=4, loss=1.670]Epoch 0:   1%|          | 190/16924 [01:33<2:16:55,  2.04it/s, v_num=4, loss=1.670]Epoch 0:   1%|          | 190/16924 [01:33<2:17:22,  2.03it/s, v_num=4, loss=1.760]Epoch 0:   1%|          | 191/16924 [01:33<2:16:53,  2.04it/s, v_num=4, loss=1.760]Epoch 0:   1%|          | 191/16924 [01:34<2:17:21,  2.03it/s, v_num=4, loss=1.770]Epoch 0:   1%|          | 192/16924 [01:34<2:16:52,  2.04it/s, v_num=4, loss=1.770]Epoch 0:   1%|          | 192/16924 [01:34<2:17:19,  2.03it/s, v_num=4, loss=1.590]Epoch 0:   1%|          | 193/16924 [01:34<2:16:51,  2.04it/s, v_num=4, loss=1.590]Epoch 0:   1%|          | 193/16924 [01:35<2:17:17,  2.03it/s, v_num=4, loss=1.640]Epoch 0:   1%|          | 194/16924 [01:35<2:16:49,  2.04it/s, v_num=4, loss=1.640]Epoch 0:   1%|          | 194/16924 [01:35<2:17:16,  2.03it/s, v_num=4, loss=1.910]Epoch 0:   1%|          | 195/16924 [01:35<2:16:48,  2.04it/s, v_num=4, loss=1.910]Epoch 0:   1%|          | 195/16924 [01:35<2:17:15,  2.03it/s, v_num=4, loss=1.980]Epoch 0:   1%|          | 196/16924 [01:36<2:16:47,  2.04it/s, v_num=4, loss=1.980]Epoch 0:   1%|          | 196/16924 [01:36<2:17:13,  2.03it/s, v_num=4, loss=1.910]Epoch 0:   1%|          | 197/16924 [01:36<2:16:46,  2.04it/s, v_num=4, loss=1.910]Epoch 0:   1%|          | 197/16924 [01:36<2:17:10,  2.03it/s, v_num=4, loss=2.110]Epoch 0:   1%|          | 198/16924 [01:37<2:16:43,  2.04it/s, v_num=4, loss=2.110]Epoch 0:   1%|          | 198/16924 [01:37<2:17:09,  2.03it/s, v_num=4, loss=1.640]Epoch 0:   1%|          | 199/16924 [01:37<2:16:41,  2.04it/s, v_num=4, loss=1.640]Epoch 0:   1%|          | 199/16924 [01:37<2:17:07,  2.03it/s, v_num=4, loss=1.650]Epoch 0:   1%|          | 200/16924 [01:38<2:16:41,  2.04it/s, v_num=4, loss=1.650]Epoch 0:   1%|          | 200/16924 [01:38<2:17:06,  2.03it/s, v_num=4, loss=1.410]Epoch 0:   1%|          | 201/16924 [01:38<2:16:40,  2.04it/s, v_num=4, loss=1.410]Epoch 0:   1%|          | 201/16924 [01:38<2:17:05,  2.03it/s, v_num=4, loss=1.960]Epoch 0:   1%|          | 202/16924 [01:39<2:16:38,  2.04it/s, v_num=4, loss=1.960]Epoch 0:   1%|          | 202/16924 [01:39<2:17:04,  2.03it/s, v_num=4, loss=2.250]Epoch 0:   1%|          | 203/16924 [01:39<2:16:37,  2.04it/s, v_num=4, loss=2.250]Epoch 0:   1%|          | 203/16924 [01:39<2:17:03,  2.03it/s, v_num=4, loss=2.130]Epoch 0:   1%|          | 204/16924 [01:40<2:16:36,  2.04it/s, v_num=4, loss=2.130]Epoch 0:   1%|          | 204/16924 [01:40<2:17:01,  2.03it/s, v_num=4, loss=1.720]Epoch 0:   1%|          | 205/16924 [01:40<2:16:35,  2.04it/s, v_num=4, loss=1.720]Epoch 0:   1%|          | 205/16924 [01:40<2:17:00,  2.03it/s, v_num=4, loss=1.860]Epoch 0:   1%|          | 206/16924 [01:40<2:16:33,  2.04it/s, v_num=4, loss=1.860]Epoch 0:   1%|          | 206/16924 [01:41<2:16:58,  2.03it/s, v_num=4, loss=2.150]Epoch 0:   1%|          | 207/16924 [01:41<2:16:32,  2.04it/s, v_num=4, loss=2.150]Epoch 0:   1%|          | 207/16924 [01:41<2:16:57,  2.03it/s, v_num=4, loss=1.910]Epoch 0:   1%|          | 208/16924 [01:41<2:16:31,  2.04it/s, v_num=4, loss=1.910]Epoch 0:   1%|          | 208/16924 [01:42<2:16:56,  2.03it/s, v_num=4, loss=2.230]Epoch 0:   1%|          | 209/16924 [01:42<2:16:30,  2.04it/s, v_num=4, loss=2.230]Epoch 0:   1%|          | 209/16924 [01:42<2:16:55,  2.03it/s, v_num=4, loss=1.600]Epoch 0:   1%|          | 210/16924 [01:42<2:16:31,  2.04it/s, v_num=4, loss=1.600]Epoch 0:   1%|          | 210/16924 [01:43<2:16:54,  2.03it/s, v_num=4, loss=1.390]Epoch 0:   1%|          | 211/16924 [01:43<2:16:28,  2.04it/s, v_num=4, loss=1.390]Epoch 0:   1%|          | 211/16924 [01:43<2:16:52,  2.04it/s, v_num=4, loss=1.640]Epoch 0:   1%|         | 212/16924 [01:43<2:16:27,  2.04it/s, v_num=4, loss=1.640]Epoch 0:   1%|         | 212/16924 [01:44<2:16:50,  2.04it/s, v_num=4, loss=1.740]Epoch 0:   1%|         | 213/16924 [01:44<2:16:24,  2.04it/s, v_num=4, loss=1.740]Epoch 0:   1%|         | 213/16924 [01:44<2:16:48,  2.04it/s, v_num=4, loss=2.560]Epoch 0:   1%|         | 214/16924 [01:44<2:16:23,  2.04it/s, v_num=4, loss=2.560]Epoch 0:   1%|         | 214/16924 [01:45<2:16:47,  2.04it/s, v_num=4, loss=2.100]Epoch 0:   1%|         | 215/16924 [01:45<2:16:22,  2.04it/s, v_num=4, loss=2.100]Epoch 0:   1%|         | 215/16924 [01:45<2:16:46,  2.04it/s, v_num=4, loss=1.880]Epoch 0:   1%|         | 216/16924 [01:45<2:16:21,  2.04it/s, v_num=4, loss=1.880]Epoch 0:   1%|         | 216/16924 [01:46<2:16:45,  2.04it/s, v_num=4, loss=2.050]Epoch 0:   1%|         | 217/16924 [01:46<2:16:20,  2.04it/s, v_num=4, loss=2.050]Epoch 0:   1%|         | 217/16924 [01:46<2:16:44,  2.04it/s, v_num=4, loss=2.360]Epoch 0:   1%|         | 218/16924 [01:46<2:16:19,  2.04it/s, v_num=4, loss=2.360]Epoch 0:   1%|         | 218/16924 [01:47<2:16:42,  2.04it/s, v_num=4, loss=1.760]Epoch 0:   1%|         | 219/16924 [01:47<2:16:17,  2.04it/s, v_num=4, loss=1.760]Epoch 0:   1%|         | 219/16924 [01:47<2:16:40,  2.04it/s, v_num=4, loss=1.680]Epoch 0:   1%|         | 220/16924 [01:47<2:16:16,  2.04it/s, v_num=4, loss=1.680]Epoch 0:   1%|         | 220/16924 [01:47<2:16:39,  2.04it/s, v_num=4, loss=1.850]Epoch 0:   1%|         | 221/16924 [01:48<2:16:14,  2.04it/s, v_num=4, loss=1.850]Epoch 0:   1%|         | 221/16924 [01:48<2:16:37,  2.04it/s, v_num=4, loss=1.820]Epoch 0:   1%|         | 222/16924 [01:48<2:16:13,  2.04it/s, v_num=4, loss=1.820]Epoch 0:   1%|         | 222/16924 [01:48<2:16:36,  2.04it/s, v_num=4, loss=1.970]Epoch 0:   1%|         | 223/16924 [01:49<2:16:12,  2.04it/s, v_num=4, loss=1.970]Epoch 0:   1%|         | 223/16924 [01:49<2:16:35,  2.04it/s, v_num=4, loss=1.790]Epoch 0:   1%|         | 224/16924 [01:49<2:16:11,  2.04it/s, v_num=4, loss=1.790]Epoch 0:   1%|         | 224/16924 [01:49<2:16:34,  2.04it/s, v_num=4, loss=1.790]Epoch 0:   1%|         | 225/16924 [01:50<2:16:10,  2.04it/s, v_num=4, loss=1.790]Epoch 0:   1%|         | 225/16924 [01:50<2:16:33,  2.04it/s, v_num=4, loss=2.580]Epoch 0:   1%|         | 226/16924 [01:50<2:16:09,  2.04it/s, v_num=4, loss=2.580]Epoch 0:   1%|         | 226/16924 [01:50<2:16:31,  2.04it/s, v_num=4, loss=1.610]Epoch 0:   1%|         | 227/16924 [01:51<2:16:07,  2.04it/s, v_num=4, loss=1.610]Epoch 0:   1%|         | 227/16924 [01:51<2:16:30,  2.04it/s, v_num=4, loss=1.500]Epoch 0:   1%|         | 228/16924 [01:51<2:16:06,  2.04it/s, v_num=4, loss=1.500]Epoch 0:   1%|         | 228/16924 [01:51<2:16:28,  2.04it/s, v_num=4, loss=2.230]Epoch 0:   1%|         | 229/16924 [01:51<2:16:04,  2.04it/s, v_num=4, loss=2.230]Epoch 0:   1%|         | 229/16924 [01:52<2:16:26,  2.04it/s, v_num=4, loss=2.090]Epoch 0:   1%|         | 230/16924 [01:52<2:16:03,  2.05it/s, v_num=4, loss=2.090]Epoch 0:   1%|         | 230/16924 [01:52<2:16:25,  2.04it/s, v_num=4, loss=1.730]Epoch 0:   1%|         | 231/16924 [01:52<2:16:02,  2.05it/s, v_num=4, loss=1.730]Epoch 0:   1%|         | 231/16924 [01:53<2:16:24,  2.04it/s, v_num=4, loss=1.440]Epoch 0:   1%|         | 232/16924 [01:53<2:16:01,  2.05it/s, v_num=4, loss=1.440]Epoch 0:   1%|         | 232/16924 [01:53<2:16:23,  2.04it/s, v_num=4, loss=1.530]Epoch 0:   1%|         | 233/16924 [01:53<2:15:59,  2.05it/s, v_num=4, loss=1.530]Epoch 0:   1%|         | 233/16924 [01:54<2:16:21,  2.04it/s, v_num=4, loss=2.040]Epoch 0:   1%|         | 234/16924 [01:54<2:15:58,  2.05it/s, v_num=4, loss=2.040]Epoch 0:   1%|         | 234/16924 [01:54<2:16:20,  2.04it/s, v_num=4, loss=2.040]Epoch 0:   1%|         | 235/16924 [01:54<2:15:57,  2.05it/s, v_num=4, loss=2.040]Epoch 0:   1%|         | 235/16924 [01:55<2:16:19,  2.04it/s, v_num=4, loss=1.820]Epoch 0:   1%|         | 236/16924 [01:55<2:15:55,  2.05it/s, v_num=4, loss=1.820]Epoch 0:   1%|         | 236/16924 [01:55<2:16:18,  2.04it/s, v_num=4, loss=1.270]Epoch 0:   1%|         | 237/16924 [01:55<2:15:55,  2.05it/s, v_num=4, loss=1.270]Epoch 0:   1%|         | 237/16924 [01:56<2:16:16,  2.04it/s, v_num=4, loss=1.880]Epoch 0:   1%|         | 238/16924 [01:56<2:15:53,  2.05it/s, v_num=4, loss=1.880]Epoch 0:   1%|         | 238/16924 [01:56<2:16:16,  2.04it/s, v_num=4, loss=2.030]Epoch 0:   1%|         | 239/16924 [01:56<2:15:53,  2.05it/s, v_num=4, loss=2.030]Epoch 0:   1%|         | 239/16924 [01:57<2:16:14,  2.04it/s, v_num=4, loss=1.860]Epoch 0:   1%|         | 240/16924 [01:57<2:15:51,  2.05it/s, v_num=4, loss=1.860]Epoch 0:   1%|         | 240/16924 [01:57<2:16:14,  2.04it/s, v_num=4, loss=2.020]Epoch 0:   1%|         | 241/16924 [01:57<2:15:51,  2.05it/s, v_num=4, loss=2.020]Epoch 0:   1%|         | 241/16924 [01:58<2:16:13,  2.04it/s, v_num=4, loss=1.630]Epoch 0:   1%|         | 242/16924 [01:58<2:15:50,  2.05it/s, v_num=4, loss=1.630]Epoch 0:   1%|         | 242/16924 [01:58<2:16:11,  2.04it/s, v_num=4, loss=1.950]Epoch 0:   1%|         | 243/16924 [01:58<2:15:49,  2.05it/s, v_num=4, loss=1.950]Epoch 0:   1%|         | 243/16924 [01:59<2:16:10,  2.04it/s, v_num=4, loss=1.700]Epoch 0:   1%|         | 244/16924 [01:59<2:15:48,  2.05it/s, v_num=4, loss=1.700]Epoch 0:   1%|         | 244/16924 [01:59<2:16:10,  2.04it/s, v_num=4, loss=2.020]Epoch 0:   1%|         | 245/16924 [01:59<2:15:48,  2.05it/s, v_num=4, loss=2.020]Epoch 0:   1%|         | 245/16924 [02:00<2:16:09,  2.04it/s, v_num=4, loss=1.270]Epoch 0:   1%|         | 246/16924 [02:00<2:15:47,  2.05it/s, v_num=4, loss=1.270]Epoch 0:   1%|         | 246/16924 [02:00<2:16:08,  2.04it/s, v_num=4, loss=1.990]Epoch 0:   1%|         | 247/16924 [02:00<2:15:46,  2.05it/s, v_num=4, loss=1.990]Epoch 0:   1%|         | 247/16924 [02:00<2:16:07,  2.04it/s, v_num=4, loss=1.910]Epoch 0:   1%|         | 248/16924 [02:01<2:15:45,  2.05it/s, v_num=4, loss=1.910]Epoch 0:   1%|         | 248/16924 [02:01<2:16:06,  2.04it/s, v_num=4, loss=1.890]Epoch 0:   1%|         | 249/16924 [02:01<2:15:44,  2.05it/s, v_num=4, loss=1.890]Epoch 0:   1%|         | 249/16924 [02:01<2:16:04,  2.04it/s, v_num=4, loss=1.910]Epoch 0:   1%|         | 250/16924 [02:02<2:15:43,  2.05it/s, v_num=4, loss=1.910]Epoch 0:   1%|         | 250/16924 [02:02<2:16:03,  2.04it/s, v_num=4, loss=1.890]Epoch 0:   1%|         | 251/16924 [02:02<2:15:42,  2.05it/s, v_num=4, loss=1.890]Epoch 0:   1%|         | 251/16924 [02:02<2:16:03,  2.04it/s, v_num=4, loss=1.310]Epoch 0:   1%|         | 252/16924 [02:03<2:15:41,  2.05it/s, v_num=4, loss=1.310]Epoch 0:   1%|         | 252/16924 [02:03<2:16:01,  2.04it/s, v_num=4, loss=1.860]Epoch 0:   1%|         | 253/16924 [02:03<2:15:40,  2.05it/s, v_num=4, loss=1.860]Epoch 0:   1%|         | 253/16924 [02:03<2:16:00,  2.04it/s, v_num=4, loss=1.230]Epoch 0:   2%|         | 254/16924 [02:04<2:15:39,  2.05it/s, v_num=4, loss=1.230]Epoch 0:   2%|         | 254/16924 [02:04<2:15:59,  2.04it/s, v_num=4, loss=2.180]Epoch 0:   2%|         | 255/16924 [02:04<2:15:37,  2.05it/s, v_num=4, loss=2.180]Epoch 0:   2%|         | 255/16924 [02:04<2:15:58,  2.04it/s, v_num=4, loss=1.360]Epoch 0:   2%|         | 256/16924 [02:04<2:15:36,  2.05it/s, v_num=4, loss=1.360]Epoch 0:   2%|         | 256/16924 [02:05<2:15:57,  2.04it/s, v_num=4, loss=1.860]Epoch 0:   2%|         | 257/16924 [02:05<2:15:36,  2.05it/s, v_num=4, loss=1.860]Epoch 0:   2%|         | 257/16924 [02:05<2:15:57,  2.04it/s, v_num=4, loss=1.690]Epoch 0:   2%|         | 258/16924 [02:05<2:15:36,  2.05it/s, v_num=4, loss=1.690]Epoch 0:   2%|         | 258/16924 [02:06<2:15:55,  2.04it/s, v_num=4, loss=1.630]Epoch 0:   2%|         | 259/16924 [02:06<2:15:34,  2.05it/s, v_num=4, loss=1.630]Epoch 0:   2%|         | 259/16924 [02:06<2:15:54,  2.04it/s, v_num=4, loss=2.020]Epoch 0:   2%|         | 260/16924 [02:06<2:15:33,  2.05it/s, v_num=4, loss=2.020]Epoch 0:   2%|         | 260/16924 [02:07<2:15:52,  2.04it/s, v_num=4, loss=1.710]Epoch 0:   2%|         | 261/16924 [02:07<2:15:31,  2.05it/s, v_num=4, loss=1.710]Epoch 0:   2%|         | 261/16924 [02:07<2:15:51,  2.04it/s, v_num=4, loss=1.770]Epoch 0:   2%|         | 262/16924 [02:07<2:15:30,  2.05it/s, v_num=4, loss=1.770]Epoch 0:   2%|         | 262/16924 [02:08<2:15:50,  2.04it/s, v_num=4, loss=1.770]Epoch 0:   2%|         | 263/16924 [02:08<2:15:29,  2.05it/s, v_num=4, loss=1.770]Epoch 0:   2%|         | 263/16924 [02:08<2:15:48,  2.04it/s, v_num=4, loss=2.090]Epoch 0:   2%|         | 264/16924 [02:08<2:15:28,  2.05it/s, v_num=4, loss=2.090]Epoch 0:   2%|         | 264/16924 [02:09<2:15:48,  2.04it/s, v_num=4, loss=1.750]Epoch 0:   2%|         | 265/16924 [02:09<2:15:27,  2.05it/s, v_num=4, loss=1.750]Epoch 0:   2%|         | 265/16924 [02:09<2:15:47,  2.04it/s, v_num=4, loss=2.090]Epoch 0:   2%|         | 266/16924 [02:09<2:15:26,  2.05it/s, v_num=4, loss=2.090]Epoch 0:   2%|         | 266/16924 [02:10<2:15:46,  2.04it/s, v_num=4, loss=1.780]Epoch 0:   2%|         | 267/16924 [02:10<2:15:25,  2.05it/s, v_num=4, loss=1.780]Epoch 0:   2%|         | 267/16924 [02:10<2:15:45,  2.04it/s, v_num=4, loss=1.880]Epoch 0:   2%|         | 268/16924 [02:10<2:15:25,  2.05it/s, v_num=4, loss=1.880]Epoch 0:   2%|         | 268/16924 [02:11<2:15:44,  2.05it/s, v_num=4, loss=1.970]Epoch 0:   2%|         | 269/16924 [02:11<2:15:24,  2.05it/s, v_num=4, loss=1.970]Epoch 0:   2%|         | 269/16924 [02:11<2:15:43,  2.05it/s, v_num=4, loss=2.340]Epoch 0:   2%|         | 270/16924 [02:11<2:15:23,  2.05it/s, v_num=4, loss=2.340]Epoch 0:   2%|         | 270/16924 [02:12<2:15:42,  2.05it/s, v_num=4, loss=1.880]Epoch 0:   2%|         | 271/16924 [02:12<2:15:22,  2.05it/s, v_num=4, loss=1.880]Epoch 0:   2%|         | 271/16924 [02:12<2:15:41,  2.05it/s, v_num=4, loss=1.130]Epoch 0:   2%|         | 272/16924 [02:12<2:15:21,  2.05it/s, v_num=4, loss=1.130]Epoch 0:   2%|         | 272/16924 [02:12<2:15:41,  2.05it/s, v_num=4, loss=1.750]Epoch 0:   2%|         | 273/16924 [02:13<2:15:21,  2.05it/s, v_num=4, loss=1.750]Epoch 0:   2%|         | 273/16924 [02:13<2:15:39,  2.05it/s, v_num=4, loss=2.140]Epoch 0:   2%|         | 274/16924 [02:13<2:15:19,  2.05it/s, v_num=4, loss=2.140]Epoch 0:   2%|         | 274/16924 [02:13<2:15:38,  2.05it/s, v_num=4, loss=1.710]Epoch 0:   2%|         | 275/16924 [02:14<2:15:18,  2.05it/s, v_num=4, loss=1.710]Epoch 0:   2%|         | 275/16924 [02:14<2:15:37,  2.05it/s, v_num=4, loss=1.340]Epoch 0:   2%|         | 276/16924 [02:14<2:15:17,  2.05it/s, v_num=4, loss=1.340]Epoch 0:   2%|         | 276/16924 [02:14<2:15:36,  2.05it/s, v_num=4, loss=1.220]Epoch 0:   2%|         | 277/16924 [02:15<2:15:16,  2.05it/s, v_num=4, loss=1.220]Epoch 0:   2%|         | 277/16924 [02:15<2:15:35,  2.05it/s, v_num=4, loss=1.920]Epoch 0:   2%|         | 278/16924 [02:15<2:15:16,  2.05it/s, v_num=4, loss=1.920]Epoch 0:   2%|         | 278/16924 [02:15<2:15:35,  2.05it/s, v_num=4, loss=1.420]Epoch 0:   2%|         | 279/16924 [02:16<2:15:15,  2.05it/s, v_num=4, loss=1.420]Epoch 0:   2%|         | 279/16924 [02:16<2:15:33,  2.05it/s, v_num=4, loss=1.320]Epoch 0:   2%|         | 280/16924 [02:16<2:15:14,  2.05it/s, v_num=4, loss=1.320]Epoch 0:   2%|         | 280/16924 [02:16<2:15:32,  2.05it/s, v_num=4, loss=1.870]Epoch 0:   2%|         | 281/16924 [02:16<2:15:13,  2.05it/s, v_num=4, loss=1.870]Epoch 0:   2%|         | 281/16924 [02:17<2:15:31,  2.05it/s, v_num=4, loss=1.690]Epoch 0:   2%|         | 282/16924 [02:17<2:15:12,  2.05it/s, v_num=4, loss=1.690]Epoch 0:   2%|         | 282/16924 [02:17<2:15:31,  2.05it/s, v_num=4, loss=1.100]Epoch 0:   2%|         | 283/16924 [02:17<2:15:12,  2.05it/s, v_num=4, loss=1.100]Epoch 0:   2%|         | 283/16924 [02:18<2:15:30,  2.05it/s, v_num=4, loss=2.100]Epoch 0:   2%|         | 284/16924 [02:18<2:15:11,  2.05it/s, v_num=4, loss=2.100]Epoch 0:   2%|         | 284/16924 [02:18<2:15:29,  2.05it/s, v_num=4, loss=1.570]Epoch 0:   2%|         | 285/16924 [02:18<2:15:10,  2.05it/s, v_num=4, loss=1.570]Epoch 0:   2%|         | 285/16924 [02:19<2:15:28,  2.05it/s, v_num=4, loss=1.760]Epoch 0:   2%|         | 286/16924 [02:19<2:15:09,  2.05it/s, v_num=4, loss=1.760]Epoch 0:   2%|         | 286/16924 [02:19<2:15:28,  2.05it/s, v_num=4, loss=2.340]Epoch 0:   2%|         | 287/16924 [02:19<2:15:09,  2.05it/s, v_num=4, loss=2.340]Epoch 0:   2%|         | 287/16924 [02:20<2:15:27,  2.05it/s, v_num=4, loss=1.650]Epoch 0:   2%|         | 288/16924 [02:20<2:15:08,  2.05it/s, v_num=4, loss=1.650]Epoch 0:   2%|         | 288/16924 [02:20<2:15:26,  2.05it/s, v_num=4, loss=1.320]Epoch 0:   2%|         | 289/16924 [02:20<2:15:07,  2.05it/s, v_num=4, loss=1.320]Epoch 0:   2%|         | 289/16924 [02:21<2:15:25,  2.05it/s, v_num=4, loss=2.060]Epoch 0:   2%|         | 290/16924 [02:21<2:15:07,  2.05it/s, v_num=4, loss=2.060]Epoch 0:   2%|         | 290/16924 [02:21<2:15:24,  2.05it/s, v_num=4, loss=2.000]Epoch 0:   2%|         | 291/16924 [02:21<2:15:06,  2.05it/s, v_num=4, loss=2.000]/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 8,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
[rank: 3] Global seed set to 42
[rank: 1] Global seed set to 42
[rank: 2] Global seed set to 42
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/home/eechengyang/anaconda3/envs/promptmrg/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory ./save/mimic_cxr/v1_shallow/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,3,7]

  | Name           | Type                | Params
-------------------------------------------------------
0 | visual_encoder | SwinModel           | 86.7 M
1 | lm_model       | StableLmForCausalLM | 2.8 B 
2 | embed_tokens   | Embedding           | 128 M 
3 | lm_proj        | Linear              | 2.6 M 
4 | layer_norm     | LayerNorm           | 5.1 K 
-------------------------------------------------------
2.6 M     Trainable params
2.9 B     Non-trainable params
2.9 B     Total params
11,539.262Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 8,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 8,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 8,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0:  50%|     | 1/2 [00:07<00:07,  7.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:11<00:00,  5.51s/it]                                                                           {'Bleu_1': 0.06750241080032063, 'Bleu_2': 0.025712646004389692, 'Bleu_3': 0.01095769375123071, 'Bleu_4': 1.0739654149883732e-06, 'ROUGE_L': np.float64(0.04895191266198126), 'CIDEr': np.float64(0.008333836694757329)}
Sanity Checking DataLoader 0: 100%|| 2/2 [00:11<00:00,  5.60s/it]                                                                           Saving checkpoint at step 0 to ./save/mimic_cxr/v1_shallow/checkpoints/checkpoint_epoch0_step0_bleu0.000001_cider0.008334.pth.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:11<00:00,  5.61s/it]                                                                           huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/8462 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/8462 [00:00<?, ?it/s] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Epoch 0:   0%|          | 1/8462 [00:02<5:56:36,  2.53s/it]Epoch 0:   0%|          | 1/8462 [00:02<6:07:25,  2.61s/it, v_num=5, loss=3.760]Epoch 0:   0%|          | 2/8462 [00:02<3:24:26,  1.45s/it, v_num=5, loss=3.760]Epoch 0:   0%|          | 2/8462 [00:03<3:34:38,  1.52s/it, v_num=5, loss=3.300]Epoch 0:   0%|          | 3/8462 [00:03<2:36:41,  1.11s/it, v_num=5, loss=3.300]Epoch 0:   0%|          | 3/8462 [00:03<2:41:33,  1.15s/it, v_num=5, loss=2.840]Epoch 0:   0%|          | 4/8462 [00:03<2:11:26,  1.07it/s, v_num=5, loss=2.840]Epoch 0:   0%|          | 4/8462 [00:03<2:14:54,  1.04it/s, v_num=5, loss=2.790]Epoch 0:   0%|          | 5/8462 [00:04<1:56:11,  1.21it/s, v_num=5, loss=2.790]Epoch 0:   0%|          | 5/8462 [00:04<1:58:52,  1.19it/s, v_num=5, loss=2.710]Epoch 0:   0%|          | 6/8462 [00:04<1:45:46,  1.33it/s, v_num=5, loss=2.710]Epoch 0:   0%|          | 6/8462 [00:04<1:48:12,  1.30it/s, v_num=5, loss=2.820]Epoch 0:   0%|          | 7/8462 [00:04<1:38:26,  1.43it/s, v_num=5, loss=2.820]Epoch 0:   0%|          | 7/8462 [00:04<1:40:37,  1.40it/s, v_num=5, loss=2.760]Epoch 0:   0%|          | 8/8462 [00:05<1:33:08,  1.51it/s, v_num=5, loss=2.760]Epoch 0:   0%|          | 8/8462 [00:05<1:34:47,  1.49it/s, v_num=5, loss=2.980]Epoch 0:   0%|          | 9/8462 [00:05<1:28:54,  1.58it/s, v_num=5, loss=2.980]Epoch 0:   0%|          | 9/8462 [00:05<1:30:37,  1.55it/s, v_num=5, loss=2.570]Epoch 0:   0%|          | 10/8462 [00:06<1:25:34,  1.65it/s, v_num=5, loss=2.570]Epoch 0:   0%|          | 10/8462 [00:06<1:27:02,  1.62it/s, v_num=5, loss=2.670]Epoch 0:   0%|          | 11/8462 [00:06<1:22:46,  1.70it/s, v_num=5, loss=2.670]Epoch 0:   0%|          | 11/8462 [00:06<1:24:16,  1.67it/s, v_num=5, loss=2.560]Epoch 0:   0%|          | 12/8462 [00:06<1:20:36,  1.75it/s, v_num=5, loss=2.560]Epoch 0:   0%|          | 12/8462 [00:06<1:22:06,  1.72it/s, v_num=5, loss=2.700]Epoch 0:   0%|          | 13/8462 [00:07<1:18:51,  1.79it/s, v_num=5, loss=2.700]Epoch 0:   0%|          | 13/8462 [00:07<1:19:57,  1.76it/s, v_num=5, loss=2.570]Epoch 0:   0%|          | 14/8462 [00:07<1:17:08,  1.83it/s, v_num=5, loss=2.570]Epoch 0:   0%|          | 14/8462 [00:07<1:18:08,  1.80it/s, v_num=5, loss=2.490]Epoch 0:   0%|          | 15/8462 [00:08<1:15:35,  1.86it/s, v_num=5, loss=2.490]Epoch 0:   0%|          | 15/8462 [00:08<1:16:31,  1.84it/s, v_num=5, loss=2.470]Epoch 0:   0%|          | 16/8462 [00:08<1:14:17,  1.89it/s, v_num=5, loss=2.470]Epoch 0:   0%|          | 16/8462 [00:08<1:15:15,  1.87it/s, v_num=5, loss=2.440]Epoch 0:   0%|          | 17/8462 [00:08<1:13:12,  1.92it/s, v_num=5, loss=2.440]Epoch 0:   0%|          | 17/8462 [00:08<1:14:06,  1.90it/s, v_num=5, loss=2.180]Epoch 0:   0%|          | 18/8462 [00:09<1:12:14,  1.95it/s, v_num=5, loss=2.180]Epoch 0:   0%|          | 18/8462 [00:09<1:12:59,  1.93it/s, v_num=5, loss=2.010]Epoch 0:   0%|          | 19/8462 [00:09<1:11:16,  1.97it/s, v_num=5, loss=2.010]Epoch 0:   0%|          | 19/8462 [00:09<1:11:59,  1.95it/s, v_num=5, loss=2.360]Epoch 0:   0%|          | 20/8462 [00:10<1:10:24,  2.00it/s, v_num=5, loss=2.360]Epoch 0:   0%|          | 20/8462 [00:10<1:11:08,  1.98it/s, v_num=5, loss=2.150]Epoch 0:   0%|          | 21/8462 [00:10<1:09:41,  2.02it/s, v_num=5, loss=2.150]Epoch 0:   0%|          | 21/8462 [00:10<1:10:23,  2.00it/s, v_num=5, loss=2.110]Epoch 0:   0%|          | 22/8462 [00:10<1:09:05,  2.04it/s, v_num=5, loss=2.110]Epoch 0:   0%|          | 22/8462 [00:10<1:09:56,  2.01it/s, v_num=5, loss=2.600]Epoch 0:   0%|          | 23/8462 [00:11<1:08:37,  2.05it/s, v_num=5, loss=2.600]Epoch 0:   0%|          | 23/8462 [00:11<1:09:13,  2.03it/s, v_num=5, loss=2.490]Epoch 0:   0%|          | 24/8462 [00:11<1:08:00,  2.07it/s, v_num=5, loss=2.490]Epoch 0:   0%|          | 24/8462 [00:11<1:08:35,  2.05it/s, v_num=5, loss=2.140]Epoch 0:   0%|          | 25/8462 [00:11<1:07:26,  2.09it/s, v_num=5, loss=2.140]Epoch 0:   0%|          | 25/8462 [00:12<1:08:01,  2.07it/s, v_num=5, loss=2.010]Epoch 0:   0%|          | 26/8462 [00:12<1:06:58,  2.10it/s, v_num=5, loss=2.010]Epoch 0:   0%|          | 26/8462 [00:12<1:07:36,  2.08it/s, v_num=5, loss=2.070]Epoch 0:   0%|          | 27/8462 [00:12<1:06:42,  2.11it/s, v_num=5, loss=2.070]Epoch 0:   0%|          | 27/8462 [00:12<1:07:07,  2.09it/s, v_num=5, loss=2.320]Epoch 0:   0%|          | 28/8462 [00:13<1:06:09,  2.12it/s, v_num=5, loss=2.320]Epoch 0:   0%|          | 28/8462 [00:13<1:06:41,  2.11it/s, v_num=5, loss=1.880]Epoch 0:   0%|          | 29/8462 [00:13<1:05:45,  2.14it/s, v_num=5, loss=1.880]Epoch 0:   0%|          | 29/8462 [00:13<1:06:13,  2.12it/s, v_num=5, loss=1.610]Epoch 0:   0%|          | 30/8462 [00:13<1:05:20,  2.15it/s, v_num=5, loss=1.610]Epoch 0:   0%|          | 30/8462 [00:14<1:05:49,  2.14it/s, v_num=5, loss=1.760]Epoch 0:   0%|          | 31/8462 [00:14<1:04:59,  2.16it/s, v_num=5, loss=1.760]Epoch 0:   0%|          | 31/8462 [00:14<1:05:27,  2.15it/s, v_num=5, loss=2.050]Epoch 0:   0%|          | 32/8462 [00:14<1:04:38,  2.17it/s, v_num=5, loss=2.050]Epoch 0:   0%|          | 32/8462 [00:14<1:05:12,  2.15it/s, v_num=5, loss=2.240]Epoch 0:   0%|          | 33/8462 [00:15<1:04:25,  2.18it/s, v_num=5, loss=2.240]Epoch 0:   0%|          | 33/8462 [00:15<1:04:52,  2.17it/s, v_num=5, loss=2.260]Epoch 0:   0%|          | 34/8462 [00:15<1:04:08,  2.19it/s, v_num=5, loss=2.260]Epoch 0:   0%|          | 34/8462 [00:15<1:04:33,  2.18it/s, v_num=5, loss=2.000]Epoch 0:   0%|          | 35/8462 [00:15<1:03:50,  2.20it/s, v_num=5, loss=2.000]Epoch 0:   0%|          | 35/8462 [00:16<1:04:14,  2.19it/s, v_num=5, loss=2.360]Epoch 0:   0%|          | 36/8462 [00:16<1:03:35,  2.21it/s, v_num=5, loss=2.360]Epoch 0:   0%|          | 36/8462 [00:16<1:03:57,  2.20it/s, v_num=5, loss=2.210]Epoch 0:   0%|          | 37/8462 [00:16<1:03:18,  2.22it/s, v_num=5, loss=2.210]Epoch 0:   0%|          | 37/8462 [00:16<1:03:45,  2.20it/s, v_num=5, loss=2.500]Epoch 0:   0%|          | 38/8462 [00:17<1:03:06,  2.22it/s, v_num=5, loss=2.500]Epoch 0:   0%|          | 38/8462 [00:17<1:03:30,  2.21it/s, v_num=5, loss=1.530]Epoch 0:   0%|          | 39/8462 [00:17<1:02:53,  2.23it/s, v_num=5, loss=1.530]Epoch 0:   0%|          | 39/8462 [00:17<1:03:29,  2.21it/s, v_num=5, loss=2.230]Epoch 0:   0%|          | 40/8462 [00:17<1:02:53,  2.23it/s, v_num=5, loss=2.230]Epoch 0:   0%|          | 40/8462 [00:18<1:03:16,  2.22it/s, v_num=5, loss=2.210]Epoch 0:   0%|          | 41/8462 [00:18<1:02:42,  2.24it/s, v_num=5, loss=2.210]Epoch 0:   0%|          | 41/8462 [00:18<1:03:03,  2.23it/s, v_num=5, loss=2.310]Epoch 0:   0%|          | 42/8462 [00:18<1:02:29,  2.25it/s, v_num=5, loss=2.310]Epoch 0:   0%|          | 42/8462 [00:18<1:02:54,  2.23it/s, v_num=5, loss=1.960]Epoch 0:   1%|          | 43/8462 [00:19<1:02:21,  2.25it/s, v_num=5, loss=1.960]Epoch 0:   1%|          | 43/8462 [00:19<1:02:41,  2.24it/s, v_num=5, loss=2.310]Epoch 0:   1%|          | 44/8462 [00:19<1:02:09,  2.26it/s, v_num=5, loss=2.310]Epoch 0:   1%|          | 44/8462 [00:19<1:02:28,  2.25it/s, v_num=5, loss=2.020]Epoch 0:   1%|          | 45/8462 [00:19<1:01:58,  2.26it/s, v_num=5, loss=2.020]Epoch 0:   1%|          | 45/8462 [00:19<1:02:17,  2.25it/s, v_num=5, loss=2.010]Epoch 0:   1%|          | 46/8462 [00:20<1:01:47,  2.27it/s, v_num=5, loss=2.010]Epoch 0:   1%|          | 46/8462 [00:20<1:02:05,  2.26it/s, v_num=5, loss=2.130]Epoch 0:   1%|          | 47/8462 [00:20<1:01:36,  2.28it/s, v_num=5, loss=2.130]Epoch 0:   1%|          | 47/8462 [00:20<1:01:54,  2.27it/s, v_num=5, loss=2.220]Epoch 0:   1%|          | 48/8462 [00:21<1:01:26,  2.28it/s, v_num=5, loss=2.220]Epoch 0:   1%|          | 48/8462 [00:21<1:01:43,  2.27it/s, v_num=5, loss=1.870]Epoch 0:   1%|          | 49/8462 [00:21<1:01:15,  2.29it/s, v_num=5, loss=1.870]Epoch 0:   1%|          | 49/8462 [00:21<1:01:35,  2.28it/s, v_num=5, loss=1.790]Epoch 0:   1%|          | 50/8462 [00:21<1:01:09,  2.29it/s, v_num=5, loss=1.790]Epoch 0:   1%|          | 50/8462 [00:21<1:01:31,  2.28it/s, v_num=5, loss=1.860]Epoch 0:   1%|          | 51/8462 [00:22<1:01:06,  2.29it/s, v_num=5, loss=1.860]Epoch 0:   1%|          | 51/8462 [00:22<1:01:22,  2.28it/s, v_num=5, loss=1.980]Epoch 0:   1%|          | 52/8462 [00:22<1:00:57,  2.30it/s, v_num=5, loss=1.980]Epoch 0:   1%|          | 52/8462 [00:22<1:01:13,  2.29it/s, v_num=5, loss=1.980]Epoch 0:   1%|          | 53/8462 [00:22<1:00:48,  2.30it/s, v_num=5, loss=1.980]Epoch 0:   1%|          | 53/8462 [00:23<1:01:04,  2.29it/s, v_num=5, loss=2.380]Epoch 0:   1%|          | 54/8462 [00:23<1:00:40,  2.31it/s, v_num=5, loss=2.380]Epoch 0:   1%|          | 54/8462 [00:23<1:00:55,  2.30it/s, v_num=5, loss=1.950]Epoch 0:   1%|          | 55/8462 [00:23<1:00:32,  2.31it/s, v_num=5, loss=1.950]Epoch 0:   1%|          | 55/8462 [00:23<1:00:47,  2.30it/s, v_num=5, loss=2.280]Epoch 0:   1%|          | 56/8462 [00:24<1:00:24,  2.32it/s, v_num=5, loss=2.280]Epoch 0:   1%|          | 56/8462 [00:24<1:00:40,  2.31it/s, v_num=5, loss=1.810]Epoch 0:   1%|          | 57/8462 [00:24<1:00:17,  2.32it/s, v_num=5, loss=1.810]Epoch 0:   1%|          | 57/8462 [00:24<1:00:33,  2.31it/s, v_num=5, loss=1.920]Epoch 0:   1%|          | 58/8462 [00:24<1:00:11,  2.33it/s, v_num=5, loss=1.920]Epoch 0:   1%|          | 58/8462 [00:25<1:00:26,  2.32it/s, v_num=5, loss=2.500]Epoch 0:   1%|          | 59/8462 [00:25<1:00:05,  2.33it/s, v_num=5, loss=2.500]Epoch 0:   1%|          | 59/8462 [00:25<1:00:23,  2.32it/s, v_num=5, loss=2.020]Epoch 0:   1%|          | 60/8462 [00:25<1:00:02,  2.33it/s, v_num=5, loss=2.020]Epoch 0:   1%|          | 60/8462 [00:25<1:00:18,  2.32it/s, v_num=5, loss=1.820]Epoch 0:   1%|          | 61/8462 [00:26<59:57,  2.34it/s, v_num=5, loss=1.820]  Epoch 0:   1%|          | 61/8462 [00:26<1:00:11,  2.33it/s, v_num=5, loss=2.030]Epoch 0:   1%|          | 62/8462 [00:26<59:51,  2.34it/s, v_num=5, loss=2.030]  Epoch 0:   1%|          | 62/8462 [00:26<1:00:05,  2.33it/s, v_num=5, loss=2.300]Epoch 0:   1%|          | 63/8462 [00:26<59:46,  2.34it/s, v_num=5, loss=2.300]  Epoch 0:   1%|          | 63/8462 [00:27<1:00:10,  2.33it/s, v_num=5, loss=1.590]Epoch 0:   1%|          | 64/8462 [00:27<59:50,  2.34it/s, v_num=5, loss=1.590]  Epoch 0:   1%|          | 64/8462 [00:27<1:00:03,  2.33it/s, v_num=5, loss=2.240]Epoch 0:   1%|          | 65/8462 [00:27<59:45,  2.34it/s, v_num=5, loss=2.240]  Epoch 0:   1%|          | 65/8462 [00:27<59:59,  2.33it/s, v_num=5, loss=2.110]Epoch 0:   1%|          | 66/8462 [00:28<59:40,  2.34it/s, v_num=5, loss=2.110]Epoch 0:   1%|          | 66/8462 [00:28<59:54,  2.34it/s, v_num=5, loss=2.160]Epoch 0:   1%|          | 67/8462 [00:28<59:35,  2.35it/s, v_num=5, loss=2.160]Epoch 0:   1%|          | 67/8462 [00:28<59:50,  2.34it/s, v_num=5, loss=2.100]Epoch 0:   1%|          | 68/8462 [00:28<59:33,  2.35it/s, v_num=5, loss=2.100]Epoch 0:   1%|          | 68/8462 [00:29<59:47,  2.34it/s, v_num=5, loss=1.940]Epoch 0:   1%|          | 69/8462 [00:29<59:30,  2.35it/s, v_num=5, loss=1.940]Epoch 0:   1%|          | 69/8462 [00:29<59:41,  2.34it/s, v_num=5, loss=2.250]Epoch 0:   1%|          | 70/8462 [00:29<59:25,  2.35it/s, v_num=5, loss=2.250]Epoch 0:   1%|          | 70/8462 [00:29<59:40,  2.34it/s, v_num=5, loss=2.010]Epoch 0:   1%|          | 71/8462 [00:30<59:23,  2.35it/s, v_num=5, loss=2.010]Epoch 0:   1%|          | 71/8462 [00:30<59:34,  2.35it/s, v_num=5, loss=2.250]Epoch 0:   1%|          | 72/8462 [00:30<59:18,  2.36it/s, v_num=5, loss=2.250]Epoch 0:   1%|          | 72/8462 [00:30<59:29,  2.35it/s, v_num=5, loss=2.030]Epoch 0:   1%|          | 73/8462 [00:30<59:13,  2.36it/s, v_num=5, loss=2.030]Epoch 0:   1%|          | 73/8462 [00:31<59:25,  2.35it/s, v_num=5, loss=1.970]Epoch 0:   1%|          | 74/8462 [00:31<59:09,  2.36it/s, v_num=5, loss=1.970]Epoch 0:   1%|          | 74/8462 [00:31<59:22,  2.35it/s, v_num=5, loss=1.760]Epoch 0:   1%|          | 75/8462 [00:31<59:06,  2.36it/s, v_num=5, loss=1.760]Epoch 0:   1%|          | 75/8462 [00:31<59:19,  2.36it/s, v_num=5, loss=1.910]Epoch 0:   1%|          | 76/8462 [00:32<59:06,  2.36it/s, v_num=5, loss=1.910]Epoch 0:   1%|          | 76/8462 [00:32<59:15,  2.36it/s, v_num=5, loss=2.490]Epoch 0:   1%|          | 77/8462 [00:32<59:00,  2.37it/s, v_num=5, loss=2.490]Epoch 0:   1%|          | 77/8462 [00:32<59:12,  2.36it/s, v_num=5, loss=2.440]Epoch 0:   1%|          | 78/8462 [00:32<58:57,  2.37it/s, v_num=5, loss=2.440]Epoch 0:   1%|          | 78/8462 [00:33<59:09,  2.36it/s, v_num=5, loss=1.830]Epoch 0:   1%|          | 79/8462 [00:33<58:55,  2.37it/s, v_num=5, loss=1.830]Epoch 0:   1%|          | 79/8462 [00:33<59:05,  2.36it/s, v_num=5, loss=2.150]Epoch 0:   1%|          | 80/8462 [00:33<58:52,  2.37it/s, v_num=5, loss=2.150]Epoch 0:   1%|          | 80/8462 [00:33<59:03,  2.37it/s, v_num=5, loss=1.970]Epoch 0:   1%|          | 81/8462 [00:34<58:49,  2.37it/s, v_num=5, loss=1.970]Epoch 0:   1%|          | 81/8462 [00:34<59:00,  2.37it/s, v_num=5, loss=1.850]Epoch 0:   1%|          | 82/8462 [00:34<58:45,  2.38it/s, v_num=5, loss=1.850]Epoch 0:   1%|          | 82/8462 [00:34<58:58,  2.37it/s, v_num=5, loss=1.990]Epoch 0:   1%|          | 83/8462 [00:34<58:44,  2.38it/s, v_num=5, loss=1.990]Epoch 0:   1%|          | 83/8462 [00:35<58:56,  2.37it/s, v_num=5, loss=1.970]Epoch 0:   1%|          | 84/8462 [00:35<58:43,  2.38it/s, v_num=5, loss=1.970]Epoch 0:   1%|          | 84/8462 [00:35<58:53,  2.37it/s, v_num=5, loss=2.170]Epoch 0:   1%|          | 85/8462 [00:35<58:39,  2.38it/s, v_num=5, loss=2.170]Epoch 0:   1%|          | 85/8462 [00:35<58:50,  2.37it/s, v_num=5, loss=2.180]Epoch 0:   1%|          | 86/8462 [00:36<58:36,  2.38it/s, v_num=5, loss=2.180]Epoch 0:   1%|          | 86/8462 [00:36<58:46,  2.38it/s, v_num=5, loss=2.220]Epoch 0:   1%|          | 87/8462 [00:36<58:33,  2.38it/s, v_num=5, loss=2.220]Epoch 0:   1%|          | 87/8462 [00:36<58:43,  2.38it/s, v_num=5, loss=2.100]Epoch 0:   1%|          | 88/8462 [00:36<58:31,  2.38it/s, v_num=5, loss=2.100]Epoch 0:   1%|          | 88/8462 [00:36<58:39,  2.38it/s, v_num=5, loss=2.050]Epoch 0:   1%|          | 89/8462 [00:37<58:27,  2.39it/s, v_num=5, loss=2.050]Epoch 0:   1%|          | 89/8462 [00:37<58:37,  2.38it/s, v_num=5, loss=2.060]Epoch 0:   1%|          | 90/8462 [00:37<58:24,  2.39it/s, v_num=5, loss=2.060]Epoch 0:   1%|          | 90/8462 [00:37<58:34,  2.38it/s, v_num=5, loss=1.860]Epoch 0:   1%|          | 91/8462 [00:38<58:21,  2.39it/s, v_num=5, loss=1.860]Epoch 0:   1%|          | 91/8462 [00:38<58:32,  2.38it/s, v_num=5, loss=2.100]Epoch 0:   1%|          | 92/8462 [00:38<58:19,  2.39it/s, v_num=5, loss=2.100]Epoch 0:   1%|          | 92/8462 [00:38<58:29,  2.39it/s, v_num=5, loss=2.030]Epoch 0:   1%|          | 93/8462 [00:38<58:16,  2.39it/s, v_num=5, loss=2.030]Epoch 0:   1%|          | 93/8462 [00:38<58:26,  2.39it/s, v_num=5, loss=1.850]Epoch 0:   1%|          | 94/8462 [00:39<58:13,  2.40it/s, v_num=5, loss=1.850]Epoch 0:   1%|          | 94/8462 [00:39<58:23,  2.39it/s, v_num=5, loss=2.100]Epoch 0:   1%|          | 95/8462 [00:39<58:10,  2.40it/s, v_num=5, loss=2.100]Epoch 0:   1%|          | 95/8462 [00:39<58:20,  2.39it/s, v_num=5, loss=1.820]Epoch 0:   1%|          | 96/8462 [00:40<58:09,  2.40it/s, v_num=5, loss=1.820]Epoch 0:   1%|          | 96/8462 [00:40<58:22,  2.39it/s, v_num=5, loss=1.820]Epoch 0:   1%|          | 97/8462 [00:40<58:10,  2.40it/s, v_num=5, loss=1.820]Epoch 0:   1%|          | 97/8462 [00:40<58:19,  2.39it/s, v_num=5, loss=1.860]Epoch 0:   1%|          | 98/8462 [00:40<58:07,  2.40it/s, v_num=5, loss=1.860]Epoch 0:   1%|          | 98/8462 [00:40<58:18,  2.39it/s, v_num=5, loss=2.050]Epoch 0:   1%|          | 99/8462 [00:41<58:06,  2.40it/s, v_num=5, loss=2.050]Epoch 0:   1%|          | 99/8462 [00:41<58:16,  2.39it/s, v_num=5, loss=2.000]Epoch 0:   1%|          | 100/8462 [00:41<58:05,  2.40it/s, v_num=5, loss=2.000]Epoch 0:   1%|          | 100/8462 [00:41<58:15,  2.39it/s, v_num=5, loss=1.670]Epoch 0:   1%|          | 101/8462 [00:42<58:06,  2.40it/s, v_num=5, loss=1.670]Epoch 0:   1%|          | 101/8462 [00:42<58:13,  2.39it/s, v_num=5, loss=2.220]Epoch 0:   1%|          | 102/8462 [00:42<58:03,  2.40it/s, v_num=5, loss=2.220]Epoch 0:   1%|          | 102/8462 [00:42<58:10,  2.40it/s, v_num=5, loss=2.090]Epoch 0:   1%|          | 103/8462 [00:42<58:00,  2.40it/s, v_num=5, loss=2.090]Epoch 0:   1%|          | 103/8462 [00:42<58:07,  2.40it/s, v_num=5, loss=2.120]Epoch 0:   1%|          | 104/8462 [00:43<57:56,  2.40it/s, v_num=5, loss=2.120]Epoch 0:   1%|          | 104/8462 [00:43<58:05,  2.40it/s, v_num=5, loss=2.190]Epoch 0:   1%|          | 105/8462 [00:43<57:55,  2.40it/s, v_num=5, loss=2.190]Epoch 0:   1%|          | 105/8462 [00:43<58:04,  2.40it/s, v_num=5, loss=1.640]Epoch 0:   1%|         | 106/8462 [00:44<57:54,  2.41it/s, v_num=5, loss=1.640]Epoch 0:   1%|         | 106/8462 [00:44<58:01,  2.40it/s, v_num=5, loss=1.790]Epoch 0:   1%|         | 107/8462 [00:44<57:51,  2.41it/s, v_num=5, loss=1.790]Epoch 0:   1%|         | 107/8462 [00:44<57:59,  2.40it/s, v_num=5, loss=2.400]Epoch 0:   1%|         | 108/8462 [00:44<57:49,  2.41it/s, v_num=5, loss=2.400]Epoch 0:   1%|         | 108/8462 [00:44<57:57,  2.40it/s, v_num=5, loss=2.050]Epoch 0:   1%|         | 109/8462 [00:45<57:46,  2.41it/s, v_num=5, loss=2.050]Epoch 0:   1%|         | 109/8462 [00:45<57:55,  2.40it/s, v_num=5, loss=2.150]Epoch 0:   1%|         | 110/8462 [00:45<57:45,  2.41it/s, v_num=5, loss=2.150]Epoch 0:   1%|         | 110/8462 [00:45<57:54,  2.40it/s, v_num=5, loss=1.840]Epoch 0:   1%|         | 111/8462 [00:46<57:44,  2.41it/s, v_num=5, loss=1.840]Epoch 0:   1%|         | 111/8462 [00:46<57:52,  2.40it/s, v_num=5, loss=2.040]Epoch 0:   1%|         | 112/8462 [00:46<57:42,  2.41it/s, v_num=5, loss=2.040]Epoch 0:   1%|         | 112/8462 [00:46<57:51,  2.41it/s, v_num=5, loss=1.870]Epoch 0:   1%|         | 113/8462 [00:46<57:41,  2.41it/s, v_num=5, loss=1.870]Epoch 0:   1%|         | 113/8462 [00:46<57:48,  2.41it/s, v_num=5, loss=2.210]Epoch 0:   1%|         | 114/8462 [00:47<57:39,  2.41it/s, v_num=5, loss=2.210]Epoch 0:   1%|         | 114/8462 [00:47<57:46,  2.41it/s, v_num=5, loss=1.990]Epoch 0:   1%|         | 115/8462 [00:47<57:37,  2.41it/s, v_num=5, loss=1.990]Epoch 0:   1%|         | 115/8462 [00:47<57:44,  2.41it/s, v_num=5, loss=2.030]Epoch 0:   1%|         | 116/8462 [00:48<57:34,  2.42it/s, v_num=5, loss=2.030]Epoch 0:   1%|         | 116/8462 [00:48<57:43,  2.41it/s, v_num=5, loss=1.590]Epoch 0:   1%|         | 117/8462 [00:48<57:33,  2.42it/s, v_num=5, loss=1.590]Epoch 0:   1%|         | 117/8462 [00:48<57:41,  2.41it/s, v_num=5, loss=2.100]Epoch 0:   1%|         | 118/8462 [00:48<57:32,  2.42it/s, v_num=5, loss=2.100]Epoch 0:   1%|         | 118/8462 [00:48<57:40,  2.41it/s, v_num=5, loss=1.670]Epoch 0:   1%|         | 119/8462 [00:49<57:30,  2.42it/s, v_num=5, loss=1.670]Epoch 0:   1%|         | 119/8462 [00:49<57:38,  2.41it/s, v_num=5, loss=2.010]Epoch 0:   1%|         | 120/8462 [00:49<57:28,  2.42it/s, v_num=5, loss=2.010]Epoch 0:   1%|         | 120/8462 [00:49<57:38,  2.41it/s, v_num=5, loss=2.030]Epoch 0:   1%|         | 121/8462 [00:50<57:29,  2.42it/s, v_num=5, loss=2.030]Epoch 0:   1%|         | 121/8462 [00:50<57:38,  2.41it/s, v_num=5, loss=1.870]Epoch 0:   1%|         | 122/8462 [00:50<57:29,  2.42it/s, v_num=5, loss=1.870]Epoch 0:   1%|         | 122/8462 [00:50<57:36,  2.41it/s, v_num=5, loss=1.930]Epoch 0:   1%|         | 123/8462 [00:50<57:28,  2.42it/s, v_num=5, loss=1.930]Epoch 0:   1%|         | 123/8462 [00:50<57:35,  2.41it/s, v_num=5, loss=1.750]Epoch 0:   1%|         | 124/8462 [00:51<57:27,  2.42it/s, v_num=5, loss=1.750]Epoch 0:   1%|         | 124/8462 [00:51<57:34,  2.41it/s, v_num=5, loss=1.960]Epoch 0:   1%|         | 125/8462 [00:51<57:24,  2.42it/s, v_num=5, loss=1.960]Epoch 0:   1%|         | 125/8462 [00:51<57:32,  2.41it/s, v_num=5, loss=1.980]Epoch 0:   1%|         | 126/8462 [00:52<57:23,  2.42it/s, v_num=5, loss=1.980]Epoch 0:   1%|         | 126/8462 [00:52<57:31,  2.42it/s, v_num=5, loss=1.700]Epoch 0:   2%|         | 127/8462 [00:52<57:22,  2.42it/s, v_num=5, loss=1.700]Epoch 0:   2%|         | 127/8462 [00:52<57:30,  2.42it/s, v_num=5, loss=1.830]Epoch 0:   2%|         | 128/8462 [00:52<57:21,  2.42it/s, v_num=5, loss=1.830]Epoch 0:   2%|         | 128/8462 [00:52<57:28,  2.42it/s, v_num=5, loss=1.760]Epoch 0:   2%|         | 129/8462 [00:53<57:21,  2.42it/s, v_num=5, loss=1.760]Epoch 0:   2%|         | 129/8462 [00:53<57:27,  2.42it/s, v_num=5, loss=1.730]Epoch 0:   2%|         | 130/8462 [00:53<57:19,  2.42it/s, v_num=5, loss=1.730]Epoch 0:   2%|         | 130/8462 [00:53<57:25,  2.42it/s, v_num=5, loss=1.940]Epoch 0:   2%|         | 131/8462 [00:54<57:17,  2.42it/s, v_num=5, loss=1.940]Epoch 0:   2%|         | 131/8462 [00:54<57:27,  2.42it/s, v_num=5, loss=1.830]Epoch 0:   2%|         | 132/8462 [00:54<57:18,  2.42it/s, v_num=5, loss=1.830]Epoch 0:   2%|         | 132/8462 [00:54<57:26,  2.42it/s, v_num=5, loss=2.040]Epoch 0:   2%|         | 133/8462 [00:54<57:17,  2.42it/s, v_num=5, loss=2.040]Epoch 0:   2%|         | 133/8462 [00:55<57:24,  2.42it/s, v_num=5, loss=2.030]Epoch 0:   2%|         | 134/8462 [00:55<57:16,  2.42it/s, v_num=5, loss=2.030]Epoch 0:   2%|         | 134/8462 [00:55<57:24,  2.42it/s, v_num=5, loss=2.020]Epoch 0:   2%|         | 135/8462 [00:55<57:16,  2.42it/s, v_num=5, loss=2.020]Epoch 0:   2%|         | 135/8462 [00:55<57:26,  2.42it/s, v_num=5, loss=2.180]Epoch 0:   2%|         | 136/8462 [00:56<57:18,  2.42it/s, v_num=5, loss=2.180]Epoch 0:   2%|         | 136/8462 [00:56<57:25,  2.42it/s, v_num=5, loss=1.590]Epoch 0:   2%|         | 137/8462 [00:56<57:17,  2.42it/s, v_num=5, loss=1.590]Epoch 0:   2%|         | 137/8462 [00:56<57:25,  2.42it/s, v_num=5, loss=1.970]Epoch 0:   2%|         | 138/8462 [00:56<57:17,  2.42it/s, v_num=5, loss=1.970]Epoch 0:   2%|         | 138/8462 [00:57<57:25,  2.42it/s, v_num=5, loss=1.370]Epoch 0:   2%|         | 139/8462 [00:57<57:17,  2.42it/s, v_num=5, loss=1.370]Epoch 0:   2%|         | 139/8462 [00:57<57:24,  2.42it/s, v_num=5, loss=1.770]Epoch 0:   2%|         | 140/8462 [00:57<57:16,  2.42it/s, v_num=5, loss=1.770]Epoch 0:   2%|         | 140/8462 [00:57<57:22,  2.42it/s, v_num=5, loss=1.720]Epoch 0:   2%|         | 141/8462 [00:58<57:14,  2.42it/s, v_num=5, loss=1.720]Epoch 0:   2%|         | 141/8462 [00:58<57:21,  2.42it/s, v_num=5, loss=1.510]Epoch 0:   2%|         | 142/8462 [00:58<57:13,  2.42it/s, v_num=5, loss=1.510]Epoch 0:   2%|         | 142/8462 [00:58<57:19,  2.42it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 143/8462 [00:58<57:12,  2.42it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 143/8462 [00:59<57:18,  2.42it/s, v_num=5, loss=2.090]Epoch 0:   2%|         | 144/8462 [00:59<57:10,  2.42it/s, v_num=5, loss=2.090]Epoch 0:   2%|         | 144/8462 [00:59<57:17,  2.42it/s, v_num=5, loss=1.560]Epoch 0:   2%|         | 145/8462 [00:59<57:09,  2.42it/s, v_num=5, loss=1.560]Epoch 0:   2%|         | 145/8462 [00:59<57:16,  2.42it/s, v_num=5, loss=2.110]Epoch 0:   2%|         | 146/8462 [01:00<57:09,  2.43it/s, v_num=5, loss=2.110]Epoch 0:   2%|         | 146/8462 [01:00<57:15,  2.42it/s, v_num=5, loss=1.880]Epoch 0:   2%|         | 147/8462 [01:00<57:07,  2.43it/s, v_num=5, loss=1.880]Epoch 0:   2%|         | 147/8462 [01:00<57:13,  2.42it/s, v_num=5, loss=1.910]Epoch 0:   2%|         | 148/8462 [01:00<57:06,  2.43it/s, v_num=5, loss=1.910]Epoch 0:   2%|         | 148/8462 [01:01<57:13,  2.42it/s, v_num=5, loss=1.850]Epoch 0:   2%|         | 149/8462 [01:01<57:05,  2.43it/s, v_num=5, loss=1.850]Epoch 0:   2%|         | 149/8462 [01:01<57:11,  2.42it/s, v_num=5, loss=1.800]Epoch 0:   2%|         | 150/8462 [01:01<57:04,  2.43it/s, v_num=5, loss=1.800]Epoch 0:   2%|         | 150/8462 [01:01<57:10,  2.42it/s, v_num=5, loss=1.970]Epoch 0:   2%|         | 151/8462 [01:02<57:03,  2.43it/s, v_num=5, loss=1.970]Epoch 0:   2%|         | 151/8462 [01:02<57:09,  2.42it/s, v_num=5, loss=2.060]Epoch 0:   2%|         | 152/8462 [01:02<57:02,  2.43it/s, v_num=5, loss=2.060]Epoch 0:   2%|         | 152/8462 [01:02<57:08,  2.42it/s, v_num=5, loss=1.670]Epoch 0:   2%|         | 153/8462 [01:03<57:01,  2.43it/s, v_num=5, loss=1.670]Epoch 0:   2%|         | 153/8462 [01:03<57:07,  2.42it/s, v_num=5, loss=1.750]Epoch 0:   2%|         | 154/8462 [01:03<57:00,  2.43it/s, v_num=5, loss=1.750]Epoch 0:   2%|         | 154/8462 [01:03<57:06,  2.42it/s, v_num=5, loss=1.990]Epoch 0:   2%|         | 155/8462 [01:03<56:59,  2.43it/s, v_num=5, loss=1.990]Epoch 0:   2%|         | 155/8462 [01:03<57:05,  2.42it/s, v_num=5, loss=1.590]Epoch 0:   2%|         | 156/8462 [01:04<56:58,  2.43it/s, v_num=5, loss=1.590]Epoch 0:   2%|         | 156/8462 [01:04<57:04,  2.43it/s, v_num=5, loss=2.180]Epoch 0:   2%|         | 157/8462 [01:04<56:58,  2.43it/s, v_num=5, loss=2.180]Epoch 0:   2%|         | 157/8462 [01:04<57:03,  2.43it/s, v_num=5, loss=1.890]Epoch 0:   2%|         | 158/8462 [01:05<56:56,  2.43it/s, v_num=5, loss=1.890]Epoch 0:   2%|         | 158/8462 [01:05<57:02,  2.43it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 159/8462 [01:05<56:55,  2.43it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 159/8462 [01:05<57:01,  2.43it/s, v_num=5, loss=2.080]Epoch 0:   2%|         | 160/8462 [01:05<56:54,  2.43it/s, v_num=5, loss=2.080]Epoch 0:   2%|         | 160/8462 [01:05<57:01,  2.43it/s, v_num=5, loss=1.890]Epoch 0:   2%|         | 161/8462 [01:06<56:54,  2.43it/s, v_num=5, loss=1.890]Epoch 0:   2%|         | 161/8462 [01:06<57:00,  2.43it/s, v_num=5, loss=1.690]Epoch 0:   2%|         | 162/8462 [01:06<56:53,  2.43it/s, v_num=5, loss=1.690]Epoch 0:   2%|         | 162/8462 [01:06<57:00,  2.43it/s, v_num=5, loss=1.620]Epoch 0:   2%|         | 163/8462 [01:07<56:53,  2.43it/s, v_num=5, loss=1.620]Epoch 0:   2%|         | 163/8462 [01:07<56:59,  2.43it/s, v_num=5, loss=1.970]Epoch 0:   2%|         | 164/8462 [01:07<56:53,  2.43it/s, v_num=5, loss=1.970]Epoch 0:   2%|         | 164/8462 [01:07<56:58,  2.43it/s, v_num=5, loss=1.640]Epoch 0:   2%|         | 165/8462 [01:07<56:52,  2.43it/s, v_num=5, loss=1.640]Epoch 0:   2%|         | 165/8462 [01:07<56:57,  2.43it/s, v_num=5, loss=1.470]Epoch 0:   2%|         | 166/8462 [01:08<56:51,  2.43it/s, v_num=5, loss=1.470]Epoch 0:   2%|         | 166/8462 [01:08<56:56,  2.43it/s, v_num=5, loss=2.190]Epoch 0:   2%|         | 167/8462 [01:08<56:50,  2.43it/s, v_num=5, loss=2.190]Epoch 0:   2%|         | 167/8462 [01:08<56:55,  2.43it/s, v_num=5, loss=1.890]Epoch 0:   2%|         | 168/8462 [01:09<56:49,  2.43it/s, v_num=5, loss=1.890]Epoch 0:   2%|         | 168/8462 [01:09<56:55,  2.43it/s, v_num=5, loss=1.850]Epoch 0:   2%|         | 169/8462 [01:09<56:49,  2.43it/s, v_num=5, loss=1.850]Epoch 0:   2%|         | 169/8462 [01:09<56:54,  2.43it/s, v_num=5, loss=1.300]Epoch 0:   2%|         | 170/8462 [01:09<56:48,  2.43it/s, v_num=5, loss=1.300]Epoch 0:   2%|         | 170/8462 [01:09<56:53,  2.43it/s, v_num=5, loss=1.780]Epoch 0:   2%|         | 171/8462 [01:10<56:46,  2.43it/s, v_num=5, loss=1.780]Epoch 0:   2%|         | 171/8462 [01:10<56:52,  2.43it/s, v_num=5, loss=2.170]Epoch 0:   2%|         | 172/8462 [01:10<56:45,  2.43it/s, v_num=5, loss=2.170]Epoch 0:   2%|         | 172/8462 [01:10<56:51,  2.43it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 173/8462 [01:11<56:45,  2.43it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 173/8462 [01:11<56:50,  2.43it/s, v_num=5, loss=1.860]Epoch 0:   2%|         | 174/8462 [01:11<56:44,  2.43it/s, v_num=5, loss=1.860]Epoch 0:   2%|         | 174/8462 [01:11<56:50,  2.43it/s, v_num=5, loss=1.870]Epoch 0:   2%|         | 175/8462 [01:11<56:44,  2.43it/s, v_num=5, loss=1.870]Epoch 0:   2%|         | 175/8462 [01:12<56:49,  2.43it/s, v_num=5, loss=1.480]Epoch 0:   2%|         | 176/8462 [01:12<56:44,  2.43it/s, v_num=5, loss=1.480]Epoch 0:   2%|         | 176/8462 [01:12<56:48,  2.43it/s, v_num=5, loss=1.710]Epoch 0:   2%|         | 177/8462 [01:12<56:42,  2.43it/s, v_num=5, loss=1.710]Epoch 0:   2%|         | 177/8462 [01:12<56:47,  2.43it/s, v_num=5, loss=1.910]Epoch 0:   2%|         | 178/8462 [01:13<56:41,  2.44it/s, v_num=5, loss=1.910]Epoch 0:   2%|         | 178/8462 [01:13<56:46,  2.43it/s, v_num=5, loss=1.620]Epoch 0:   2%|         | 179/8462 [01:13<56:40,  2.44it/s, v_num=5, loss=1.620]Epoch 0:   2%|         | 179/8462 [01:13<56:46,  2.43it/s, v_num=5, loss=1.660]Epoch 0:   2%|         | 180/8462 [01:13<56:40,  2.44it/s, v_num=5, loss=1.660]Epoch 0:   2%|         | 180/8462 [01:14<56:44,  2.43it/s, v_num=5, loss=1.790]Epoch 0:   2%|         | 181/8462 [01:14<56:38,  2.44it/s, v_num=5, loss=1.790]Epoch 0:   2%|         | 181/8462 [01:14<56:43,  2.43it/s, v_num=5, loss=1.400]Epoch 0:   2%|         | 182/8462 [01:14<56:37,  2.44it/s, v_num=5, loss=1.400]Epoch 0:   2%|         | 182/8462 [01:14<56:42,  2.43it/s, v_num=5, loss=1.730]Epoch 0:   2%|         | 183/8462 [01:15<56:36,  2.44it/s, v_num=5, loss=1.730]Epoch 0:   2%|         | 183/8462 [01:15<56:41,  2.43it/s, v_num=5, loss=1.750]Epoch 0:   2%|         | 184/8462 [01:15<56:35,  2.44it/s, v_num=5, loss=1.750]Epoch 0:   2%|         | 184/8462 [01:15<56:41,  2.43it/s, v_num=5, loss=1.500]Epoch 0:   2%|         | 185/8462 [01:15<56:35,  2.44it/s, v_num=5, loss=1.500]Epoch 0:   2%|         | 185/8462 [01:16<56:40,  2.43it/s, v_num=5, loss=2.150]Epoch 0:   2%|         | 186/8462 [01:16<56:34,  2.44it/s, v_num=5, loss=2.150]Epoch 0:   2%|         | 186/8462 [01:16<56:39,  2.43it/s, v_num=5, loss=1.940]Epoch 0:   2%|         | 187/8462 [01:16<56:33,  2.44it/s, v_num=5, loss=1.940]Epoch 0:   2%|         | 187/8462 [01:16<56:39,  2.43it/s, v_num=5, loss=1.570]Epoch 0:   2%|         | 188/8462 [01:17<56:33,  2.44it/s, v_num=5, loss=1.570]Epoch 0:   2%|         | 188/8462 [01:17<56:38,  2.43it/s, v_num=5, loss=2.080]Epoch 0:   2%|         | 189/8462 [01:17<56:32,  2.44it/s, v_num=5, loss=2.080]Epoch 0:   2%|         | 189/8462 [01:17<56:37,  2.44it/s, v_num=5, loss=1.920]Epoch 0:   2%|         | 190/8462 [01:17<56:31,  2.44it/s, v_num=5, loss=1.920]Epoch 0:   2%|         | 190/8462 [01:18<56:37,  2.43it/s, v_num=5, loss=1.960]Epoch 0:   2%|         | 191/8462 [01:18<56:31,  2.44it/s, v_num=5, loss=1.960]Epoch 0:   2%|         | 191/8462 [01:18<56:36,  2.44it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 192/8462 [01:18<56:30,  2.44it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 192/8462 [01:18<56:35,  2.44it/s, v_num=5, loss=2.010]Epoch 0:   2%|         | 193/8462 [01:19<56:29,  2.44it/s, v_num=5, loss=2.010]Epoch 0:   2%|         | 193/8462 [01:19<56:34,  2.44it/s, v_num=5, loss=1.750]Epoch 0:   2%|         | 194/8462 [01:19<56:28,  2.44it/s, v_num=5, loss=1.750]Epoch 0:   2%|         | 194/8462 [01:19<56:33,  2.44it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 195/8462 [01:19<56:27,  2.44it/s, v_num=5, loss=1.930]Epoch 0:   2%|         | 195/8462 [01:20<56:32,  2.44it/s, v_num=5, loss=1.390]Epoch 0:   2%|         | 196/8462 [01:20<56:26,  2.44it/s, v_num=5, loss=1.390]Epoch 0:   2%|         | 196/8462 [01:20<56:31,  2.44it/s, v_num=5, loss=1.760]Epoch 0:   2%|         | 197/8462 [01:20<56:26,  2.44it/s, v_num=5, loss=1.760]Epoch 0:   2%|         | 197/8462 [01:20<56:31,  2.44it/s, v_num=5, loss=1.780]Epoch 0:   2%|         | 198/8462 [01:21<56:25,  2.44it/s, v_num=5, loss=1.780]Epoch 0:   2%|         | 198/8462 [01:21<56:29,  2.44it/s, v_num=5, loss=1.880]Epoch 0:   2%|         | 199/8462 [01:21<56:24,  2.44it/s, v_num=5, loss=1.880]Epoch 0:   2%|         | 199/8462 [01:21<56:29,  2.44it/s, v_num=5, loss=1.950]Epoch 0:   2%|         | 200/8462 [01:21<56:24,  2.44it/s, v_num=5, loss=1.950]Epoch 0:   2%|         | 200/8462 [01:22<56:28,  2.44it/s, v_num=5, loss=1.920]Epoch 0:   2%|         | 201/8462 [01:22<56:24,  2.44it/s, v_num=5, loss=1.920]Epoch 0:   2%|         | 201/8462 [01:22<56:28,  2.44it/s, v_num=5, loss=1.610]Epoch 0:   2%|         | 202/8462 [01:22<56:22,  2.44it/s, v_num=5, loss=1.610]Epoch 0:   2%|         | 202/8462 [01:22<56:27,  2.44it/s, v_num=5, loss=1.590]Epoch 0:   2%|         | 203/8462 [01:23<56:21,  2.44it/s, v_num=5, loss=1.590]Epoch 0:   2%|         | 203/8462 [01:23<56:26,  2.44it/s, v_num=5, loss=1.770]Epoch 0:   2%|         | 204/8462 [01:23<56:21,  2.44it/s, v_num=5, loss=1.770]Epoch 0:   2%|         | 204/8462 [01:23<56:25,  2.44it/s, v_num=5, loss=1.840]Epoch 0:   2%|         | 205/8462 [01:23<56:20,  2.44it/s, v_num=5, loss=1.840]Epoch 0:   2%|         | 205/8462 [01:24<56:25,  2.44it/s, v_num=5, loss=2.180]Epoch 0:   2%|         | 206/8462 [01:24<56:20,  2.44it/s, v_num=5, loss=2.180]Epoch 0:   2%|         | 206/8462 [01:24<56:24,  2.44it/s, v_num=5, loss=2.030]Epoch 0:   2%|         | 207/8462 [01:24<56:19,  2.44it/s, v_num=5, loss=2.030]Epoch 0:   2%|         | 207/8462 [01:24<56:24,  2.44it/s, v_num=5, loss=1.650]Epoch 0:   2%|         | 208/8462 [01:25<56:19,  2.44it/s, v_num=5, loss=1.650]Epoch 0:   2%|         | 208/8462 [01:25<56:24,  2.44it/s, v_num=5, loss=1.670]Epoch 0:   2%|         | 209/8462 [01:25<56:18,  2.44it/s, v_num=5, loss=1.670]Epoch 0:   2%|         | 209/8462 [01:25<56:23,  2.44it/s, v_num=5, loss=2.110]Epoch 0:   2%|         | 210/8462 [01:25<56:18,  2.44it/s, v_num=5, loss=2.110]Epoch 0:   2%|         | 210/8462 [01:26<56:22,  2.44it/s, v_num=5, loss=1.390]Epoch 0:   2%|         | 211/8462 [01:26<56:17,  2.44it/s, v_num=5, loss=1.390]Epoch 0:   2%|         | 211/8462 [01:26<56:21,  2.44it/s, v_num=5, loss=2.270]Epoch 0:   3%|         | 212/8462 [01:26<56:16,  2.44it/s, v_num=5, loss=2.270]Epoch 0:   3%|         | 212/8462 [01:26<56:20,  2.44it/s, v_num=5, loss=1.810]Epoch 0:   3%|         | 213/8462 [01:27<56:15,  2.44it/s, v_num=5, loss=1.810]Epoch 0:   3%|         | 213/8462 [01:27<56:20,  2.44it/s, v_num=5, loss=1.690]Epoch 0:   3%|         | 214/8462 [01:27<56:15,  2.44it/s, v_num=5, loss=1.690]Epoch 0:   3%|         | 214/8462 [01:27<56:19,  2.44it/s, v_num=5, loss=2.120]Epoch 0:   3%|         | 215/8462 [01:27<56:14,  2.44it/s, v_num=5, loss=2.120]Epoch 0:   3%|         | 215/8462 [01:28<56:19,  2.44it/s, v_num=5, loss=1.820]Epoch 0:   3%|         | 216/8462 [01:28<56:14,  2.44it/s, v_num=5, loss=1.820]Epoch 0:   3%|         | 216/8462 [01:28<56:18,  2.44it/s, v_num=5, loss=2.150]Epoch 0:   3%|         | 217/8462 [01:28<56:14,  2.44it/s, v_num=5, loss=2.150]Epoch 0:   3%|         | 217/8462 [01:28<56:18,  2.44it/s, v_num=5, loss=1.700]Epoch 0:   3%|         | 218/8462 [01:29<56:13,  2.44it/s, v_num=5, loss=1.700]Epoch 0:   3%|         | 218/8462 [01:29<56:17,  2.44it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 219/8462 [01:29<56:12,  2.44it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 219/8462 [01:29<56:16,  2.44it/s, v_num=5, loss=2.020]Epoch 0:   3%|         | 220/8462 [01:30<56:11,  2.44it/s, v_num=5, loss=2.020]Epoch 0:   3%|         | 220/8462 [01:30<56:16,  2.44it/s, v_num=5, loss=1.940]Epoch 0:   3%|         | 221/8462 [01:30<56:11,  2.44it/s, v_num=5, loss=1.940]Epoch 0:   3%|         | 221/8462 [01:30<56:15,  2.44it/s, v_num=5, loss=1.990]Epoch 0:   3%|         | 222/8462 [01:30<56:10,  2.44it/s, v_num=5, loss=1.990]Epoch 0:   3%|         | 222/8462 [01:30<56:14,  2.44it/s, v_num=5, loss=1.880]Epoch 0:   3%|         | 223/8462 [01:31<56:09,  2.44it/s, v_num=5, loss=1.880]Epoch 0:   3%|         | 223/8462 [01:31<56:14,  2.44it/s, v_num=5, loss=1.750]Epoch 0:   3%|         | 224/8462 [01:31<56:09,  2.45it/s, v_num=5, loss=1.750]Epoch 0:   3%|         | 224/8462 [01:31<56:13,  2.44it/s, v_num=5, loss=1.960]Epoch 0:   3%|         | 225/8462 [01:32<56:08,  2.45it/s, v_num=5, loss=1.960]Epoch 0:   3%|         | 225/8462 [01:32<56:12,  2.44it/s, v_num=5, loss=1.950]Epoch 0:   3%|         | 226/8462 [01:32<56:07,  2.45it/s, v_num=5, loss=1.950]Epoch 0:   3%|         | 226/8462 [01:32<56:11,  2.44it/s, v_num=5, loss=1.730]Epoch 0:   3%|         | 227/8462 [01:32<56:06,  2.45it/s, v_num=5, loss=1.730]Epoch 0:   3%|         | 227/8462 [01:32<56:10,  2.44it/s, v_num=5, loss=1.470]Epoch 0:   3%|         | 228/8462 [01:33<56:05,  2.45it/s, v_num=5, loss=1.470]Epoch 0:   3%|         | 228/8462 [01:33<56:09,  2.44it/s, v_num=5, loss=2.080]Epoch 0:   3%|         | 229/8462 [01:33<56:05,  2.45it/s, v_num=5, loss=2.080]Epoch 0:   3%|         | 229/8462 [01:33<56:09,  2.44it/s, v_num=5, loss=1.530]Epoch 0:   3%|         | 230/8462 [01:34<56:04,  2.45it/s, v_num=5, loss=1.530]Epoch 0:   3%|         | 230/8462 [01:34<56:09,  2.44it/s, v_num=5, loss=1.630]Epoch 0:   3%|         | 231/8462 [01:34<56:04,  2.45it/s, v_num=5, loss=1.630]Epoch 0:   3%|         | 231/8462 [01:34<56:08,  2.44it/s, v_num=5, loss=1.930]Epoch 0:   3%|         | 232/8462 [01:34<56:04,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   3%|         | 232/8462 [01:34<56:08,  2.44it/s, v_num=5, loss=2.020]Epoch 0:   3%|         | 233/8462 [01:35<56:03,  2.45it/s, v_num=5, loss=2.020]Epoch 0:   3%|         | 233/8462 [01:35<56:07,  2.44it/s, v_num=5, loss=2.210]Epoch 0:   3%|         | 234/8462 [01:35<56:02,  2.45it/s, v_num=5, loss=2.210]Epoch 0:   3%|         | 234/8462 [01:35<56:06,  2.44it/s, v_num=5, loss=1.680]Epoch 0:   3%|         | 235/8462 [01:36<56:02,  2.45it/s, v_num=5, loss=1.680]Epoch 0:   3%|         | 235/8462 [01:36<56:05,  2.44it/s, v_num=5, loss=1.660]Epoch 0:   3%|         | 236/8462 [01:36<56:01,  2.45it/s, v_num=5, loss=1.660]Epoch 0:   3%|         | 236/8462 [01:36<56:05,  2.44it/s, v_num=5, loss=1.800]Epoch 0:   3%|         | 237/8462 [01:36<56:00,  2.45it/s, v_num=5, loss=1.800]Epoch 0:   3%|         | 237/8462 [01:36<56:04,  2.44it/s, v_num=5, loss=1.990]Epoch 0:   3%|         | 238/8462 [01:37<55:59,  2.45it/s, v_num=5, loss=1.990]Epoch 0:   3%|         | 238/8462 [01:37<56:03,  2.44it/s, v_num=5, loss=1.750]Epoch 0:   3%|         | 239/8462 [01:37<55:59,  2.45it/s, v_num=5, loss=1.750]Epoch 0:   3%|         | 239/8462 [01:37<56:03,  2.45it/s, v_num=5, loss=1.640]Epoch 0:   3%|         | 240/8462 [01:38<55:58,  2.45it/s, v_num=5, loss=1.640]Epoch 0:   3%|         | 240/8462 [01:38<56:02,  2.45it/s, v_num=5, loss=1.650]Epoch 0:   3%|         | 241/8462 [01:38<55:57,  2.45it/s, v_num=5, loss=1.650]Epoch 0:   3%|         | 241/8462 [01:38<56:01,  2.45it/s, v_num=5, loss=1.790]Epoch 0:   3%|         | 242/8462 [01:38<55:57,  2.45it/s, v_num=5, loss=1.790]Epoch 0:   3%|         | 242/8462 [01:38<56:01,  2.45it/s, v_num=5, loss=1.590]Epoch 0:   3%|         | 243/8462 [01:39<55:56,  2.45it/s, v_num=5, loss=1.590]Epoch 0:   3%|         | 243/8462 [01:39<56:00,  2.45it/s, v_num=5, loss=1.910]Epoch 0:   3%|         | 244/8462 [01:39<55:55,  2.45it/s, v_num=5, loss=1.910]Epoch 0:   3%|         | 244/8462 [01:39<55:59,  2.45it/s, v_num=5, loss=2.040]Epoch 0:   3%|         | 245/8462 [01:40<55:55,  2.45it/s, v_num=5, loss=2.040]Epoch 0:   3%|         | 245/8462 [01:40<55:59,  2.45it/s, v_num=5, loss=1.880]Epoch 0:   3%|         | 246/8462 [01:40<55:54,  2.45it/s, v_num=5, loss=1.880]Epoch 0:   3%|         | 246/8462 [01:40<55:59,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   3%|         | 247/8462 [01:40<55:54,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   3%|         | 247/8462 [01:40<55:58,  2.45it/s, v_num=5, loss=1.750]Epoch 0:   3%|         | 248/8462 [01:41<55:53,  2.45it/s, v_num=5, loss=1.750]Epoch 0:   3%|         | 248/8462 [01:41<55:57,  2.45it/s, v_num=5, loss=2.020]Epoch 0:   3%|         | 249/8462 [01:41<55:53,  2.45it/s, v_num=5, loss=2.020]Epoch 0:   3%|         | 249/8462 [01:41<55:56,  2.45it/s, v_num=5, loss=1.790]Epoch 0:   3%|         | 250/8462 [01:42<55:52,  2.45it/s, v_num=5, loss=1.790]Epoch 0:   3%|         | 250/8462 [01:42<55:56,  2.45it/s, v_num=5, loss=1.780]Epoch 0:   3%|         | 251/8462 [01:42<55:52,  2.45it/s, v_num=5, loss=1.780]Epoch 0:   3%|         | 251/8462 [01:42<55:58,  2.44it/s, v_num=5, loss=2.050]Epoch 0:   3%|         | 252/8462 [01:42<55:54,  2.45it/s, v_num=5, loss=2.050]Epoch 0:   3%|         | 252/8462 [01:43<55:57,  2.45it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 253/8462 [01:43<55:53,  2.45it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 253/8462 [01:43<55:57,  2.45it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 254/8462 [01:43<55:52,  2.45it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 254/8462 [01:43<55:56,  2.45it/s, v_num=5, loss=1.900]Epoch 0:   3%|         | 255/8462 [01:44<55:52,  2.45it/s, v_num=5, loss=1.900]Epoch 0:   3%|         | 255/8462 [01:44<55:56,  2.45it/s, v_num=5, loss=1.800]Epoch 0:   3%|         | 256/8462 [01:44<55:52,  2.45it/s, v_num=5, loss=1.800]Epoch 0:   3%|         | 256/8462 [01:44<55:55,  2.45it/s, v_num=5, loss=1.770]Epoch 0:   3%|         | 257/8462 [01:44<55:51,  2.45it/s, v_num=5, loss=1.770]Epoch 0:   3%|         | 257/8462 [01:45<55:54,  2.45it/s, v_num=5, loss=1.850]Epoch 0:   3%|         | 258/8462 [01:45<55:50,  2.45it/s, v_num=5, loss=1.850]Epoch 0:   3%|         | 258/8462 [01:45<55:54,  2.45it/s, v_num=5, loss=2.200]Epoch 0:   3%|         | 259/8462 [01:45<55:50,  2.45it/s, v_num=5, loss=2.200]Epoch 0:   3%|         | 259/8462 [01:45<55:53,  2.45it/s, v_num=5, loss=2.000]Epoch 0:   3%|         | 260/8462 [01:46<55:49,  2.45it/s, v_num=5, loss=2.000]Epoch 0:   3%|         | 260/8462 [01:46<55:52,  2.45it/s, v_num=5, loss=1.250]Epoch 0:   3%|         | 261/8462 [01:46<55:48,  2.45it/s, v_num=5, loss=1.250]Epoch 0:   3%|         | 261/8462 [01:46<55:52,  2.45it/s, v_num=5, loss=1.730]Epoch 0:   3%|         | 262/8462 [01:46<55:48,  2.45it/s, v_num=5, loss=1.730]Epoch 0:   3%|         | 262/8462 [01:47<55:51,  2.45it/s, v_num=5, loss=1.520]Epoch 0:   3%|         | 263/8462 [01:47<55:47,  2.45it/s, v_num=5, loss=1.520]Epoch 0:   3%|         | 263/8462 [01:47<55:51,  2.45it/s, v_num=5, loss=1.610]Epoch 0:   3%|         | 264/8462 [01:47<55:47,  2.45it/s, v_num=5, loss=1.610]Epoch 0:   3%|         | 264/8462 [01:47<55:51,  2.45it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 265/8462 [01:48<55:46,  2.45it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 265/8462 [01:48<55:50,  2.45it/s, v_num=5, loss=1.600]Epoch 0:   3%|         | 266/8462 [01:48<55:46,  2.45it/s, v_num=5, loss=1.600]Epoch 0:   3%|         | 266/8462 [01:48<55:49,  2.45it/s, v_num=5, loss=1.870]Epoch 0:   3%|         | 267/8462 [01:49<55:45,  2.45it/s, v_num=5, loss=1.870]Epoch 0:   3%|         | 267/8462 [01:49<55:48,  2.45it/s, v_num=5, loss=2.170]Epoch 0:   3%|         | 268/8462 [01:49<55:44,  2.45it/s, v_num=5, loss=2.170]Epoch 0:   3%|         | 268/8462 [01:49<55:48,  2.45it/s, v_num=5, loss=1.590]Epoch 0:   3%|         | 269/8462 [01:49<55:44,  2.45it/s, v_num=5, loss=1.590]Epoch 0:   3%|         | 269/8462 [01:49<55:47,  2.45it/s, v_num=5, loss=1.960]Epoch 0:   3%|         | 270/8462 [01:50<55:43,  2.45it/s, v_num=5, loss=1.960]Epoch 0:   3%|         | 270/8462 [01:50<55:47,  2.45it/s, v_num=5, loss=1.880]Epoch 0:   3%|         | 271/8462 [01:50<55:43,  2.45it/s, v_num=5, loss=1.880]Epoch 0:   3%|         | 271/8462 [01:50<55:46,  2.45it/s, v_num=5, loss=1.770]Epoch 0:   3%|         | 272/8462 [01:51<55:42,  2.45it/s, v_num=5, loss=1.770]Epoch 0:   3%|         | 272/8462 [01:51<55:45,  2.45it/s, v_num=5, loss=1.760]Epoch 0:   3%|         | 273/8462 [01:51<55:41,  2.45it/s, v_num=5, loss=1.760]Epoch 0:   3%|         | 273/8462 [01:51<55:45,  2.45it/s, v_num=5, loss=1.630]Epoch 0:   3%|         | 274/8462 [01:51<55:41,  2.45it/s, v_num=5, loss=1.630]Epoch 0:   3%|         | 274/8462 [01:51<55:44,  2.45it/s, v_num=5, loss=1.230]Epoch 0:   3%|         | 275/8462 [01:52<55:40,  2.45it/s, v_num=5, loss=1.230]Epoch 0:   3%|         | 275/8462 [01:52<55:43,  2.45it/s, v_num=5, loss=1.750]Epoch 0:   3%|         | 276/8462 [01:52<55:39,  2.45it/s, v_num=5, loss=1.750]Epoch 0:   3%|         | 276/8462 [01:52<55:43,  2.45it/s, v_num=5, loss=2.050]Epoch 0:   3%|         | 277/8462 [01:53<55:39,  2.45it/s, v_num=5, loss=2.050]Epoch 0:   3%|         | 277/8462 [01:53<55:42,  2.45it/s, v_num=5, loss=1.620]Epoch 0:   3%|         | 278/8462 [01:53<55:38,  2.45it/s, v_num=5, loss=1.620]Epoch 0:   3%|         | 278/8462 [01:53<55:41,  2.45it/s, v_num=5, loss=2.040]Epoch 0:   3%|         | 279/8462 [01:53<55:38,  2.45it/s, v_num=5, loss=2.040]Epoch 0:   3%|         | 279/8462 [01:53<55:41,  2.45it/s, v_num=5, loss=1.950]Epoch 0:   3%|         | 280/8462 [01:54<55:37,  2.45it/s, v_num=5, loss=1.950]Epoch 0:   3%|         | 280/8462 [01:54<55:40,  2.45it/s, v_num=5, loss=2.090]Epoch 0:   3%|         | 281/8462 [01:54<55:37,  2.45it/s, v_num=5, loss=2.090]Epoch 0:   3%|         | 281/8462 [01:54<55:40,  2.45it/s, v_num=5, loss=1.970]Epoch 0:   3%|         | 282/8462 [01:55<55:36,  2.45it/s, v_num=5, loss=1.970]Epoch 0:   3%|         | 282/8462 [01:55<55:39,  2.45it/s, v_num=5, loss=2.520]Epoch 0:   3%|         | 283/8462 [01:55<55:35,  2.45it/s, v_num=5, loss=2.520]Epoch 0:   3%|         | 283/8462 [01:55<55:38,  2.45it/s, v_num=5, loss=1.940]Epoch 0:   3%|         | 284/8462 [01:55<55:35,  2.45it/s, v_num=5, loss=1.940]Epoch 0:   3%|         | 284/8462 [01:55<55:38,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   3%|         | 285/8462 [01:56<55:34,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   3%|         | 285/8462 [01:56<55:37,  2.45it/s, v_num=5, loss=2.210]Epoch 0:   3%|         | 286/8462 [01:56<55:34,  2.45it/s, v_num=5, loss=2.210]Epoch 0:   3%|         | 286/8462 [01:56<55:37,  2.45it/s, v_num=5, loss=1.330]Epoch 0:   3%|         | 287/8462 [01:57<55:33,  2.45it/s, v_num=5, loss=1.330]Epoch 0:   3%|         | 287/8462 [01:57<55:36,  2.45it/s, v_num=5, loss=1.740]Epoch 0:   3%|         | 288/8462 [01:57<55:32,  2.45it/s, v_num=5, loss=1.740]Epoch 0:   3%|         | 288/8462 [01:57<55:35,  2.45it/s, v_num=5, loss=1.460]Epoch 0:   3%|         | 289/8462 [01:57<55:32,  2.45it/s, v_num=5, loss=1.460]Epoch 0:   3%|         | 289/8462 [01:57<55:35,  2.45it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 290/8462 [01:58<55:31,  2.45it/s, v_num=5, loss=1.580]Epoch 0:   3%|         | 290/8462 [01:58<55:34,  2.45it/s, v_num=5, loss=2.030]Epoch 0:   3%|         | 291/8462 [01:58<55:30,  2.45it/s, v_num=5, loss=2.030]Epoch 0:   3%|         | 291/8462 [01:58<55:33,  2.45it/s, v_num=5, loss=1.630]Epoch 0:   3%|         | 292/8462 [01:59<55:30,  2.45it/s, v_num=5, loss=1.630]Epoch 0:   3%|         | 292/8462 [01:59<55:33,  2.45it/s, v_num=5, loss=2.110]Epoch 0:   3%|         | 293/8462 [01:59<55:29,  2.45it/s, v_num=5, loss=2.110]Epoch 0:   3%|         | 293/8462 [01:59<55:32,  2.45it/s, v_num=5, loss=1.990]Epoch 0:   3%|         | 294/8462 [01:59<55:28,  2.45it/s, v_num=5, loss=1.990]Epoch 0:   3%|         | 294/8462 [01:59<55:32,  2.45it/s, v_num=5, loss=1.720]Epoch 0:   3%|         | 295/8462 [02:00<55:28,  2.45it/s, v_num=5, loss=1.720]Epoch 0:   3%|         | 295/8462 [02:00<55:31,  2.45it/s, v_num=5, loss=1.480]Epoch 0:   3%|         | 296/8462 [02:00<55:27,  2.45it/s, v_num=5, loss=1.480]Epoch 0:   3%|         | 296/8462 [02:00<55:30,  2.45it/s, v_num=5, loss=1.450]Epoch 0:   4%|         | 297/8462 [02:01<55:27,  2.45it/s, v_num=5, loss=1.450]Epoch 0:   4%|         | 297/8462 [02:01<55:30,  2.45it/s, v_num=5, loss=1.590]Epoch 0:   4%|         | 298/8462 [02:01<55:26,  2.45it/s, v_num=5, loss=1.590]Epoch 0:   4%|         | 298/8462 [02:01<55:29,  2.45it/s, v_num=5, loss=2.110]Epoch 0:   4%|         | 299/8462 [02:01<55:26,  2.45it/s, v_num=5, loss=2.110]Epoch 0:   4%|         | 299/8462 [02:01<55:29,  2.45it/s, v_num=5, loss=2.120]Epoch 0:   4%|         | 300/8462 [02:02<55:26,  2.45it/s, v_num=5, loss=2.120]Epoch 0:   4%|         | 300/8462 [02:02<55:28,  2.45it/s, v_num=5, loss=1.880]Epoch 0:   4%|         | 301/8462 [02:02<55:25,  2.45it/s, v_num=5, loss=1.880]Epoch 0:   4%|         | 301/8462 [02:02<55:28,  2.45it/s, v_num=5, loss=2.010]Epoch 0:   4%|         | 302/8462 [02:03<55:25,  2.45it/s, v_num=5, loss=2.010]Epoch 0:   4%|         | 302/8462 [02:03<55:28,  2.45it/s, v_num=5, loss=1.910]Epoch 0:   4%|         | 303/8462 [02:03<55:24,  2.45it/s, v_num=5, loss=1.910]Epoch 0:   4%|         | 303/8462 [02:03<55:27,  2.45it/s, v_num=5, loss=1.950]Epoch 0:   4%|         | 304/8462 [02:03<55:24,  2.45it/s, v_num=5, loss=1.950]Epoch 0:   4%|         | 304/8462 [02:03<55:27,  2.45it/s, v_num=5, loss=1.700]Epoch 0:   4%|         | 305/8462 [02:04<55:23,  2.45it/s, v_num=5, loss=1.700]Epoch 0:   4%|         | 305/8462 [02:04<55:26,  2.45it/s, v_num=5, loss=1.830]Epoch 0:   4%|         | 306/8462 [02:04<55:23,  2.45it/s, v_num=5, loss=1.830]Epoch 0:   4%|         | 306/8462 [02:04<55:26,  2.45it/s, v_num=5, loss=1.720]Epoch 0:   4%|         | 307/8462 [02:05<55:22,  2.45it/s, v_num=5, loss=1.720]Epoch 0:   4%|         | 307/8462 [02:05<55:25,  2.45it/s, v_num=5, loss=2.000]Epoch 0:   4%|         | 308/8462 [02:05<55:21,  2.45it/s, v_num=5, loss=2.000]Epoch 0:   4%|         | 308/8462 [02:05<55:24,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   4%|         | 309/8462 [02:05<55:21,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   4%|         | 309/8462 [02:06<55:25,  2.45it/s, v_num=5, loss=1.660]Epoch 0:   4%|         | 310/8462 [02:06<55:21,  2.45it/s, v_num=5, loss=1.660]Epoch 0:   4%|         | 310/8462 [02:06<55:24,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   4%|         | 311/8462 [02:06<55:21,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   4%|         | 311/8462 [02:06<55:24,  2.45it/s, v_num=5, loss=2.200]Epoch 0:   4%|         | 312/8462 [02:07<55:20,  2.45it/s, v_num=5, loss=2.200]Epoch 0:   4%|         | 312/8462 [02:07<55:23,  2.45it/s, v_num=5, loss=1.530]Epoch 0:   4%|         | 313/8462 [02:07<55:19,  2.45it/s, v_num=5, loss=1.530]Epoch 0:   4%|         | 313/8462 [02:07<55:22,  2.45it/s, v_num=5, loss=1.710]Epoch 0:   4%|         | 314/8462 [02:07<55:19,  2.45it/s, v_num=5, loss=1.710]Epoch 0:   4%|         | 314/8462 [02:08<55:22,  2.45it/s, v_num=5, loss=1.730]Epoch 0:   4%|         | 315/8462 [02:08<55:18,  2.45it/s, v_num=5, loss=1.730]Epoch 0:   4%|         | 315/8462 [02:08<55:21,  2.45it/s, v_num=5, loss=2.310]Epoch 0:   4%|         | 316/8462 [02:08<55:18,  2.45it/s, v_num=5, loss=2.310]Epoch 0:   4%|         | 316/8462 [02:08<55:21,  2.45it/s, v_num=5, loss=1.790]Epoch 0:   4%|         | 317/8462 [02:09<55:17,  2.45it/s, v_num=5, loss=1.790]Epoch 0:   4%|         | 317/8462 [02:09<55:20,  2.45it/s, v_num=5, loss=1.530]Epoch 0:   4%|         | 318/8462 [02:09<55:17,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   4%|         | 318/8462 [02:09<55:20,  2.45it/s, v_num=5, loss=1.490]Epoch 0:   4%|         | 319/8462 [02:09<55:17,  2.45it/s, v_num=5, loss=1.490]Epoch 0:   4%|         | 319/8462 [02:10<55:19,  2.45it/s, v_num=5, loss=1.710]Epoch 0:   4%|         | 320/8462 [02:10<55:16,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   4%|         | 320/8462 [02:10<55:19,  2.45it/s, v_num=5, loss=1.800]Epoch 0:   4%|         | 321/8462 [02:10<55:15,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   4%|         | 321/8462 [02:10<55:18,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   4%|         | 322/8462 [02:11<55:15,  2.46it/s, v_num=5, loss=1.930]Epoch 0:   4%|         | 322/8462 [02:11<55:18,  2.45it/s, v_num=5, loss=1.870]Epoch 0:   4%|         | 323/8462 [02:11<55:14,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   4%|         | 323/8462 [02:11<55:17,  2.45it/s, v_num=5, loss=1.740]Epoch 0:   4%|         | 324/8462 [02:11<55:14,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   4%|         | 324/8462 [02:12<55:17,  2.45it/s, v_num=5, loss=1.610]Epoch 0:   4%|         | 325/8462 [02:12<55:14,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   4%|         | 325/8462 [02:12<55:16,  2.45it/s, v_num=5, loss=2.050]Epoch 0:   4%|         | 326/8462 [02:12<55:13,  2.46it/s, v_num=5, loss=2.050]Epoch 0:   4%|         | 326/8462 [02:12<55:16,  2.45it/s, v_num=5, loss=1.820]Epoch 0:   4%|         | 327/8462 [02:13<55:12,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   4%|         | 327/8462 [02:13<55:15,  2.45it/s, v_num=5, loss=2.100]Epoch 0:   4%|         | 328/8462 [02:13<55:12,  2.46it/s, v_num=5, loss=2.100]Epoch 0:   4%|         | 328/8462 [02:13<55:15,  2.45it/s, v_num=5, loss=1.490]Epoch 0:   4%|         | 329/8462 [02:13<55:11,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   4%|         | 329/8462 [02:14<55:14,  2.45it/s, v_num=5, loss=1.450]Epoch 0:   4%|         | 330/8462 [02:14<55:11,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   4%|         | 330/8462 [02:14<55:13,  2.45it/s, v_num=5, loss=1.860]Epoch 0:   4%|         | 331/8462 [02:14<55:10,  2.46it/s, v_num=5, loss=1.860]Epoch 0:   4%|         | 331/8462 [02:14<55:13,  2.45it/s, v_num=5, loss=1.600]Epoch 0:   4%|         | 332/8462 [02:15<55:09,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   4%|         | 332/8462 [02:15<55:12,  2.45it/s, v_num=5, loss=1.660]Epoch 0:   4%|         | 333/8462 [02:15<55:09,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   4%|         | 333/8462 [02:15<55:12,  2.45it/s, v_num=5, loss=1.630]Epoch 0:   4%|         | 334/8462 [02:15<55:08,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   4%|         | 334/8462 [02:16<55:12,  2.45it/s, v_num=5, loss=1.560]Epoch 0:   4%|         | 335/8462 [02:16<55:08,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   4%|         | 335/8462 [02:16<55:11,  2.45it/s, v_num=5, loss=2.330]Epoch 0:   4%|         | 336/8462 [02:16<55:08,  2.46it/s, v_num=5, loss=2.330]Epoch 0:   4%|         | 336/8462 [02:16<55:11,  2.45it/s, v_num=5, loss=1.290]Epoch 0:   4%|         | 337/8462 [02:17<55:08,  2.46it/s, v_num=5, loss=1.290]Epoch 0:   4%|         | 337/8462 [02:17<55:10,  2.45it/s, v_num=5, loss=1.840]Epoch 0:   4%|         | 338/8462 [02:17<55:07,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   4%|         | 338/8462 [02:17<55:10,  2.45it/s, v_num=5, loss=1.830]Epoch 0:   4%|         | 339/8462 [02:18<55:07,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   4%|         | 339/8462 [02:18<55:09,  2.45it/s, v_num=5, loss=1.920]Epoch 0:   4%|         | 340/8462 [02:18<55:06,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   4%|         | 340/8462 [02:18<55:09,  2.45it/s, v_num=5, loss=1.440]Epoch 0:   4%|         | 341/8462 [02:18<55:06,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   4%|         | 341/8462 [02:18<55:08,  2.45it/s, v_num=5, loss=1.610]Epoch 0:   4%|         | 342/8462 [02:19<55:05,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   4%|         | 342/8462 [02:19<55:08,  2.45it/s, v_num=5, loss=1.660]Epoch 0:   4%|         | 343/8462 [02:19<55:05,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   4%|         | 343/8462 [02:19<55:07,  2.45it/s, v_num=5, loss=1.930]Epoch 0:   4%|         | 344/8462 [02:20<55:04,  2.46it/s, v_num=5, loss=1.930]Epoch 0:   4%|         | 344/8462 [02:20<55:07,  2.45it/s, v_num=5, loss=1.900]Epoch 0:   4%|         | 345/8462 [02:20<55:04,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   4%|         | 345/8462 [02:20<55:06,  2.45it/s, v_num=5, loss=2.150]Epoch 0:   4%|         | 346/8462 [02:20<55:03,  2.46it/s, v_num=5, loss=2.150]Epoch 0:   4%|         | 346/8462 [02:20<55:06,  2.45it/s, v_num=5, loss=1.880]Epoch 0:   4%|         | 347/8462 [02:21<55:03,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   4%|         | 347/8462 [02:21<55:05,  2.45it/s, v_num=5, loss=2.070]Epoch 0:   4%|         | 348/8462 [02:21<55:02,  2.46it/s, v_num=5, loss=2.070]Epoch 0:   4%|         | 348/8462 [02:21<55:05,  2.45it/s, v_num=5, loss=2.350]Epoch 0:   4%|         | 349/8462 [02:22<55:02,  2.46it/s, v_num=5, loss=2.350]Epoch 0:   4%|         | 349/8462 [02:22<55:04,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   4%|         | 350/8462 [02:22<55:01,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   4%|         | 350/8462 [02:22<55:04,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   4%|         | 351/8462 [02:22<55:01,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   4%|         | 351/8462 [02:22<55:03,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   4%|         | 352/8462 [02:23<55:00,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   4%|         | 352/8462 [02:23<55:03,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   4%|         | 353/8462 [02:23<55:00,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   4%|         | 353/8462 [02:23<55:03,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   4%|         | 354/8462 [02:24<54:59,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   4%|         | 354/8462 [02:24<55:02,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   4%|         | 355/8462 [02:24<54:59,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   4%|         | 355/8462 [02:24<55:02,  2.46it/s, v_num=5, loss=1.640]Epoch 0:   4%|         | 356/8462 [02:24<54:58,  2.46it/s, v_num=5, loss=1.640]Epoch 0:   4%|         | 356/8462 [02:24<55:01,  2.46it/s, v_num=5, loss=1.230]Epoch 0:   4%|         | 357/8462 [02:25<54:58,  2.46it/s, v_num=5, loss=1.230]Epoch 0:   4%|         | 357/8462 [02:25<55:01,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   4%|         | 358/8462 [02:25<54:57,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   4%|         | 358/8462 [02:25<55:00,  2.46it/s, v_num=5, loss=1.910]Epoch 0:   4%|         | 359/8462 [02:26<54:57,  2.46it/s, v_num=5, loss=1.910]Epoch 0:   4%|         | 359/8462 [02:26<55:00,  2.46it/s, v_num=5, loss=2.030]Epoch 0:   4%|         | 360/8462 [02:26<54:57,  2.46it/s, v_num=5, loss=2.030]Epoch 0:   4%|         | 360/8462 [02:26<54:59,  2.46it/s, v_num=5, loss=2.030]Epoch 0:   4%|         | 361/8462 [02:26<54:56,  2.46it/s, v_num=5, loss=2.030]Epoch 0:   4%|         | 361/8462 [02:27<54:59,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   4%|         | 362/8462 [02:27<54:56,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   4%|         | 362/8462 [02:27<54:58,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   4%|         | 363/8462 [02:27<54:55,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   4%|         | 363/8462 [02:27<54:57,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   4%|         | 364/8462 [02:28<54:54,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   4%|         | 364/8462 [02:28<54:57,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   4%|         | 365/8462 [02:28<54:54,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   4%|         | 365/8462 [02:28<54:56,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   4%|         | 366/8462 [02:28<54:53,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   4%|         | 366/8462 [02:29<54:56,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   4%|         | 367/8462 [02:29<54:53,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   4%|         | 367/8462 [02:29<54:56,  2.46it/s, v_num=5, loss=1.990]Epoch 0:   4%|         | 368/8462 [02:29<54:53,  2.46it/s, v_num=5, loss=1.990]Epoch 0:   4%|         | 368/8462 [02:29<54:55,  2.46it/s, v_num=5, loss=2.200]Epoch 0:   4%|         | 369/8462 [02:30<54:52,  2.46it/s, v_num=5, loss=2.200]Epoch 0:   4%|         | 369/8462 [02:30<54:55,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   4%|         | 370/8462 [02:30<54:52,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   4%|         | 370/8462 [02:30<54:54,  2.46it/s, v_num=5, loss=1.950]Epoch 0:   4%|         | 371/8462 [02:30<54:51,  2.46it/s, v_num=5, loss=1.950]Epoch 0:   4%|         | 371/8462 [02:31<54:54,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   4%|         | 372/8462 [02:31<54:51,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   4%|         | 372/8462 [02:31<54:53,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   4%|         | 373/8462 [02:31<54:50,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   4%|         | 373/8462 [02:31<54:53,  2.46it/s, v_num=5, loss=2.120]Epoch 0:   4%|         | 374/8462 [02:32<54:50,  2.46it/s, v_num=5, loss=2.120]Epoch 0:   4%|         | 374/8462 [02:32<54:52,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   4%|         | 375/8462 [02:32<54:49,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   4%|         | 375/8462 [02:32<54:52,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   4%|         | 376/8462 [02:32<54:49,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   4%|         | 376/8462 [02:33<54:51,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   4%|         | 377/8462 [02:33<54:49,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   4%|         | 377/8462 [02:33<54:51,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   4%|         | 378/8462 [02:33<54:48,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   4%|         | 378/8462 [02:33<54:51,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   4%|         | 379/8462 [02:34<54:48,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   4%|         | 379/8462 [02:34<54:50,  2.46it/s, v_num=5, loss=2.110]Epoch 0:   4%|         | 380/8462 [02:34<54:47,  2.46it/s, v_num=5, loss=2.110]Epoch 0:   4%|         | 380/8462 [02:34<54:50,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   5%|         | 381/8462 [02:34<54:47,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   5%|         | 381/8462 [02:35<54:49,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   5%|         | 382/8462 [02:35<54:46,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   5%|         | 382/8462 [02:35<54:49,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   5%|         | 383/8462 [02:35<54:46,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   5%|         | 383/8462 [02:35<54:48,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   5%|         | 384/8462 [02:36<54:45,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   5%|         | 384/8462 [02:36<54:48,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   5%|         | 385/8462 [02:36<54:45,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   5%|         | 385/8462 [02:36<54:48,  2.46it/s, v_num=5, loss=2.160]Epoch 0:   5%|         | 386/8462 [02:37<54:45,  2.46it/s, v_num=5, loss=2.160]Epoch 0:   5%|         | 386/8462 [02:37<54:47,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   5%|         | 387/8462 [02:37<54:44,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   5%|         | 387/8462 [02:37<54:47,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 388/8462 [02:37<54:44,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 388/8462 [02:37<54:46,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   5%|         | 389/8462 [02:38<54:43,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   5%|         | 389/8462 [02:38<54:46,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   5%|         | 390/8462 [02:38<54:43,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   5%|         | 390/8462 [02:38<54:45,  2.46it/s, v_num=5, loss=2.180]Epoch 0:   5%|         | 391/8462 [02:39<54:42,  2.46it/s, v_num=5, loss=2.180]Epoch 0:   5%|         | 391/8462 [02:39<54:45,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   5%|         | 392/8462 [02:39<54:42,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   5%|         | 392/8462 [02:39<54:44,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   5%|         | 393/8462 [02:39<54:42,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   5%|         | 393/8462 [02:39<54:44,  2.46it/s, v_num=5, loss=2.090]Epoch 0:   5%|         | 394/8462 [02:40<54:41,  2.46it/s, v_num=5, loss=2.090]Epoch 0:   5%|         | 394/8462 [02:40<54:43,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 395/8462 [02:40<54:41,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 395/8462 [02:40<54:43,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   5%|         | 396/8462 [02:41<54:40,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   5%|         | 396/8462 [02:41<54:42,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   5%|         | 397/8462 [02:41<54:40,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   5%|         | 397/8462 [02:41<54:42,  2.46it/s, v_num=5, loss=1.410]Epoch 0:   5%|         | 398/8462 [02:41<54:39,  2.46it/s, v_num=5, loss=1.410]Epoch 0:   5%|         | 398/8462 [02:41<54:41,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   5%|         | 399/8462 [02:42<54:39,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   5%|         | 399/8462 [02:42<54:41,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   5%|         | 400/8462 [02:42<54:38,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   5%|         | 400/8462 [02:42<54:40,  2.46it/s, v_num=5, loss=2.150]Epoch 0:   5%|         | 401/8462 [02:43<54:37,  2.46it/s, v_num=5, loss=2.150]Epoch 0:   5%|         | 401/8462 [02:43<54:40,  2.46it/s, v_num=5, loss=1.640]Epoch 0:   5%|         | 402/8462 [02:43<54:37,  2.46it/s, v_num=5, loss=1.640]Epoch 0:   5%|         | 402/8462 [02:43<54:39,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   5%|         | 403/8462 [02:43<54:37,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   5%|         | 403/8462 [02:43<54:39,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   5%|         | 404/8462 [02:44<54:36,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   5%|         | 404/8462 [02:44<54:38,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   5%|         | 405/8462 [02:44<54:36,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   5%|         | 405/8462 [02:44<54:38,  2.46it/s, v_num=5, loss=2.130]Epoch 0:   5%|         | 406/8462 [02:45<54:35,  2.46it/s, v_num=5, loss=2.130]Epoch 0:   5%|         | 406/8462 [02:45<54:38,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   5%|         | 407/8462 [02:45<54:35,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   5%|         | 407/8462 [02:45<54:37,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   5%|         | 408/8462 [02:45<54:35,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   5%|         | 408/8462 [02:46<54:37,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   5%|         | 409/8462 [02:46<54:34,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   5%|         | 409/8462 [02:46<54:36,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   5%|         | 410/8462 [02:46<54:33,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   5%|         | 410/8462 [02:46<54:35,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   5%|         | 411/8462 [02:47<54:33,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   5%|         | 411/8462 [02:47<54:35,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   5%|         | 412/8462 [02:47<54:32,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   5%|         | 412/8462 [02:47<54:35,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   5%|         | 413/8462 [02:47<54:32,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   5%|         | 413/8462 [02:48<54:34,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 414/8462 [02:48<54:32,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 414/8462 [02:48<54:34,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   5%|         | 415/8462 [02:48<54:31,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   5%|         | 415/8462 [02:48<54:33,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   5%|         | 416/8462 [02:49<54:30,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   5%|         | 416/8462 [02:49<54:33,  2.46it/s, v_num=5, loss=1.980]Epoch 0:   5%|         | 417/8462 [02:49<54:30,  2.46it/s, v_num=5, loss=1.980]Epoch 0:   5%|         | 417/8462 [02:49<54:33,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   5%|         | 418/8462 [02:49<54:30,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   5%|         | 418/8462 [02:50<54:32,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   5%|         | 419/8462 [02:50<54:30,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   5%|         | 419/8462 [02:50<54:32,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   5%|         | 420/8462 [02:50<54:29,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   5%|         | 420/8462 [02:50<54:31,  2.46it/s, v_num=5, loss=1.280]Epoch 0:   5%|         | 421/8462 [02:51<54:29,  2.46it/s, v_num=5, loss=1.280]Epoch 0:   5%|         | 421/8462 [02:51<54:31,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   5%|         | 422/8462 [02:51<54:28,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   5%|         | 422/8462 [02:51<54:30,  2.46it/s, v_num=5, loss=2.070]Epoch 0:   5%|         | 423/8462 [02:51<54:28,  2.46it/s, v_num=5, loss=2.070]Epoch 0:   5%|         | 423/8462 [02:52<54:30,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   5%|         | 424/8462 [02:52<54:27,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   5%|         | 424/8462 [02:52<54:29,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   5%|         | 425/8462 [02:52<54:27,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   5%|         | 425/8462 [02:52<54:29,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   5%|         | 426/8462 [02:53<54:26,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   5%|         | 426/8462 [02:53<54:28,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   5%|         | 427/8462 [02:53<54:26,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   5%|         | 427/8462 [02:53<54:28,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   5%|         | 428/8462 [02:53<54:25,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   5%|         | 428/8462 [02:54<54:28,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   5%|         | 429/8462 [02:54<54:25,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   5%|         | 429/8462 [02:54<54:27,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   5%|         | 430/8462 [02:54<54:25,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   5%|         | 430/8462 [02:54<54:27,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   5%|         | 431/8462 [02:55<54:24,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   5%|         | 431/8462 [02:55<54:26,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   5%|         | 432/8462 [02:55<54:24,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   5%|         | 432/8462 [02:55<54:26,  2.46it/s, v_num=5, loss=1.470]Epoch 0:   5%|         | 433/8462 [02:56<54:23,  2.46it/s, v_num=5, loss=1.470]Epoch 0:   5%|         | 433/8462 [02:56<54:26,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   5%|         | 434/8462 [02:56<54:23,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   5%|         | 434/8462 [02:56<54:25,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   5%|         | 435/8462 [02:56<54:22,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   5%|         | 435/8462 [02:56<54:25,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 436/8462 [02:57<54:22,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 436/8462 [02:57<54:24,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   5%|         | 437/8462 [02:57<54:22,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   5%|         | 437/8462 [02:57<54:24,  2.46it/s, v_num=5, loss=1.930]Epoch 0:   5%|         | 438/8462 [02:58<54:21,  2.46it/s, v_num=5, loss=1.930]Epoch 0:   5%|         | 438/8462 [02:58<54:23,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   5%|         | 439/8462 [02:58<54:20,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   5%|         | 439/8462 [02:58<54:23,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   5%|         | 440/8462 [02:58<54:20,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   5%|         | 440/8462 [02:58<54:22,  2.46it/s, v_num=5, loss=1.400]Epoch 0:   5%|         | 441/8462 [02:59<54:20,  2.46it/s, v_num=5, loss=1.400]Epoch 0:   5%|         | 441/8462 [02:59<54:22,  2.46it/s, v_num=5, loss=1.480]Epoch 0:   5%|         | 442/8462 [02:59<54:19,  2.46it/s, v_num=5, loss=1.480]Epoch 0:   5%|         | 442/8462 [02:59<54:21,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   5%|         | 443/8462 [03:00<54:19,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   5%|         | 443/8462 [03:00<54:21,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   5%|         | 444/8462 [03:00<54:18,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   5%|         | 444/8462 [03:00<54:20,  2.46it/s, v_num=5, loss=1.640]Epoch 0:   5%|         | 445/8462 [03:00<54:18,  2.46it/s, v_num=5, loss=1.640]Epoch 0:   5%|         | 445/8462 [03:00<54:20,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 446/8462 [03:01<54:17,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   5%|         | 446/8462 [03:01<54:19,  2.46it/s, v_num=5, loss=1.790]Epoch 0:   5%|         | 447/8462 [03:01<54:17,  2.46it/s, v_num=5, loss=1.790]Epoch 0:   5%|         | 447/8462 [03:01<54:19,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   5%|         | 448/8462 [03:02<54:16,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   5%|         | 448/8462 [03:02<54:18,  2.46it/s, v_num=5, loss=2.210]Epoch 0:   5%|         | 449/8462 [03:02<54:16,  2.46it/s, v_num=5, loss=2.210]Epoch 0:   5%|         | 449/8462 [03:02<54:18,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   5%|         | 450/8462 [03:02<54:15,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   5%|         | 450/8462 [03:02<54:17,  2.46it/s, v_num=5, loss=2.180]Epoch 0:   5%|         | 451/8462 [03:03<54:15,  2.46it/s, v_num=5, loss=2.180]Epoch 0:   5%|         | 451/8462 [03:03<54:17,  2.46it/s, v_num=5, loss=1.470]Epoch 0:   5%|         | 452/8462 [03:03<54:15,  2.46it/s, v_num=5, loss=1.470]Epoch 0:   5%|         | 452/8462 [03:03<54:17,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   5%|         | 453/8462 [03:04<54:14,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   5%|         | 453/8462 [03:04<54:16,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   5%|         | 454/8462 [03:04<54:14,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   5%|         | 454/8462 [03:04<54:16,  2.46it/s, v_num=5, loss=2.000]Epoch 0:   5%|         | 455/8462 [03:04<54:13,  2.46it/s, v_num=5, loss=2.000]Epoch 0:   5%|         | 455/8462 [03:05<54:15,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   5%|         | 456/8462 [03:05<54:13,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   5%|         | 456/8462 [03:05<54:15,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   5%|         | 457/8462 [03:05<54:13,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   5%|         | 457/8462 [03:05<54:14,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   5%|         | 458/8462 [03:06<54:12,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   5%|         | 458/8462 [03:06<54:14,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   5%|         | 459/8462 [03:06<54:12,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   5%|         | 459/8462 [03:06<54:14,  2.46it/s, v_num=5, loss=1.350]Epoch 0:   5%|         | 460/8462 [03:06<54:11,  2.46it/s, v_num=5, loss=1.350]Epoch 0:   5%|         | 460/8462 [03:07<54:13,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   5%|         | 461/8462 [03:07<54:11,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   5%|         | 461/8462 [03:07<54:13,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   5%|         | 462/8462 [03:07<54:10,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   5%|         | 462/8462 [03:07<54:12,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   5%|         | 463/8462 [03:08<54:10,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   5%|         | 463/8462 [03:08<54:12,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   5%|         | 464/8462 [03:08<54:09,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   5%|         | 464/8462 [03:08<54:11,  2.46it/s, v_num=5, loss=1.170]Epoch 0:   5%|         | 465/8462 [03:08<54:09,  2.46it/s, v_num=5, loss=1.170]Epoch 0:   5%|         | 465/8462 [03:09<54:11,  2.46it/s, v_num=5, loss=1.520]Epoch 0:   6%|         | 466/8462 [03:09<54:09,  2.46it/s, v_num=5, loss=1.520]Epoch 0:   6%|         | 466/8462 [03:09<54:11,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   6%|         | 467/8462 [03:09<54:08,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   6%|         | 467/8462 [03:09<54:10,  2.46it/s, v_num=5, loss=2.220]Epoch 0:   6%|         | 468/8462 [03:10<54:08,  2.46it/s, v_num=5, loss=2.220]Epoch 0:   6%|         | 468/8462 [03:10<54:10,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   6%|         | 469/8462 [03:10<54:07,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   6%|         | 469/8462 [03:10<54:09,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   6%|         | 470/8462 [03:10<54:07,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   6%|         | 470/8462 [03:11<54:09,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   6%|         | 471/8462 [03:11<54:07,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   6%|         | 471/8462 [03:11<54:09,  2.46it/s, v_num=5, loss=1.410]Epoch 0:   6%|         | 472/8462 [03:11<54:06,  2.46it/s, v_num=5, loss=1.410]Epoch 0:   6%|         | 472/8462 [03:11<54:08,  2.46it/s, v_num=5, loss=1.980]Epoch 0:   6%|         | 473/8462 [03:12<54:06,  2.46it/s, v_num=5, loss=1.980]Epoch 0:   6%|         | 473/8462 [03:12<54:08,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   6%|         | 474/8462 [03:12<54:06,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   6%|         | 474/8462 [03:12<54:08,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   6%|         | 475/8462 [03:13<54:05,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   6%|         | 475/8462 [03:13<54:07,  2.46it/s, v_num=5, loss=1.930]Epoch 0:   6%|         | 476/8462 [03:13<54:05,  2.46it/s, v_num=5, loss=1.930]Epoch 0:   6%|         | 476/8462 [03:13<54:07,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 477/8462 [03:13<54:05,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 477/8462 [03:13<54:06,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   6%|         | 478/8462 [03:14<54:04,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   6%|         | 478/8462 [03:14<54:06,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   6%|         | 479/8462 [03:14<54:04,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   6%|         | 479/8462 [03:14<54:05,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   6%|         | 480/8462 [03:15<54:03,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   6%|         | 480/8462 [03:15<54:05,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   6%|         | 481/8462 [03:15<54:03,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   6%|         | 481/8462 [03:15<54:04,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   6%|         | 482/8462 [03:15<54:02,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   6%|         | 482/8462 [03:15<54:04,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   6%|         | 483/8462 [03:16<54:02,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   6%|         | 483/8462 [03:16<54:03,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   6%|         | 484/8462 [03:16<54:01,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   6%|         | 484/8462 [03:16<54:03,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   6%|         | 485/8462 [03:17<54:01,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   6%|         | 485/8462 [03:17<54:03,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   6%|         | 486/8462 [03:17<54:01,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   6%|         | 486/8462 [03:17<54:03,  2.46it/s, v_num=5, loss=1.520]Epoch 0:   6%|         | 487/8462 [03:17<54:00,  2.46it/s, v_num=5, loss=1.520]Epoch 0:   6%|         | 487/8462 [03:18<54:02,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   6%|         | 488/8462 [03:18<54:00,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   6%|         | 488/8462 [03:18<54:02,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   6%|         | 489/8462 [03:18<53:59,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   6%|         | 489/8462 [03:18<54:01,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   6%|         | 490/8462 [03:19<53:59,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   6%|         | 490/8462 [03:19<54:01,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   6%|         | 491/8462 [03:19<53:59,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   6%|         | 491/8462 [03:19<54:00,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 492/8462 [03:19<53:58,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 492/8462 [03:20<54:00,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   6%|         | 493/8462 [03:20<53:57,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   6%|         | 493/8462 [03:20<53:59,  2.46it/s, v_num=5, loss=1.940]Epoch 0:   6%|         | 494/8462 [03:20<53:57,  2.46it/s, v_num=5, loss=1.940]Epoch 0:   6%|         | 494/8462 [03:20<53:59,  2.46it/s, v_num=5, loss=1.350]Epoch 0:   6%|         | 495/8462 [03:21<53:57,  2.46it/s, v_num=5, loss=1.350]Epoch 0:   6%|         | 495/8462 [03:21<53:58,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 496/8462 [03:21<53:56,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 496/8462 [03:21<53:58,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   6%|         | 497/8462 [03:21<53:56,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   6%|         | 497/8462 [03:22<53:58,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   6%|         | 498/8462 [03:22<53:56,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   6%|         | 498/8462 [03:22<53:58,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   6%|         | 499/8462 [03:22<53:55,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   6%|         | 499/8462 [03:22<53:57,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   6%|         | 500/8462 [03:23<53:55,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   6%|         | 500/8462 [03:23<53:56,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   6%|         | 501/8462 [03:23<53:54,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   6%|         | 501/8462 [03:23<53:56,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   6%|         | 502/8462 [03:23<53:54,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   6%|         | 502/8462 [03:24<53:56,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   6%|         | 503/8462 [03:24<53:53,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   6%|         | 503/8462 [03:24<53:55,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   6%|         | 504/8462 [03:24<53:53,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   6%|         | 504/8462 [03:24<53:55,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   6%|         | 505/8462 [03:25<53:53,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   6%|         | 505/8462 [03:25<53:54,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   6%|         | 506/8462 [03:25<53:52,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   6%|         | 506/8462 [03:25<53:54,  2.46it/s, v_num=5, loss=1.480]Epoch 0:   6%|         | 507/8462 [03:26<53:52,  2.46it/s, v_num=5, loss=1.480]Epoch 0:   6%|         | 507/8462 [03:26<53:54,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   6%|         | 508/8462 [03:26<53:51,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   6%|         | 508/8462 [03:26<53:53,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   6%|         | 509/8462 [03:26<53:51,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   6%|         | 509/8462 [03:26<53:53,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   6%|         | 510/8462 [03:27<53:51,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   6%|         | 510/8462 [03:27<53:52,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   6%|         | 511/8462 [03:27<53:50,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   6%|         | 511/8462 [03:27<53:52,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   6%|         | 512/8462 [03:28<53:50,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   6%|         | 512/8462 [03:28<53:51,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   6%|         | 513/8462 [03:28<53:49,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   6%|         | 513/8462 [03:28<53:51,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   6%|         | 514/8462 [03:28<53:49,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   6%|         | 514/8462 [03:28<53:50,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   6%|         | 515/8462 [03:29<53:48,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   6%|         | 515/8462 [03:29<53:50,  2.46it/s, v_num=5, loss=1.860]Epoch 0:   6%|         | 516/8462 [03:29<53:48,  2.46it/s, v_num=5, loss=1.860]Epoch 0:   6%|         | 516/8462 [03:29<53:50,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   6%|         | 517/8462 [03:30<53:47,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   6%|         | 517/8462 [03:30<53:49,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   6%|         | 518/8462 [03:30<53:47,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   6%|         | 518/8462 [03:30<53:49,  2.46it/s, v_num=5, loss=2.070]Epoch 0:   6%|         | 519/8462 [03:30<53:47,  2.46it/s, v_num=5, loss=2.070]Epoch 0:   6%|         | 519/8462 [03:30<53:49,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   6%|         | 520/8462 [03:31<53:46,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   6%|         | 520/8462 [03:31<53:48,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   6%|         | 521/8462 [03:31<53:46,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   6%|         | 521/8462 [03:31<53:48,  2.46it/s, v_num=5, loss=2.060]Epoch 0:   6%|         | 522/8462 [03:32<53:46,  2.46it/s, v_num=5, loss=2.060]Epoch 0:   6%|         | 522/8462 [03:32<53:47,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   6%|         | 523/8462 [03:32<53:45,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   6%|         | 523/8462 [03:32<53:47,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   6%|         | 524/8462 [03:32<53:45,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   6%|         | 524/8462 [03:33<53:46,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 525/8462 [03:33<53:44,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 525/8462 [03:33<53:46,  2.46it/s, v_num=5, loss=1.350]Epoch 0:   6%|         | 526/8462 [03:33<53:44,  2.46it/s, v_num=5, loss=1.350]Epoch 0:   6%|         | 526/8462 [03:33<53:45,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   6%|         | 527/8462 [03:34<53:43,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   6%|         | 527/8462 [03:34<53:45,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   6%|         | 528/8462 [03:34<53:43,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   6%|         | 528/8462 [03:34<53:44,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   6%|         | 529/8462 [03:34<53:42,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   6%|         | 529/8462 [03:35<53:44,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   6%|         | 530/8462 [03:35<53:42,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   6%|         | 530/8462 [03:35<53:44,  2.46it/s, v_num=5, loss=1.960]Epoch 0:   6%|         | 531/8462 [03:35<53:41,  2.46it/s, v_num=5, loss=1.960]Epoch 0:   6%|         | 531/8462 [03:35<53:43,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   6%|         | 532/8462 [03:36<53:41,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   6%|         | 532/8462 [03:36<53:43,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   6%|         | 533/8462 [03:36<53:41,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   6%|         | 533/8462 [03:36<53:43,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   6%|         | 534/8462 [03:36<53:41,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   6%|         | 534/8462 [03:37<53:42,  2.46it/s, v_num=5, loss=1.460]Epoch 0:   6%|         | 535/8462 [03:37<53:40,  2.46it/s, v_num=5, loss=1.460]Epoch 0:   6%|         | 535/8462 [03:37<53:42,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   6%|         | 536/8462 [03:37<53:40,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   6%|         | 536/8462 [03:37<53:41,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 537/8462 [03:38<53:39,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 537/8462 [03:38<53:41,  2.46it/s, v_num=5, loss=1.950]Epoch 0:   6%|         | 538/8462 [03:38<53:39,  2.46it/s, v_num=5, loss=1.950]Epoch 0:   6%|         | 538/8462 [03:38<53:41,  2.46it/s, v_num=5, loss=1.390]Epoch 0:   6%|         | 539/8462 [03:38<53:39,  2.46it/s, v_num=5, loss=1.390]Epoch 0:   6%|         | 539/8462 [03:39<53:40,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   6%|         | 540/8462 [03:39<53:38,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   6%|         | 540/8462 [03:39<53:40,  2.46it/s, v_num=5, loss=1.960]Epoch 0:   6%|         | 541/8462 [03:39<53:38,  2.46it/s, v_num=5, loss=1.960]Epoch 0:   6%|         | 541/8462 [03:39<53:39,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   6%|         | 542/8462 [03:40<53:37,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   6%|         | 542/8462 [03:40<53:39,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   6%|         | 543/8462 [03:40<53:37,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   6%|         | 543/8462 [03:40<53:38,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   6%|         | 544/8462 [03:41<53:36,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   6%|         | 544/8462 [03:41<53:38,  2.46it/s, v_num=5, loss=2.170]Epoch 0:   6%|         | 545/8462 [03:41<53:36,  2.46it/s, v_num=5, loss=2.170]Epoch 0:   6%|         | 545/8462 [03:41<53:38,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   6%|         | 546/8462 [03:41<53:36,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   6%|         | 546/8462 [03:41<53:37,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   6%|         | 547/8462 [03:42<53:35,  2.46it/s, v_num=5, loss=1.610]Epoch 0:   6%|         | 547/8462 [03:42<53:37,  2.46it/s, v_num=5, loss=2.190]Epoch 0:   6%|         | 548/8462 [03:42<53:35,  2.46it/s, v_num=5, loss=2.190]Epoch 0:   6%|         | 548/8462 [03:42<53:36,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 549/8462 [03:43<53:34,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   6%|         | 549/8462 [03:43<53:36,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   6%|         | 550/8462 [03:43<53:34,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   6%|         | 550/8462 [03:43<53:36,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 551/8462 [03:43<53:33,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 551/8462 [03:43<53:35,  2.46it/s, v_num=5, loss=1.860]Epoch 0:   7%|         | 552/8462 [03:44<53:33,  2.46it/s, v_num=5, loss=1.860]Epoch 0:   7%|         | 552/8462 [03:44<53:35,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   7%|         | 553/8462 [03:44<53:33,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   7%|         | 553/8462 [03:44<53:34,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   7%|         | 554/8462 [03:45<53:32,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   7%|         | 554/8462 [03:45<53:34,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   7%|         | 555/8462 [03:45<53:32,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   7%|         | 555/8462 [03:45<53:33,  2.46it/s, v_num=5, loss=1.640]Epoch 0:   7%|         | 556/8462 [03:45<53:31,  2.46it/s, v_num=5, loss=1.640]Epoch 0:   7%|         | 556/8462 [03:45<53:33,  2.46it/s, v_num=5, loss=1.910]Epoch 0:   7%|         | 557/8462 [03:46<53:31,  2.46it/s, v_num=5, loss=1.910]Epoch 0:   7%|         | 557/8462 [03:46<53:32,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   7%|         | 558/8462 [03:46<53:30,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   7%|         | 558/8462 [03:46<53:32,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 559/8462 [03:47<53:30,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 559/8462 [03:47<53:31,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   7%|         | 560/8462 [03:47<53:29,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   7%|         | 560/8462 [03:47<53:31,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   7%|         | 561/8462 [03:47<53:29,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   7%|         | 561/8462 [03:47<53:31,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   7%|         | 562/8462 [03:48<53:28,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   7%|         | 562/8462 [03:48<53:30,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   7%|         | 563/8462 [03:48<53:28,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   7%|         | 563/8462 [03:48<53:30,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   7%|         | 564/8462 [03:49<53:28,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   7%|         | 564/8462 [03:49<53:29,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   7%|         | 565/8462 [03:49<53:27,  2.46it/s, v_num=5, loss=1.830]Epoch 0:   7%|         | 565/8462 [03:49<53:29,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   7%|         | 566/8462 [03:49<53:27,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   7%|         | 566/8462 [03:50<53:28,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   7%|         | 567/8462 [03:50<53:26,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   7%|         | 567/8462 [03:50<53:28,  2.46it/s, v_num=5, loss=2.010]Epoch 0:   7%|         | 568/8462 [03:50<53:26,  2.46it/s, v_num=5, loss=2.010]Epoch 0:   7%|         | 568/8462 [03:50<53:28,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   7%|         | 569/8462 [03:51<53:26,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   7%|         | 569/8462 [03:51<53:27,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   7%|         | 570/8462 [03:51<53:25,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   7%|         | 570/8462 [03:51<53:27,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 571/8462 [03:51<53:25,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 571/8462 [03:52<53:26,  2.46it/s, v_num=5, loss=1.090]Epoch 0:   7%|         | 572/8462 [03:52<53:24,  2.46it/s, v_num=5, loss=1.090]Epoch 0:   7%|         | 572/8462 [03:52<53:26,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   7%|         | 573/8462 [03:52<53:24,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   7%|         | 573/8462 [03:52<53:25,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   7%|         | 574/8462 [03:53<53:23,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   7%|         | 574/8462 [03:53<53:25,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   7%|         | 575/8462 [03:53<53:23,  2.46it/s, v_num=5, loss=1.820]Epoch 0:   7%|         | 575/8462 [03:53<53:24,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   7%|         | 576/8462 [03:53<53:22,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   7%|         | 576/8462 [03:54<53:24,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   7%|         | 577/8462 [03:54<53:22,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   7%|         | 577/8462 [03:54<53:24,  2.46it/s, v_num=5, loss=1.060]Epoch 0:   7%|         | 578/8462 [03:54<53:22,  2.46it/s, v_num=5, loss=1.060]Epoch 0:   7%|         | 578/8462 [03:54<53:24,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   7%|         | 579/8462 [03:55<53:22,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   7%|         | 579/8462 [03:55<53:23,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   7%|         | 580/8462 [03:55<53:21,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   7%|         | 580/8462 [03:55<53:23,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   7%|         | 581/8462 [03:56<53:21,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   7%|         | 581/8462 [03:56<53:23,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   7%|         | 582/8462 [03:56<53:20,  2.46it/s, v_num=5, loss=1.800]Epoch 0:   7%|         | 582/8462 [03:56<53:22,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   7%|         | 583/8462 [03:56<53:20,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   7%|         | 583/8462 [03:56<53:21,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   7%|         | 584/8462 [03:57<53:19,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   7%|         | 584/8462 [03:57<53:21,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   7%|         | 585/8462 [03:57<53:19,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   7%|         | 585/8462 [03:57<53:21,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   7%|         | 586/8462 [03:58<53:19,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   7%|         | 586/8462 [03:58<53:21,  2.46it/s, v_num=5, loss=1.430]Epoch 0:   7%|         | 587/8462 [03:58<53:19,  2.46it/s, v_num=5, loss=1.430]Epoch 0:   7%|         | 587/8462 [03:58<53:20,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 588/8462 [03:58<53:18,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 588/8462 [03:58<53:20,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   7%|         | 589/8462 [03:59<53:18,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   7%|         | 589/8462 [03:59<53:19,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   7%|         | 590/8462 [03:59<53:17,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   7%|         | 590/8462 [03:59<53:19,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   7%|         | 591/8462 [04:00<53:17,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   7%|         | 591/8462 [04:00<53:18,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   7%|         | 592/8462 [04:00<53:17,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   7%|         | 592/8462 [04:00<53:18,  2.46it/s, v_num=5, loss=2.250]Epoch 0:   7%|         | 593/8462 [04:00<53:16,  2.46it/s, v_num=5, loss=2.250]Epoch 0:   7%|         | 593/8462 [04:01<53:18,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   7%|         | 594/8462 [04:01<53:16,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   7%|         | 594/8462 [04:01<53:17,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   7%|         | 595/8462 [04:01<53:15,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   7%|         | 595/8462 [04:01<53:17,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   7%|         | 596/8462 [04:02<53:15,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   7%|         | 596/8462 [04:02<53:17,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   7%|         | 597/8462 [04:02<53:15,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   7%|         | 597/8462 [04:02<53:16,  2.46it/s, v_num=5, loss=1.500]Epoch 0:   7%|         | 598/8462 [04:02<53:14,  2.46it/s, v_num=5, loss=1.500]Epoch 0:   7%|         | 598/8462 [04:03<53:16,  2.46it/s, v_num=5, loss=1.520]Epoch 0:   7%|         | 599/8462 [04:03<53:14,  2.46it/s, v_num=5, loss=1.520]Epoch 0:   7%|         | 599/8462 [04:03<53:15,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   7%|         | 600/8462 [04:03<53:13,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   7%|         | 600/8462 [04:03<53:15,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   7%|         | 601/8462 [04:04<53:13,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   7%|         | 601/8462 [04:04<53:14,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   7%|         | 602/8462 [04:04<53:12,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   7%|         | 602/8462 [04:04<53:14,  2.46it/s, v_num=5, loss=1.180]Epoch 0:   7%|         | 603/8462 [04:04<53:12,  2.46it/s, v_num=5, loss=1.180]Epoch 0:   7%|         | 603/8462 [04:05<53:13,  2.46it/s, v_num=5, loss=1.480]Epoch 0:   7%|         | 604/8462 [04:05<53:11,  2.46it/s, v_num=5, loss=1.480]Epoch 0:   7%|         | 604/8462 [04:05<53:13,  2.46it/s, v_num=5, loss=2.010]Epoch 0:   7%|         | 605/8462 [04:05<53:11,  2.46it/s, v_num=5, loss=2.010]Epoch 0:   7%|         | 605/8462 [04:05<53:13,  2.46it/s, v_num=5, loss=1.960]Epoch 0:   7%|         | 606/8462 [04:06<53:11,  2.46it/s, v_num=5, loss=1.960]Epoch 0:   7%|         | 606/8462 [04:06<53:12,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   7%|         | 607/8462 [04:06<53:10,  2.46it/s, v_num=5, loss=1.740]Epoch 0:   7%|         | 607/8462 [04:06<53:12,  2.46it/s, v_num=5, loss=1.460]Epoch 0:   7%|         | 608/8462 [04:06<53:10,  2.46it/s, v_num=5, loss=1.460]Epoch 0:   7%|         | 608/8462 [04:07<53:11,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   7%|         | 609/8462 [04:07<53:09,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   7%|         | 609/8462 [04:07<53:11,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   7%|         | 610/8462 [04:07<53:09,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   7%|         | 610/8462 [04:07<53:10,  2.46it/s, v_num=5, loss=1.390]Epoch 0:   7%|         | 611/8462 [04:08<53:09,  2.46it/s, v_num=5, loss=1.390]Epoch 0:   7%|         | 611/8462 [04:08<53:10,  2.46it/s, v_num=5, loss=1.520]Epoch 0:   7%|         | 612/8462 [04:08<53:08,  2.46it/s, v_num=5, loss=1.520]Epoch 0:   7%|         | 612/8462 [04:08<53:10,  2.46it/s, v_num=5, loss=1.860]Epoch 0:   7%|         | 613/8462 [04:09<53:08,  2.46it/s, v_num=5, loss=1.860]Epoch 0:   7%|         | 613/8462 [04:09<53:10,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   7%|         | 614/8462 [04:09<53:08,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   7%|         | 614/8462 [04:09<53:09,  2.46it/s, v_num=5, loss=2.140]Epoch 0:   7%|         | 615/8462 [04:09<53:07,  2.46it/s, v_num=5, loss=2.140]Epoch 0:   7%|         | 615/8462 [04:09<53:09,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   7%|         | 616/8462 [04:10<53:07,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   7%|         | 616/8462 [04:10<53:08,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   7%|         | 617/8462 [04:10<53:06,  2.46it/s, v_num=5, loss=1.890]Epoch 0:   7%|         | 617/8462 [04:10<53:08,  2.46it/s, v_num=5, loss=1.420]Epoch 0:   7%|         | 618/8462 [04:11<53:06,  2.46it/s, v_num=5, loss=1.420]Epoch 0:   7%|         | 618/8462 [04:11<53:07,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   7%|         | 619/8462 [04:11<53:05,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   7%|         | 619/8462 [04:11<53:07,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   7%|         | 620/8462 [04:11<53:05,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   7%|         | 620/8462 [04:11<53:06,  2.46it/s, v_num=5, loss=1.940]Epoch 0:   7%|         | 621/8462 [04:12<53:05,  2.46it/s, v_num=5, loss=1.940]Epoch 0:   7%|         | 621/8462 [04:12<53:06,  2.46it/s, v_num=5, loss=1.210]Epoch 0:   7%|         | 622/8462 [04:12<53:04,  2.46it/s, v_num=5, loss=1.210]Epoch 0:   7%|         | 622/8462 [04:12<53:06,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 623/8462 [04:13<53:04,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   7%|         | 623/8462 [04:13<53:05,  2.46it/s, v_num=5, loss=2.170]Epoch 0:   7%|         | 624/8462 [04:13<53:03,  2.46it/s, v_num=5, loss=2.170]Epoch 0:   7%|         | 624/8462 [04:13<53:05,  2.46it/s, v_num=5, loss=1.980]Epoch 0:   7%|         | 625/8462 [04:13<53:03,  2.46it/s, v_num=5, loss=1.980]Epoch 0:   7%|         | 625/8462 [04:13<53:04,  2.46it/s, v_num=5, loss=1.910]Epoch 0:   7%|         | 626/8462 [04:14<53:02,  2.46it/s, v_num=5, loss=1.910]Epoch 0:   7%|         | 626/8462 [04:14<53:04,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   7%|         | 627/8462 [04:14<53:02,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   7%|         | 627/8462 [04:14<53:03,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   7%|         | 628/8462 [04:15<53:01,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   7%|         | 628/8462 [04:15<53:03,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   7%|         | 629/8462 [04:15<53:01,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   7%|         | 629/8462 [04:15<53:02,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   7%|         | 630/8462 [04:15<53:00,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   7%|         | 630/8462 [04:15<53:02,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   7%|         | 631/8462 [04:16<53:00,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   7%|         | 631/8462 [04:16<53:01,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   7%|         | 632/8462 [04:16<52:59,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   7%|         | 632/8462 [04:16<53:01,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   7%|         | 633/8462 [04:17<52:59,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   7%|         | 633/8462 [04:17<53:00,  2.46it/s, v_num=5, loss=1.430]Epoch 0:   7%|         | 634/8462 [04:17<52:58,  2.46it/s, v_num=5, loss=1.430]Epoch 0:   7%|         | 634/8462 [04:17<53:00,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   8%|         | 635/8462 [04:17<52:58,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   8%|         | 635/8462 [04:17<52:59,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   8%|         | 636/8462 [04:18<52:57,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   8%|         | 636/8462 [04:18<52:59,  2.46it/s, v_num=5, loss=1.460]Epoch 0:   8%|         | 637/8462 [04:18<52:57,  2.46it/s, v_num=5, loss=1.460]Epoch 0:   8%|         | 637/8462 [04:18<52:58,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   8%|         | 638/8462 [04:19<52:57,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   8%|         | 638/8462 [04:19<52:58,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   8%|         | 639/8462 [04:19<52:56,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   8%|         | 639/8462 [04:19<52:58,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   8%|         | 640/8462 [04:19<52:56,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   8%|         | 640/8462 [04:20<52:57,  2.46it/s, v_num=5, loss=2.150]Epoch 0:   8%|         | 641/8462 [04:20<52:55,  2.46it/s, v_num=5, loss=2.150]Epoch 0:   8%|         | 641/8462 [04:20<52:57,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   8%|         | 642/8462 [04:20<52:55,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   8%|         | 642/8462 [04:20<52:56,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   8%|         | 643/8462 [04:21<52:55,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   8%|         | 643/8462 [04:21<52:56,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   8%|         | 644/8462 [04:21<52:54,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   8%|         | 644/8462 [04:21<52:55,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   8%|         | 645/8462 [04:21<52:54,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   8%|         | 645/8462 [04:22<52:55,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   8%|         | 646/8462 [04:22<52:53,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   8%|         | 646/8462 [04:22<52:55,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   8%|         | 647/8462 [04:22<52:53,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   8%|         | 647/8462 [04:22<52:54,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   8%|         | 648/8462 [04:23<52:52,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   8%|         | 648/8462 [04:23<52:54,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   8%|         | 649/8462 [04:23<52:52,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   8%|         | 649/8462 [04:23<52:53,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   8%|         | 650/8462 [04:23<52:52,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   8%|         | 650/8462 [04:24<52:53,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   8%|         | 651/8462 [04:24<52:51,  2.46it/s, v_num=5, loss=1.970]Epoch 0:   8%|         | 651/8462 [04:24<52:53,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 652/8462 [04:24<52:51,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 652/8462 [04:24<52:52,  2.46it/s, v_num=5, loss=2.030]Epoch 0:   8%|         | 653/8462 [04:25<52:50,  2.46it/s, v_num=5, loss=2.030]Epoch 0:   8%|         | 653/8462 [04:25<52:52,  2.46it/s, v_num=5, loss=1.990]Epoch 0:   8%|         | 654/8462 [04:25<52:50,  2.46it/s, v_num=5, loss=1.990]Epoch 0:   8%|         | 654/8462 [04:25<52:51,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   8%|         | 655/8462 [04:25<52:49,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   8%|         | 655/8462 [04:26<52:51,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   8%|         | 656/8462 [04:26<52:49,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   8%|         | 656/8462 [04:26<52:50,  2.46it/s, v_num=5, loss=1.340]Epoch 0:   8%|         | 657/8462 [04:26<52:48,  2.46it/s, v_num=5, loss=1.340]Epoch 0:   8%|         | 657/8462 [04:26<52:50,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   8%|         | 658/8462 [04:27<52:48,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   8%|         | 658/8462 [04:27<52:49,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   8%|         | 659/8462 [04:27<52:48,  2.46it/s, v_num=5, loss=1.880]Epoch 0:   8%|         | 659/8462 [04:27<52:49,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   8%|         | 660/8462 [04:27<52:47,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   8%|         | 660/8462 [04:28<52:49,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   8%|         | 661/8462 [04:28<52:47,  2.46it/s, v_num=5, loss=1.730]Epoch 0:   8%|         | 661/8462 [04:28<52:48,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   8%|         | 662/8462 [04:28<52:46,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   8%|         | 662/8462 [04:28<52:48,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   8%|         | 663/8462 [04:29<52:46,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   8%|         | 663/8462 [04:29<52:47,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   8%|         | 664/8462 [04:29<52:45,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   8%|         | 664/8462 [04:29<52:47,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   8%|         | 665/8462 [04:30<52:45,  2.46it/s, v_num=5, loss=1.690]Epoch 0:   8%|         | 665/8462 [04:30<52:46,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 666/8462 [04:30<52:44,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 666/8462 [04:30<52:46,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   8%|         | 667/8462 [04:30<52:44,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   8%|         | 667/8462 [04:30<52:45,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   8%|         | 668/8462 [04:31<52:44,  2.46it/s, v_num=5, loss=1.450]Epoch 0:   8%|         | 668/8462 [04:31<52:45,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   8%|         | 669/8462 [04:31<52:43,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   8%|         | 669/8462 [04:31<52:44,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 670/8462 [04:31<52:43,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 670/8462 [04:32<52:44,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 671/8462 [04:32<52:42,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 671/8462 [04:32<52:44,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   8%|         | 672/8462 [04:32<52:42,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   8%|         | 672/8462 [04:32<52:43,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   8%|         | 673/8462 [04:33<52:41,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   8%|         | 673/8462 [04:33<52:43,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   8%|         | 674/8462 [04:33<52:41,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   8%|         | 674/8462 [04:33<52:42,  2.46it/s, v_num=5, loss=2.000]Epoch 0:   8%|         | 675/8462 [04:34<52:40,  2.46it/s, v_num=5, loss=2.000]Epoch 0:   8%|         | 675/8462 [04:34<52:42,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   8%|         | 676/8462 [04:34<52:40,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   8%|         | 676/8462 [04:34<52:41,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   8%|         | 677/8462 [04:34<52:40,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   8%|         | 677/8462 [04:34<52:41,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   8%|         | 678/8462 [04:35<52:39,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   8%|         | 678/8462 [04:35<52:41,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 679/8462 [04:35<52:39,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 679/8462 [04:35<52:40,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   8%|         | 680/8462 [04:36<52:38,  2.46it/s, v_num=5, loss=1.570]Epoch 0:   8%|         | 680/8462 [04:36<52:40,  2.46it/s, v_num=5, loss=2.090]Epoch 0:   8%|         | 681/8462 [04:36<52:38,  2.46it/s, v_num=5, loss=2.090]Epoch 0:   8%|         | 681/8462 [04:36<52:39,  2.46it/s, v_num=5, loss=1.380]Epoch 0:   8%|         | 682/8462 [04:36<52:37,  2.46it/s, v_num=5, loss=1.380]Epoch 0:   8%|         | 682/8462 [04:36<52:39,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   8%|         | 683/8462 [04:37<52:37,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   8%|         | 683/8462 [04:37<52:38,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   8%|         | 684/8462 [04:37<52:37,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   8%|         | 684/8462 [04:37<52:38,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   8%|         | 685/8462 [04:38<52:36,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   8%|         | 685/8462 [04:38<52:37,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   8%|         | 686/8462 [04:38<52:36,  2.46it/s, v_num=5, loss=1.720]Epoch 0:   8%|         | 686/8462 [04:38<52:37,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   8%|         | 687/8462 [04:38<52:35,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   8%|         | 687/8462 [04:38<52:36,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 688/8462 [04:39<52:35,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 688/8462 [04:39<52:36,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   8%|         | 689/8462 [04:39<52:34,  2.46it/s, v_num=5, loss=1.590]Epoch 0:   8%|         | 689/8462 [04:39<52:36,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   8%|         | 690/8462 [04:40<52:34,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   8%|         | 690/8462 [04:40<52:35,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   8%|         | 691/8462 [04:40<52:33,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   8%|         | 691/8462 [04:40<52:35,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   8%|         | 692/8462 [04:40<52:33,  2.46it/s, v_num=5, loss=1.870]Epoch 0:   8%|         | 692/8462 [04:40<52:34,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   8%|         | 693/8462 [04:41<52:32,  2.46it/s, v_num=5, loss=1.540]Epoch 0:   8%|         | 693/8462 [04:41<52:34,  2.46it/s, v_num=5, loss=1.140]Epoch 0:   8%|         | 694/8462 [04:41<52:32,  2.46it/s, v_num=5, loss=1.140]Epoch 0:   8%|         | 694/8462 [04:41<52:33,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   8%|         | 695/8462 [04:42<52:31,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   8%|         | 695/8462 [04:42<52:33,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   8%|         | 696/8462 [04:42<52:31,  2.46it/s, v_num=5, loss=1.750]Epoch 0:   8%|         | 696/8462 [04:42<52:32,  2.46it/s, v_num=5, loss=2.120]Epoch 0:   8%|         | 697/8462 [04:42<52:30,  2.46it/s, v_num=5, loss=2.120]Epoch 0:   8%|         | 697/8462 [04:42<52:32,  2.46it/s, v_num=5, loss=2.370]Epoch 0:   8%|         | 698/8462 [04:43<52:30,  2.46it/s, v_num=5, loss=2.370]Epoch 0:   8%|         | 698/8462 [04:43<52:31,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   8%|         | 699/8462 [04:43<52:30,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   8%|         | 699/8462 [04:43<52:31,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   8%|         | 700/8462 [04:44<52:29,  2.46it/s, v_num=5, loss=1.510]Epoch 0:   8%|         | 700/8462 [04:44<52:30,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   8%|         | 701/8462 [04:44<52:29,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   8%|         | 701/8462 [04:44<52:30,  2.46it/s, v_num=5, loss=1.400]Epoch 0:   8%|         | 702/8462 [04:44<52:28,  2.46it/s, v_num=5, loss=1.400]Epoch 0:   8%|         | 702/8462 [04:44<52:29,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   8%|         | 703/8462 [04:45<52:28,  2.46it/s, v_num=5, loss=1.440]Epoch 0:   8%|         | 703/8462 [04:45<52:29,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   8%|         | 704/8462 [04:45<52:27,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   8%|         | 704/8462 [04:45<52:29,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 705/8462 [04:46<52:27,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 705/8462 [04:46<52:28,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   8%|         | 706/8462 [04:46<52:26,  2.46it/s, v_num=5, loss=1.770]Epoch 0:   8%|         | 706/8462 [04:46<52:28,  2.46it/s, v_num=5, loss=2.140]Epoch 0:   8%|         | 707/8462 [04:46<52:26,  2.46it/s, v_num=5, loss=2.140]Epoch 0:   8%|         | 707/8462 [04:46<52:27,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   8%|         | 708/8462 [04:47<52:25,  2.46it/s, v_num=5, loss=1.840]Epoch 0:   8%|         | 708/8462 [04:47<52:27,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   8%|         | 709/8462 [04:47<52:25,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   8%|         | 709/8462 [04:47<52:26,  2.46it/s, v_num=5, loss=2.150]Epoch 0:   8%|         | 710/8462 [04:48<52:25,  2.46it/s, v_num=5, loss=2.150]Epoch 0:   8%|         | 710/8462 [04:48<52:26,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 711/8462 [04:48<52:24,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 711/8462 [04:48<52:26,  2.46it/s, v_num=5, loss=1.500]Epoch 0:   8%|         | 712/8462 [04:48<52:24,  2.46it/s, v_num=5, loss=1.500]Epoch 0:   8%|         | 712/8462 [04:48<52:25,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   8%|         | 713/8462 [04:49<52:23,  2.46it/s, v_num=5, loss=1.850]Epoch 0:   8%|         | 713/8462 [04:49<52:25,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   8%|         | 714/8462 [04:49<52:23,  2.46it/s, v_num=5, loss=1.680]Epoch 0:   8%|         | 714/8462 [04:49<52:24,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 715/8462 [04:50<52:23,  2.46it/s, v_num=5, loss=1.620]Epoch 0:   8%|         | 715/8462 [04:50<52:24,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   8%|         | 716/8462 [04:50<52:22,  2.46it/s, v_num=5, loss=2.020]Epoch 0:   8%|         | 716/8462 [04:50<52:23,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   8%|         | 717/8462 [04:50<52:22,  2.46it/s, v_num=5, loss=1.490]Epoch 0:   8%|         | 717/8462 [04:51<52:23,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   8%|         | 718/8462 [04:51<52:21,  2.46it/s, v_num=5, loss=1.780]Epoch 0:   8%|         | 718/8462 [04:51<52:23,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 719/8462 [04:51<52:21,  2.46it/s, v_num=5, loss=1.580]Epoch 0:   8%|         | 719/8462 [04:51<52:22,  2.46it/s, v_num=5, loss=1.380]Epoch 0:   9%|         | 720/8462 [04:52<52:21,  2.46it/s, v_num=5, loss=1.380]Epoch 0:   9%|         | 720/8462 [04:52<52:22,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   9%|         | 721/8462 [04:52<52:20,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   9%|         | 721/8462 [04:52<52:21,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   9%|         | 722/8462 [04:52<52:20,  2.46it/s, v_num=5, loss=1.550]Epoch 0:   9%|         | 722/8462 [04:53<52:21,  2.46it/s, v_num=5, loss=1.810]Epoch 0:   9%|         | 723/8462 [04:53<52:19,  2.47it/s, v_num=5, loss=1.810]Epoch 0:   9%|         | 723/8462 [04:53<52:20,  2.46it/s, v_num=5, loss=1.700]Epoch 0:   9%|         | 724/8462 [04:53<52:18,  2.47it/s, v_num=5, loss=1.700]Epoch 0:   9%|         | 724/8462 [04:53<52:20,  2.46it/s, v_num=5, loss=2.210]Epoch 0:   9%|         | 725/8462 [04:54<52:18,  2.47it/s, v_num=5, loss=2.210]Epoch 0:   9%|         | 725/8462 [04:54<52:19,  2.46it/s, v_num=5, loss=1.250]Epoch 0:   9%|         | 726/8462 [04:54<52:18,  2.47it/s, v_num=5, loss=1.250]Epoch 0:   9%|         | 726/8462 [04:54<52:19,  2.46it/s, v_num=5, loss=1.670]Epoch 0:   9%|         | 727/8462 [04:54<52:17,  2.47it/s, v_num=5, loss=1.670]Epoch 0:   9%|         | 727/8462 [04:55<52:18,  2.46it/s, v_num=5, loss=1.630]Epoch 0:   9%|         | 728/8462 [04:55<52:17,  2.47it/s, v_num=5, loss=1.630]Epoch 0:   9%|         | 728/8462 [04:55<52:18,  2.46it/s, v_num=5, loss=1.370]Epoch 0:   9%|         | 729/8462 [04:55<52:16,  2.47it/s, v_num=5, loss=1.370]Epoch 0:   9%|         | 729/8462 [04:55<52:17,  2.46it/s, v_num=5, loss=1.560]Epoch 0:   9%|         | 730/8462 [04:56<52:16,  2.47it/s, v_num=5, loss=1.560]Epoch 0:   9%|         | 730/8462 [04:56<52:17,  2.46it/s, v_num=5, loss=1.500]Epoch 0:   9%|         | 731/8462 [04:56<52:15,  2.47it/s, v_num=5, loss=1.500]Epoch 0:   9%|         | 731/8462 [04:56<52:16,  2.46it/s, v_num=5, loss=1.650]Epoch 0:   9%|         | 732/8462 [04:56<52:15,  2.47it/s, v_num=5, loss=1.650]Epoch 0:   9%|         | 732/8462 [04:57<52:16,  2.46it/s, v_num=5, loss=1.920]Epoch 0:   9%|         | 733/8462 [04:57<52:14,  2.47it/s, v_num=5, loss=1.920]Epoch 0:   9%|         | 733/8462 [04:57<52:15,  2.46it/s, v_num=5, loss=1.460]Epoch 0:   9%|         | 734/8462 [04:57<52:14,  2.47it/s, v_num=5, loss=1.460]Epoch 0:   9%|         | 734/8462 [04:57<52:15,  2.46it/s, v_num=5, loss=2.080]Epoch 0:   9%|         | 735/8462 [04:58<52:13,  2.47it/s, v_num=5, loss=2.080]Epoch 0:   9%|         | 735/8462 [04:58<52:15,  2.46it/s, v_num=5, loss=1.500]Epoch 0:   9%|         | 736/8462 [04:58<52:13,  2.47it/s, v_num=5, loss=1.500]Epoch 0:   9%|         | 736/8462 [04:58<52:14,  2.46it/s, v_num=5, loss=1.530]Epoch 0:   9%|         | 737/8462 [04:58<52:13,  2.47it/s, v_num=5, loss=1.530]Epoch 0:   9%|         | 737/8462 [04:59<52:14,  2.46it/s, v_num=5, loss=1.710]Epoch 0:   9%|         | 738/8462 [04:59<52:12,  2.47it/s, v_num=5, loss=1.710]Epoch 0:   9%|         | 738/8462 [04:59<52:13,  2.46it/s, v_num=5, loss=1.470]Epoch 0:   9%|         | 739/8462 [04:59<52:12,  2.47it/s, v_num=5, loss=1.470]Epoch 0:   9%|         | 739/8462 [04:59<52:13,  2.46it/s, v_num=5, loss=1.600]Epoch 0:   9%|         | 740/8462 [05:00<52:11,  2.47it/s, v_num=5, loss=1.600]Epoch 0:   9%|         | 740/8462 [05:00<52:12,  2.46it/s, v_num=5, loss=1.470]Epoch 0:   9%|         | 741/8462 [05:00<52:11,  2.47it/s, v_num=5, loss=1.470]Epoch 0:   9%|         | 741/8462 [05:00<52:12,  2.46it/s, v_num=5, loss=1.500]Epoch 0:   9%|         | 742/8462 [05:00<52:10,  2.47it/s, v_num=5, loss=1.500]Epoch 0:   9%|         | 742/8462 [05:01<52:12,  2.46it/s, v_num=5, loss=1.660]Epoch 0:   9%|         | 743/8462 [05:01<52:10,  2.47it/s, v_num=5, loss=1.660]Epoch 0:   9%|         | 743/8462 [05:01<52:11,  2.46it/s, v_num=5, loss=1.220]Epoch 0:   9%|         | 744/8462 [05:01<52:10,  2.47it/s, v_num=5, loss=1.220]Epoch 0:   9%|         | 744/8462 [05:01<52:11,  2.46it/s, v_num=5, loss=1.900]Epoch 0:   9%|         | 745/8462 [05:02<52:09,  2.47it/s, v_num=5, loss=1.900]Epoch 0:   9%|         | 745/8462 [05:02<52:10,  2.46it/s, v_num=5, loss=1.760]Epoch 0:   9%|         | 746/8462 [05:02<52:09,  2.47it/s, v_num=5, loss=1.760]Epoch 0:   9%|         | 746/8462 [05:02<52:10,  2.47it/s, v_num=5, loss=1.740]Epoch 0:   9%|         | 747/8462 [05:02<52:08,  2.47it/s, v_num=5, loss=1.740]Epoch 0:   9%|         | 747/8462 [05:03<52:09,  2.47it/s, v_num=5, loss=1.130]Epoch 0:   9%|         | 748/8462 [05:03<52:08,  2.47it/s, v_num=5, loss=1.130]Epoch 0:   9%|         | 748/8462 [05:03<52:09,  2.47it/s, v_num=5, loss=1.720]Epoch 0:   9%|         | 749/8462 [05:03<52:07,  2.47it/s, v_num=5, loss=1.720]Epoch 0:   9%|         | 749/8462 [05:03<52:08,  2.47it/s, v_num=5, loss=1.670]Epoch 0:   9%|         | 750/8462 [05:04<52:07,  2.47it/s, v_num=5, loss=1.670]Epoch 0:   9%|         | 750/8462 [05:04<52:08,  2.47it/s, v_num=5, loss=1.600]Epoch 0:   9%|         | 751/8462 [05:04<52:06,  2.47it/s, v_num=5, loss=1.600]Epoch 0:   9%|         | 751/8462 [05:04<52:07,  2.47it/s, v_num=5, loss=1.750]Epoch 0:   9%|         | 752/8462 [05:04<52:06,  2.47it/s, v_num=5, loss=1.750]Epoch 0:   9%|         | 752/8462 [05:05<52:07,  2.47it/s, v_num=5, loss=1.230]Epoch 0:   9%|         | 753/8462 [05:05<52:05,  2.47it/s, v_num=5, loss=1.230]Epoch 0:   9%|         | 753/8462 [05:05<52:07,  2.47it/s, v_num=5, loss=2.050]Epoch 0:   9%|         | 754/8462 [05:05<52:05,  2.47it/s, v_num=5, loss=2.050]Epoch 0:   9%|         | 754/8462 [05:05<52:06,  2.47it/s, v_num=5, loss=1.320]Epoch 0:   9%|         | 755/8462 [05:06<52:04,  2.47it/s, v_num=5, loss=1.320]Epoch 0:   9%|         | 755/8462 [05:06<52:06,  2.47it/s, v_num=5, loss=1.650]Epoch 0:   9%|         | 756/8462 [05:06<52:04,  2.47it/s, v_num=5, loss=1.650]Epoch 0:   9%|         | 756/8462 [05:06<52:05,  2.47it/s, v_num=5, loss=1.800]Epoch 0:   9%|         | 757/8462 [05:06<52:04,  2.47it/s, v_num=5, loss=1.800]Epoch 0:   9%|         | 757/8462 [05:07<52:05,  2.47it/s, v_num=5, loss=1.620]Epoch 0:   9%|         | 758/8462 [05:07<52:03,  2.47it/s, v_num=5, loss=1.620]Epoch 0:   9%|         | 758/8462 [05:07<52:04,  2.47it/s, v_num=5, loss=1.720]Epoch 0:   9%|         | 759/8462 [05:07<52:03,  2.47it/s, v_num=5, loss=1.720]Epoch 0:   9%|         | 759/8462 [05:07<52:04,  2.47it/s, v_num=5, loss=1.750]Epoch 0:   9%|         | 760/8462 [05:08<52:02,  2.47it/s, v_num=5, loss=1.750]Epoch 0:   9%|         | 760/8462 [05:08<52:03,  2.47it/s, v_num=5, loss=1.650]Epoch 0:   9%|         | 761/8462 [05:08<52:02,  2.47it/s, v_num=5, loss=1.650]Epoch 0:   9%|         | 761/8462 [05:08<52:03,  2.47it/s, v_num=5, loss=1.610]Epoch 0:   9%|         | 762/8462 [05:08<52:01,  2.47it/s, v_num=5, loss=1.610]Epoch 0:   9%|         | 762/8462 [05:09<52:02,  2.47it/s, v_num=5, loss=1.690]Epoch 0:   9%|         | 763/8462 [05:09<52:01,  2.47it/s, v_num=5, loss=1.690]Epoch 0:   9%|         | 763/8462 [05:09<52:02,  2.47it/s, v_num=5, loss=2.070]Epoch 0:   9%|         | 764/8462 [05:09<52:00,  2.47it/s, v_num=5, loss=2.070]Epoch 0:   9%|         | 764/8462 [05:09<52:02,  2.47it/s, v_num=5, loss=1.810]Epoch 0:   9%|         | 765/8462 [05:10<52:00,  2.47it/s, v_num=5, loss=1.810]Epoch 0:   9%|         | 765/8462 [05:10<52:01,  2.47it/s, v_num=5, loss=1.430]Epoch 0:   9%|         | 766/8462 [05:10<52:00,  2.47it/s, v_num=5, loss=1.430]Epoch 0:   9%|         | 766/8462 [05:10<52:01,  2.47it/s, v_num=5, loss=1.690]Epoch 0:   9%|         | 767/8462 [05:10<51:59,  2.47it/s, v_num=5, loss=1.690]Epoch 0:   9%|         | 767/8462 [05:11<52:00,  2.47it/s, v_num=5, loss=1.750]Epoch 0:   9%|         | 768/8462 [05:11<51:59,  2.47it/s, v_num=5, loss=1.750]Epoch 0:   9%|         | 768/8462 [05:11<52:00,  2.47it/s, v_num=5, loss=1.590]Epoch 0:   9%|         | 769/8462 [05:11<51:58,  2.47it/s, v_num=5, loss=1.590]Epoch 0:   9%|         | 769/8462 [05:11<51:59,  2.47it/s, v_num=5, loss=2.210]Epoch 0:   9%|         | 770/8462 [05:12<51:58,  2.47it/s, v_num=5, loss=2.210]Epoch 0:   9%|         | 770/8462 [05:12<51:59,  2.47it/s, v_num=5, loss=1.830]Epoch 0:   9%|         | 771/8462 [05:12<51:57,  2.47it/s, v_num=5, loss=1.830]Epoch 0:   9%|         | 771/8462 [05:12<51:58,  2.47it/s, v_num=5, loss=1.930]Epoch 0:   9%|         | 772/8462 [05:12<51:57,  2.47it/s, v_num=5, loss=1.930]Epoch 0:   9%|         | 772/8462 [05:13<51:58,  2.47it/s, v_num=5, loss=1.550]Epoch 0:   9%|         | 773/8462 [05:13<51:56,  2.47it/s, v_num=5, loss=1.550]Epoch 0:   9%|         | 773/8462 [05:13<51:58,  2.47it/s, v_num=5, loss=1.750]Epoch 0:   9%|         | 774/8462 [05:13<51:56,  2.47it/s, v_num=5, loss=1.750]Epoch 0:   9%|         | 774/8462 [05:13<51:57,  2.47it/s, v_num=5, loss=1.660]{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 8,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Global seed set to 42
/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train.py --dataset mimic_cxr --annotation /nfs/scrat ...
  rank_zero_warn(
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[W623 12:05:28.637262653 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:43863 (errno: 97 - Address family not supported by protocol).
[rank: 2] Global seed set to 42
[rank: 1] Global seed set to 42
[rank: 3] Global seed set to 42
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
[W623 12:05:34.410567412 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:43863 (errno: 97 - Address family not supported by protocol).
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[W623 12:05:34.458448764 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:43863 (errno: 97 - Address family not supported by protocol).
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[W623 12:05:34.492119568 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:43863 (errno: 97 - Address family not supported by protocol).
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory ./save/mimic_cxr/v1_shallow/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name           | Type                | Params
-------------------------------------------------------
0 | visual_encoder | SwinModel           | 86.7 M
1 | lm_model       | StableLmForCausalLM | 2.8 B 
2 | embed_tokens   | Embedding           | 128 M 
3 | lm_proj        | Linear              | 2.6 M 
4 | layer_norm     | LayerNorm           | 5.1 K 
-------------------------------------------------------
2.6 M     Trainable params
2.9 B     Non-trainable params
2.9 B     Total params
11,539.262Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 8,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 8,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/nfs/scratch/eechengyang/Data/mimic-cxr/mimic_annotation_all.json',
 'base_dir': '/nfs/scratch/eechengyang/Data/mimic-cxr/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'mimic_cxr',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': 1,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 0.5,
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lm_model': 'stabilityai/stablelm-3b-4e1t',
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 5,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'repetition_penalty': 2.0,
 'savedmodel_path': './save/mimic_cxr/v1_shallow',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp',
 'temperature': 0,
 'test': False,
 'test_batch_size': 4,
 'val_batch_size': 8,
 'val_check_interval': 0.5,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
Loading vision encoder:microsoft/swin-base-patch4-window7-224
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading StableLM-3B-4E1T
stabilityai/stablelm-3b-4e1t
Loading StableLM Done
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
/home/eechengyang/anaconda3/envs/mrg/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Sanity Checking DataLoader 0:  50%|     | 1/2 [00:05<00:05,  5.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:10<00:00,  5.01s/it]                                                                           {'Bleu_1': 0.05624999999994978, 'Bleu_2': 0.02141400902698677, 'Bleu_3': 0.009446385997478157, 'Bleu_4': 9.416739781096768e-07, 'ROUGE_L': 0.04822482292257275, 'CIDEr': 0.0030150759551051012}
Sanity Checking DataLoader 0: 100%|| 2/2 [00:12<00:00,  6.41s/it]                                                                           Saving checkpoint at step 0 to ./save/mimic_cxr/v1_shallow/checkpoints/checkpoint_epoch0_step0_bleu0.000001_cider0.003015.pth.
Sanity Checking DataLoader 0: 100%|| 2/2 [00:12<00:00,  6.41s/it]                                                                           huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/8462 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/8462 [00:00<?, ?it/s] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Epoch 0:   0%|          | 1/8462 [00:04<10:21:35,  4.41s/it]Epoch 0:   0%|          | 1/8462 [00:04<10:21:43,  4.41s/it, v_num=6, loss=4.010]Epoch 0:   0%|          | 2/8462 [00:04<5:32:47,  2.36s/it, v_num=6, loss=4.010] Epoch 0:   0%|          | 2/8462 [00:04<5:40:12,  2.41s/it, v_num=6, loss=3.350]Epoch 0:   0%|          | 3/8462 [00:05<4:01:10,  1.71s/it, v_num=6, loss=3.350]Epoch 0:   0%|          | 3/8462 [00:05<4:05:28,  1.74s/it, v_num=6, loss=2.870]Epoch 0:   0%|          | 4/8462 [00:05<3:14:42,  1.38s/it, v_num=6, loss=2.870]Epoch 0:   0%|          | 4/8462 [00:05<3:17:56,  1.40s/it, v_num=6, loss=2.790]Epoch 0:   0%|          | 5/8462 [00:05<2:47:15,  1.19s/it, v_num=6, loss=2.790]Epoch 0:   0%|          | 5/8462 [00:06<2:49:43,  1.20s/it, v_num=6, loss=2.720]Epoch 0:   0%|          | 6/8462 [00:06<2:28:30,  1.05s/it, v_num=6, loss=2.720]Epoch 0:   0%|          | 6/8462 [00:06<2:30:41,  1.07s/it, v_num=6, loss=2.840]Epoch 0:   0%|          | 7/8462 [00:06<2:15:16,  1.04it/s, v_num=6, loss=2.840]Epoch 0:   0%|          | 7/8462 [00:06<2:17:10,  1.03it/s, v_num=6, loss=2.780]Epoch 0:   0%|          | 8/8462 [00:07<2:05:18,  1.12it/s, v_num=6, loss=2.780]Epoch 0:   0%|          | 8/8462 [00:07<2:07:03,  1.11it/s, v_num=6, loss=3.010]Epoch 0:   0%|          | 9/8462 [00:07<2:04:38,  1.13it/s, v_num=6, loss=3.010]Epoch 0:   0%|          | 9/8462 [00:08<2:06:02,  1.12it/s, v_num=6, loss=2.590]Epoch 0:   0%|          | 10/8462 [00:08<1:57:45,  1.20it/s, v_num=6, loss=2.590]Epoch 0:   0%|          | 10/8462 [00:08<1:59:05,  1.18it/s, v_num=6, loss=2.690]Epoch 0:   0%|          | 11/8462 [00:08<1:54:02,  1.24it/s, v_num=6, loss=2.690]Epoch 0:   0%|          | 11/8462 [00:08<1:55:10,  1.22it/s, v_num=6, loss=2.610]Epoch 0:   0%|          | 12/8462 [00:09<1:49:07,  1.29it/s, v_num=6, loss=2.610]Epoch 0:   0%|          | 12/8462 [00:09<1:50:19,  1.28it/s, v_num=6, loss=2.750]Epoch 0:   0%|          | 13/8462 [00:09<1:45:12,  1.34it/s, v_num=6, loss=2.750]Epoch 0:   0%|          | 13/8462 [00:09<1:46:08,  1.33it/s, v_num=6, loss=2.600]Epoch 0:   0%|          | 14/8462 [00:10<1:41:37,  1.39it/s, v_num=6, loss=2.600]Epoch 0:   0%|          | 14/8462 [00:10<1:42:34,  1.37it/s, v_num=6, loss=2.540]Epoch 0:   0%|          | 15/8462 [00:10<1:38:36,  1.43it/s, v_num=6, loss=2.540]Epoch 0:   0%|          | 15/8462 [00:10<1:39:26,  1.42it/s, v_num=6, loss=2.520]Epoch 0:   0%|          | 16/8462 [00:10<1:35:54,  1.47it/s, v_num=6, loss=2.520]Epoch 0:   0%|          | 16/8462 [00:10<1:36:42,  1.46it/s, v_num=6, loss=2.480]Epoch 0:   0%|          | 17/8462 [00:11<1:35:43,  1.47it/s, v_num=6, loss=2.480]Epoch 0:   0%|          | 17/8462 [00:11<1:37:24,  1.45it/s, v_num=6, loss=2.230]Epoch 0:   0%|          | 18/8462 [00:12<1:34:24,  1.49it/s, v_num=6, loss=2.230]Epoch 0:   0%|          | 18/8462 [00:12<1:35:05,  1.48it/s, v_num=6, loss=2.080]Epoch 0:   0%|          | 19/8462 [00:12<1:33:32,  1.50it/s, v_num=6, loss=2.080]Epoch 0:   0%|          | 19/8462 [00:12<1:34:11,  1.49it/s, v_num=6, loss=2.430]Epoch 0:   0%|          | 20/8462 [00:13<1:31:38,  1.54it/s, v_num=6, loss=2.430]Epoch 0:   0%|          | 20/8462 [00:13<1:32:17,  1.52it/s, v_num=6, loss=2.230]Epoch 0:   0%|          | 21/8462 [00:13<1:29:56,  1.56it/s, v_num=6, loss=2.230]Epoch 0:   0%|          | 21/8462 [00:13<1:30:32,  1.55it/s, v_num=6, loss=2.170]Epoch 0:   0%|          | 22/8462 [00:13<1:28:21,  1.59it/s, v_num=6, loss=2.170]Epoch 0:   0%|          | 22/8462 [00:13<1:29:02,  1.58it/s, v_num=6, loss=2.630]Epoch 0:   0%|          | 23/8462 [00:14<1:27:02,  1.62it/s, v_num=6, loss=2.630]Epoch 0:   0%|          | 23/8462 [00:14<1:27:35,  1.61it/s, v_num=6, loss=2.520]Epoch 0:   0%|          | 24/8462 [00:14<1:25:42,  1.64it/s, v_num=6, loss=2.520]Epoch 0:   0%|          | 24/8462 [00:14<1:26:17,  1.63it/s, v_num=6, loss=2.180]Epoch 0:   0%|          | 25/8462 [00:15<1:28:58,  1.58it/s, v_num=6, loss=2.180]Epoch 0:   0%|          | 25/8462 [00:15<1:29:28,  1.57it/s, v_num=6, loss=2.060]Epoch 0:   0%|          | 26/8462 [00:16<1:27:43,  1.60it/s, v_num=6, loss=2.060]Epoch 0:   0%|          | 26/8462 [00:16<1:28:12,  1.59it/s, v_num=6, loss=2.110]Epoch 0:   0%|          | 27/8462 [00:16<1:26:30,  1.63it/s, v_num=6, loss=2.110]Epoch 0:   0%|          | 27/8462 [00:16<1:27:01,  1.62it/s, v_num=6, loss=2.370]Epoch 0:   0%|          | 28/8462 [00:17<1:25:25,  1.65it/s, v_num=6, loss=2.370]Epoch 0:   0%|          | 28/8462 [00:17<1:25:57,  1.64it/s, v_num=6, loss=1.940]Epoch 0:   0%|          | 29/8462 [00:17<1:24:27,  1.66it/s, v_num=6, loss=1.940]Epoch 0:   0%|          | 29/8462 [00:17<1:24:54,  1.66it/s, v_num=6, loss=1.640]Epoch 0:   0%|          | 30/8462 [00:17<1:23:30,  1.68it/s, v_num=6, loss=1.640]Epoch 0:   0%|          | 30/8462 [00:17<1:24:00,  1.67it/s, v_num=6, loss=1.760]Epoch 0:   0%|          | 31/8462 [00:18<1:22:41,  1.70it/s, v_num=6, loss=1.760]Epoch 0:   0%|          | 31/8462 [00:18<1:23:07,  1.69it/s, v_num=6, loss=2.080]Epoch 0:   0%|          | 32/8462 [00:18<1:21:52,  1.72it/s, v_num=6, loss=2.080]Epoch 0:   0%|          | 32/8462 [00:18<1:22:16,  1.71it/s, v_num=6, loss=2.260]Epoch 0:   0%|          | 33/8462 [00:19<1:23:53,  1.67it/s, v_num=6, loss=2.260]Epoch 0:   0%|          | 33/8462 [00:19<1:24:16,  1.67it/s, v_num=6, loss=2.260]Epoch 0:   0%|          | 34/8462 [00:20<1:23:08,  1.69it/s, v_num=6, loss=2.260]Epoch 0:   0%|          | 34/8462 [00:20<1:23:57,  1.67it/s, v_num=6, loss=2.020]Epoch 0:   0%|          | 35/8462 [00:20<1:22:47,  1.70it/s, v_num=6, loss=2.020]Epoch 0:   0%|          | 35/8462 [00:20<1:23:12,  1.69it/s, v_num=6, loss=2.380]Epoch 0:   0%|          | 36/8462 [00:21<1:22:04,  1.71it/s, v_num=6, loss=2.380]Epoch 0:   0%|          | 36/8462 [00:21<1:22:26,  1.70it/s, v_num=6, loss=2.210]Epoch 0:   0%|          | 37/8462 [00:21<1:21:22,  1.73it/s, v_num=6, loss=2.210]Epoch 0:   0%|          | 37/8462 [00:21<1:21:46,  1.72it/s, v_num=6, loss=2.510]Epoch 0:   0%|          | 38/8462 [00:21<1:20:45,  1.74it/s, v_num=6, loss=2.510]Epoch 0:   0%|          | 38/8462 [00:21<1:21:07,  1.73it/s, v_num=6, loss=1.520]Epoch 0:   0%|          | 39/8462 [00:22<1:20:09,  1.75it/s, v_num=6, loss=1.520]Epoch 0:   0%|          | 39/8462 [00:22<1:20:29,  1.74it/s, v_num=6, loss=2.230]Epoch 0:   0%|          | 40/8462 [00:22<1:19:34,  1.76it/s, v_num=6, loss=2.230]Epoch 0:   0%|          | 40/8462 [00:22<1:19:54,  1.76it/s, v_num=6, loss=2.220]Epoch 0:   0%|          | 41/8462 [00:23<1:19:04,  1.78it/s, v_num=6, loss=2.220]Epoch 0:   0%|          | 41/8462 [00:23<1:19:37,  1.76it/s, v_num=6, loss=2.310]Epoch 0:   0%|          | 42/8462 [00:23<1:18:45,  1.78it/s, v_num=6, loss=2.310]Epoch 0:   0%|          | 42/8462 [00:23<1:19:54,  1.76it/s, v_num=6, loss=1.980]Epoch 0:   1%|          | 43/8462 [00:24<1:19:02,  1.78it/s, v_num=6, loss=1.980]Epoch 0:   1%|          | 43/8462 [00:24<1:19:20,  1.77it/s, v_num=6, loss=2.310]Epoch 0:   1%|          | 44/8462 [00:24<1:18:30,  1.79it/s, v_num=6, loss=2.310]Epoch 0:   1%|          | 44/8462 [00:24<1:18:48,  1.78it/s, v_num=6, loss=2.040]Epoch 0:   1%|          | 45/8462 [00:25<1:17:59,  1.80it/s, v_num=6, loss=2.040]Epoch 0:   1%|          | 45/8462 [00:25<1:18:18,  1.79it/s, v_num=6, loss=2.020]Epoch 0:   1%|          | 46/8462 [00:25<1:17:31,  1.81it/s, v_num=6, loss=2.020]Epoch 0:   1%|          | 46/8462 [00:25<1:17:49,  1.80it/s, v_num=6, loss=2.130]Epoch 0:   1%|          | 47/8462 [00:25<1:17:04,  1.82it/s, v_num=6, loss=2.130]Epoch 0:   1%|          | 47/8462 [00:25<1:17:22,  1.81it/s, v_num=6, loss=2.240]Epoch 0:   1%|          | 48/8462 [00:26<1:16:39,  1.83it/s, v_num=6, loss=2.240]Epoch 0:   1%|          | 48/8462 [00:26<1:16:55,  1.82it/s, v_num=6, loss=1.890]Epoch 0:   1%|          | 49/8462 [00:26<1:16:14,  1.84it/s, v_num=6, loss=1.890]Epoch 0:   1%|          | 49/8462 [00:26<1:16:57,  1.82it/s, v_num=6, loss=1.810]Epoch 0:   1%|          | 50/8462 [00:27<1:16:16,  1.84it/s, v_num=6, loss=1.810]Epoch 0:   1%|          | 50/8462 [00:27<1:18:21,  1.79it/s, v_num=6, loss=1.880]Epoch 0:   1%|          | 51/8462 [00:28<1:17:40,  1.80it/s, v_num=6, loss=1.880]Epoch 0:   1%|          | 51/8462 [00:28<1:17:55,  1.80it/s, v_num=6, loss=2.000]Epoch 0:   1%|          | 52/8462 [00:28<1:17:15,  1.81it/s, v_num=6, loss=2.000]Epoch 0:   1%|          | 52/8462 [00:28<1:17:30,  1.81it/s, v_num=6, loss=2.000]Epoch 0:   1%|          | 53/8462 [00:29<1:16:49,  1.82it/s, v_num=6, loss=2.000]Epoch 0:   1%|          | 53/8462 [00:29<1:17:05,  1.82it/s, v_num=6, loss=2.390]Epoch 0:   1%|          | 54/8462 [00:29<1:16:26,  1.83it/s, v_num=6, loss=2.390]Epoch 0:   1%|          | 54/8462 [00:29<1:16:41,  1.83it/s, v_num=6, loss=1.960]Epoch 0:   1%|          | 55/8462 [00:29<1:16:06,  1.84it/s, v_num=6, loss=1.960]Epoch 0:   1%|          | 55/8462 [00:29<1:16:20,  1.84it/s, v_num=6, loss=2.290]Epoch 0:   1%|          | 56/8462 [00:30<1:15:44,  1.85it/s, v_num=6, loss=2.290]Epoch 0:   1%|          | 56/8462 [00:30<1:15:59,  1.84it/s, v_num=6, loss=1.830]Epoch 0:   1%|          | 57/8462 [00:30<1:15:26,  1.86it/s, v_num=6, loss=1.830]