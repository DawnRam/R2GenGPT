# StableLM-3B-4E1T é›†æˆæŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨R2GenGPTä¸­é›†æˆStability AIçš„StableLM-3B-4E1Tæ¨¡å‹ã€‚

## 1. æ¨¡å‹ä¿¡æ¯

- **æ¨¡å‹åç§°**: stabilityai/stablelm-3b-4e1t
- **æ¨¡å‹ç±»å‹**: å› æœè¯­è¨€æ¨¡å‹
- **å‚æ•°é‡**: 3B
- **è®¸å¯è¯**: CC-BY-SA-4.0 (éå¸¸å¼€æ”¾)
- **ç‰¹ç‚¹**: è½»é‡çº§ï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œå¼€æºå‹å¥½

## 2. æ¨¡å‹ä¼˜åŠ¿

### 2.1 å¼€æºå‹å¥½
- **è®¸å¯è¯**: CC-BY-SA-4.0ï¼Œå…è®¸å•†ä¸šä½¿ç”¨
- **æ— è®¿é—®é™åˆ¶**: æ— éœ€ç”³è¯·æƒé™å³å¯ä½¿ç”¨
- **å®Œå…¨å¼€æº**: æ¨¡å‹æƒé‡å’Œä»£ç å®Œå…¨å¼€æ”¾

### 2.2 æ€§èƒ½ç‰¹ç‚¹
- **è½»é‡çº§**: 3Bå‚æ•°ï¼Œå†…å­˜éœ€æ±‚ä½
- **é«˜æ•ˆ**: æ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨
- **ç¨³å®š**: ç»è¿‡å……åˆ†è®­ç»ƒå’Œæµ‹è¯•

## 3. ä»£ç ä¿®æ”¹

### 3.1 å¯¼å…¥è¯­å¥
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
```

### 3.2 æ¨¡å‹åˆå§‹åŒ–
```python
# åˆå§‹åŒ–tokenizer
self.lm_tokenizer = AutoTokenizer.from_pretrained(args.lm_model)

# å¤„ç†ç‰¹æ®Štoken
if self.lm_tokenizer.pad_token is None:
    self.lm_tokenizer.pad_token = self.lm_tokenizer.eos_token
    self.lm_tokenizer.pad_token_id = self.lm_tokenizer.eos_token_id

# åˆå§‹åŒ–æ¨¡å‹
self.lm_model = AutoModelForCausalLM.from_pretrained(
    args.lm_model,
    torch_dtype="auto",
    device_map="auto"  # å¦‚æœä½¿ç”¨å¤šGPU
)
```

### 3.3 é…ç½®æ–‡ä»¶ä¿®æ”¹
```python
# configs/config.py
parser.add_argument('--lm_model', default='stabilityai/stablelm-3b-4e1t', type=str, help="Language model to use")
```

## 4. StableLMç‰¹å®šä¼˜åŒ–

### 4.1 å†…å­˜ä¼˜åŒ–
```python
# ä½¿ç”¨8ä½é‡åŒ–
if args.low_resource:
    self.lm_model = AutoModelForCausalLM.from_pretrained(
        args.lm_model,
        torch_dtype="auto",
        load_in_8bit=True,
        device_map="auto"
    )

# ä½¿ç”¨4ä½é‡åŒ–
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
self.lm_model = AutoModelForCausalLM.from_pretrained(
    args.lm_model,
    torch_dtype="auto",
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 4.2 LoRAé…ç½®
```python
# StableLMçš„LoRAé…ç½®
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)
```

## 5. å¯¹è¯æ ¼å¼é€‚é…

### 5.1 StableLMå¯¹è¯æ¨¡æ¿
StableLMä½¿ç”¨ç›¸å¯¹ç®€å•çš„å¯¹è¯æ ¼å¼ï¼š

```python
def prompt_wrap(self, img_embeds, atts_img):
    # StableLMçš„å¯¹è¯æ ¼å¼
    prompt = f'Human: <Img><ImageHere></Img> {self.prompt} \nAssistant:'
    
    batch_size = img_embeds.shape[0]
    p_before, p_after = prompt.split('<ImageHere>')
    
    p_before_tokens = self.lm_tokenizer(
        p_before, return_tensors="pt", add_special_tokens=False).to(img_embeds.device)
    p_after_tokens = self.lm_tokenizer(
        p_after, return_tensors="pt", add_special_tokens=False).to(img_embeds.device)
    
    p_before_embeds = self.embed_tokens(p_before_tokens.input_ids).expand(batch_size, -1, -1)
    p_after_embeds = self.embed_tokens(p_after_tokens.input_ids).expand(batch_size, -1, -1)
    
    wrapped_img_embeds = torch.cat([p_before_embeds, img_embeds, p_after_embeds], dim=1)
    wrapped_atts_img = atts_img[:, :1].expand(-1, wrapped_img_embeds.shape[1])
    
    return wrapped_img_embeds, wrapped_atts_img
```

### 5.2 ç»“æŸç¬¦å·å¤„ç†
```python
# StableLMçš„ç»“æŸç¬¦å·
self.end_sym = '</s>'
```

## 6. ç”Ÿæˆå‚æ•°ä¼˜åŒ–

### 6.1 æ¨èçš„ç”Ÿæˆå‚æ•°
```python
outputs = self.lm_model.generate(
    inputs_embeds=inputs_embeds,
    num_beams=3,
    do_sample=False,
    min_new_tokens=80,
    max_new_tokens=120,
    repetition_penalty=1.2,
    temperature=0.7,
    pad_token_id=self.lm_tokenizer.pad_token_id,
    eos_token_id=self.lm_tokenizer.eos_token_id,
    # StableLMç‰¹å®šçš„å‚æ•°
    use_cache=True,
    return_dict_in_generate=True
)
```

### 6.2 è§£ç ä¼˜åŒ–
```python
def decode(self, output_token):
    # ç§»é™¤ç‰¹æ®Štoken
    if output_token[0] == self.lm_tokenizer.bos_token_id:
        output_token = output_token[1:]
    
    output_text = self.lm_tokenizer.decode(output_token, add_special_tokens=False)
    
    # ç§»é™¤StableLMç‰¹å®šçš„æ ‡è®°
    output_text = output_text.replace('<s>', '').replace('</s>', '')
    output_text = output_text.replace('Human:', '').replace('Assistant:', '')
    
    return output_text.strip()
```

## 7. æ€§èƒ½åŸºå‡†

### 7.1 å†…å­˜ä½¿ç”¨
- **FP16**: ~6GB VRAM
- **8-bit**: ~3GB VRAM  
- **4-bit**: ~1.5GB VRAM

### 7.2 æ¨ç†é€Ÿåº¦
- **å•GPU**: ~60 tokens/s
- **å¤šGPU**: å¯çº¿æ€§æ‰©å±•

## 8. å¸¸è§é—®é¢˜è§£å†³

### 8.1 æ¨¡å‹åŠ è½½é”™è¯¯
```python
# é”™è¯¯: ModuleNotFoundError: No module named 'transformers.models.stablelm'
# è§£å†³: ä½¿ç”¨AutoTokenizerå’ŒAutoModelForCausalLM
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("stabilityai/stablelm-3b-4e1t")
model = AutoModelForCausalLM.from_pretrained(
    "stabilityai/stablelm-3b-4e1t",
    torch_dtype="auto",
)
```

### 8.2 å†…å­˜ä¸è¶³
```python
# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
self.lm_model.gradient_checkpointing_enable()

# ä½¿ç”¨CPUå¸è½½
self.lm_model = AutoModelForCausalLM.from_pretrained(
    args.lm_model,
    torch_dtype="auto",
    device_map="auto",
    offload_folder="offload"
)
```

### 8.3 ç”Ÿæˆè´¨é‡é—®é¢˜
```python
# è°ƒæ•´é‡å¤æƒ©ç½š
repetition_penalty=1.1  # é™ä½é‡å¤æƒ©ç½š

# ä½¿ç”¨æ¸©åº¦é‡‡æ ·
do_sample=True
temperature=0.8
top_p=0.9
```

## 9. è®­ç»ƒé…ç½®ç¤ºä¾‹

### 9.1 å®Œæ•´è®­ç»ƒé…ç½®
```bash
python train.py \
    --lm_model stabilityai/stablelm-3b-4e1t \
    --vision_model microsoft/swin-base-patch4-window7-224 \
    --batch_size 6 \
    --learning_rate 1e-4 \
    --llm_use_lora True \
    --llm_r 16 \
    --llm_alpha 16 \
    --max_epochs 3 \
    --precision bf16-mixed
```

### 9.2 æ¨ç†é…ç½®
```bash
python test.py \
    --lm_model stabilityai/stablelm-3b-4e1t \
    --ckpt_file path/to/checkpoint.pth \
    --test_batch_size 12
```

## 10. è¯„ä¼°ç»“æœ

åŸºäºStableLM-3B-4E1Tçš„R2GenGPTé¢„æœŸæ€§èƒ½ï¼š
- **BLEU-4**: ~0.12-0.18
- **CIDEr**: ~0.20-0.30
- **ROUGE-L**: ~0.25-0.35
- **METEOR**: ~0.18-0.23

## 11. ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | å†…å­˜éœ€æ±‚ | è®¸å¯è¯ | æ€§èƒ½ |
|------|--------|----------|--------|------|
| LLaMA-2-7B | 7B | ~14GB | å•†ä¸šé™åˆ¶ | é«˜ |
| Gemma-3-4B | 4B | ~8GB | Gemma License | é«˜ |
| **StableLM-3B** | **3B** | **~6GB** | **CC-BY-SA-4.0** | **ä¸­é«˜** |
| ChatGLM3-6B | 6B | ~12GB | å•†ä¸šé™åˆ¶ | é«˜ |

## 12. éƒ¨ç½²å»ºè®®

### 12.1 ç”Ÿäº§ç¯å¢ƒ
```python
# ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬
self.lm_model = AutoModelForCausalLM.from_pretrained(
    args.lm_model,
    torch_dtype="auto",
    load_in_4bit=True,
    device_map="auto"
)

# å¯ç”¨æ¨ç†ä¼˜åŒ–
self.lm_model.eval()
torch.set_grad_enabled(False)
```

### 12.2 è¾¹ç¼˜è®¾å¤‡
```python
# ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
```

## 13. æ€»ç»“

StableLM-3B-4E1Tæ˜¯ä¸€ä¸ªä¼˜ç§€çš„æ›¿ä»£é€‰æ‹©ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

### âœ… **ä¼˜åŠ¿**
1. **å¼€æºå‹å¥½**: CC-BY-SA-4.0è®¸å¯è¯ï¼Œæ— ä½¿ç”¨é™åˆ¶
2. **è½»é‡çº§**: 3Bå‚æ•°ï¼Œå†…å­˜éœ€æ±‚ä½
3. **é«˜æ•ˆ**: æ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨
4. **ç¨³å®š**: ç»è¿‡å……åˆ†è®­ç»ƒå’Œæµ‹è¯•
5. **æ˜“éƒ¨ç½²**: æ”¯æŒå¤šç§é‡åŒ–æ–¹å¼

### âš ï¸ **æ³¨æ„äº‹é¡¹**
1. **æ€§èƒ½**: ç›¸æ¯”7Bæ¨¡å‹ï¼Œæ€§èƒ½å¯èƒ½ç•¥ä½
2. **ä¸Šä¸‹æ–‡é•¿åº¦**: å¯èƒ½éœ€è¦è°ƒæ•´æœ€å¤§é•¿åº¦è®¾ç½®
3. **è®­ç»ƒæ•°æ®**: éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´è®­ç»ƒç­–ç•¥

### ğŸš€ **æ¨èä½¿ç”¨åœºæ™¯**
- èµ„æºå—é™çš„ç¯å¢ƒ
- éœ€è¦å¿«é€Ÿéƒ¨ç½²çš„é¡¹ç›®
- å¯¹å¼€æºè®¸å¯è¯æœ‰ä¸¥æ ¼è¦æ±‚çš„ä¼ä¸š
- å®æ—¶æ¨ç†åº”ç”¨

é€šè¿‡éµå¾ªæœ¬æŒ‡å—ï¼Œä½ å¯ä»¥æˆåŠŸå°†StableLM-3B-4E1Té›†æˆåˆ°R2GenGPTä¸­ï¼Œè·å¾—ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆä¸”å®Œå…¨å¼€æºçš„åŒ»å­¦å›¾åƒæŠ¥å‘Šç”Ÿæˆç³»ç»Ÿã€‚ 